import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import os
import sys
import time
import multiprocessing as mp
from functools import partial
import psutil

# Try to import temperature monitoring libraries
try:
    # Optional module for temperature monitoring
    import py3nvml.py3nvml as nvml
    NVML_AVAILABLE = True
except ImportError:
    NVML_AVAILABLE = False

# Try to import GPU acceleration libraries
try:
    import cupy as cp
    GPU_AVAILABLE = True
    CUDF_AVAILABLE = False
    print("GPU acceleration enabled using CuPy")
    
    # Check if cuDF is also available, but don't require it
    try:
        import cudf
        CUDF_AVAILABLE = True
        print("RAPIDS cuDF library also available")
    except ImportError:
        CUDF_AVAILABLE = False
        print("CuPy available, but RAPIDS cuDF library not available. Using pandas for dataframes.")
except ImportError:
    GPU_AVAILABLE = False
    CUDF_AVAILABLE = False
    print("GPU acceleration libraries (CuPy) not available, using CPU")

try:
    import matplotlib.pyplot as plt
    import matplotlib.dates as mdates
    from matplotlib.ticker import FuncFormatter
    MATPLOTLIB_AVAILABLE = True
except ImportError:
    MATPLOTLIB_AVAILABLE = False
    
try:
    import itertools
    from tqdm import tqdm
except ImportError:
    pass

# Default parameters - UPDATED with new SD parameters and CPU management
default_params = {
    "cluster_levels": [0.2, 0.1, 0, -0.1, -0.2, -0.3, -0.4, -0.5, -0.6, -0.7, -0.8, -0.9],
    "analysis_mode": "long",
    "selected_days": None,
    "use_wdr_filter": False,
    "use_m7b_filter": True,
    "long_confirm_start": "10:30",
    "long_confirm_end": "10:55",
    "short_confirm_start": "10:30",
    "short_confirm_end": "10:50",
    "tp_multiple": 1.0,
    "sl_distance": 0.1,
    "sl_extra_ticks": 2,
    "min_trades": 5,
    "top_configs": 10,
    "target_win_rate": 50.0,
    "target_expectancy": 1.5,
    "show_raw_data": True,
    "show_overall_stats": True,
    "show_day_performance": True,
    "show_top_configs": True,
    "show_target_configs": True,
    "show_filter_stats": True,
    "calculate_profit_factor": False,
    "analyze_streaks": False,
    "perform_param_sweep": False,
    "perform_day_by_day_sweep": True,
    "perform_walk_forward": False,
    "generate_equity_curve": False,
    "use_gpu": True,
    "use_multiprocessing": True,
    "data_file": "es_data.txt",
    "output_dir": "results",
    "tick_size": 0.25,
    
    # NEW parameters for SD retracement and Target SD
    "sd_retracement_ranges": [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],
    "target_sd_ranges": [0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.5, 1.8, 2.0],
    "default_sd_retracement": 0.3,
    "default_target_sd": 1.0,
    "max_r_reduction_pct": 30.0,  # Maximum allowed R reduction percentage
    "use_sd_based_entries": False,  # Flag to switch between old and new approach
    "use_calendar_years": True,     # Use calendar years for walk-forward
    "in_sample_years": 6,           # Years for in-sample data
    "out_sample_years": 2,          # Years for out-sample data
    "keep_top_configs": 2,          # Number of top configurations to keep
    
    # NEW parameters for CPU usage and temperature management
    "max_cpu_cores": 4,              # Limit CPU cores (default to 4 instead of all 16)
    "enable_temp_monitoring": True,  # Enable temperature monitoring
    "max_temperature": 70,           # Maximum temperature in Celsius
    "cpu_usage_limit": 70,           # Maximum CPU usage percentage
    "cooling_pause_seconds": 5,      # Pause duration for cooling when temp is high
}

# =========================== CPU/TEMPERATURE MONITORING FUNCTIONS ===========================

def check_system_load(params):
    """Check CPU temperature and usage, return True if we should pause processing"""
    
    # Check CPU usage
    cpu_usage = psutil.cpu_percent(interval=0.5)
    if cpu_usage > params['cpu_usage_limit']:
        print(f"\nCPU usage ({cpu_usage}%) exceeds limit ({params['cpu_usage_limit']}%). Pausing for cooling...")
        return True
    
    # Check temperature if monitoring is enabled
    if params['enable_temp_monitoring']:
        try:
            # Try to get CPU temperature (this is platform-specific)
            if hasattr(psutil, "sensors_temperatures"):
                temps = psutil.sensors_temperatures()
                if temps:
                    # Get the highest temperature from any CPU core
                    max_temp = 0
                    for name, entries in temps.items():
                        if name.lower().startswith(('cpu', 'core', 'k10temp')):
                            for entry in entries:
                                if entry.current > max_temp:
                                    max_temp = entry.current
                    
                    if max_temp > params['max_temperature']:
                        print(f"\nCPU temperature ({max_temp}째C) exceeds limit ({params['max_temperature']}째C). Pausing for cooling...")
                        return True
            
            # Try NVIDIA GPU temperature if available
            if NVML_AVAILABLE:
                try:
                    nvml.nvmlInit()
                    device_count = nvml.nvmlDeviceGetCount()
                    for i in range(device_count):
                        handle = nvml.nvmlDeviceGetHandleByIndex(i)
                        temp = nvml.nvmlDeviceGetTemperature(handle, nvml.NVML_TEMPERATURE_GPU)
                        if temp > params['max_temperature']:
                            print(f"\nGPU temperature ({temp}째C) exceeds limit ({params['max_temperature']}째C). Pausing for cooling...")
                            return True
                except Exception:
                    pass  # Ignore NVML errors
        
        except Exception as e:
            print(f"Warning: Error monitoring temperature: {e}")
    
    # All checks passed, no need to pause
    return False

def process_with_resource_management(func, item_list, params, desc="Processing"):
    """Process items with multiprocessing while managing CPU resources"""
    
    results = []
    total_items = len(item_list)
    
    if params['use_multiprocessing']:
        # Calculate number of cores to use (respect max_cpu_cores parameter)
        num_processes = min(params['max_cpu_cores'], mp.cpu_count(), total_items)
        print(f"Using {num_processes} of {mp.cpu_count()} available CPU cores")
        
        # Process in smaller batches to allow temperature management
        batch_size = max(1, total_items // (num_processes * 2))
        
        processed = 0
        with mp.Pool(processes=num_processes) as pool:
            for i in range(0, total_items, batch_size):
                # Check system load before processing each batch
                if check_system_load(params):
                    print(f"Pausing for {params['cooling_pause_seconds']} seconds to cool down...")
                    time.sleep(params['cooling_pause_seconds'])
                
                # Process a batch
                end_idx = min(i + batch_size, total_items)
                batch = item_list[i:end_idx]
                batch_results = list(pool.map(func, batch))
                results.extend(batch_results)
                
                # Update progress
                processed += len(batch)
                progress = (processed / total_items) * 100
                sys.stdout.write(f"\r{desc}: {progress:.1f}% - Processed {processed}/{total_items}")
                sys.stdout.flush()
            
            print()  # New line after progress bar
    else:
        # Sequential processing
        for i, item in enumerate(item_list):
            # Check system load periodically
            if i % 10 == 0 and check_system_load(params):
                print(f"Pausing for {params['cooling_pause_seconds']} seconds to cool down...")
                time.sleep(params['cooling_pause_seconds'])
            
            # Process item
            result = func(item)
            results.append(result)
            
            # Update progress
            progress = ((i + 1) / total_items) * 100
            sys.stdout.write(f"\r{desc}: {progress:.1f}% - Processed {i+1}/{total_items}")
            sys.stdout.flush()
        
        print()  # New line after progress bar
    
    return results

# =========================== DISPLAY FUNCTIONS ===========================

def display_raw_data(results_df, params):
    """Display raw configuration data"""
    if not params["show_raw_data"]:
        return
    
    # Define weekday order for sorting
    weekday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']
    
    print("\n" + "=" * 100)
    print(" " * 30 + "RAW DATA - ALL CONFIGURATIONS")
    print("=" * 100)
    
    # Group by day_of_week, confirmation time, trade_direction, and candidate_cluster or sd_retracement + target_sd
    if 'candidate_cluster' in results_df.columns:
        # Old cluster-based approach
        grouped = results_df.groupby(['day_of_week', 'confirm_time_str', 'trade_direction', 'candidate_cluster'])
    else:
        # New SD-based approach
        grouped = results_df.groupby(['day_of_week', 'confirm_time_str', 'trade_direction', 'sd_retracement', 'target_sd'])
    
    # Sort days of week in proper order
    sorted_groups = []
    
    for name, group in grouped:
        if 'candidate_cluster' in results_df.columns:
            day, conf_time, direction, cluster = name
            day_index = weekday_order.index(day) if day in weekday_order else 999
            sorted_groups.append((day_index, conf_time, direction, cluster, None, None, group))
        else:
            day, conf_time, direction, sd_retrace, target_sd = name
            day_index = weekday_order.index(day) if day in weekday_order else 999
            sorted_groups.append((day_index, conf_time, direction, None, sd_retrace, target_sd, group))
    
    # Sort by day, time, direction, and cluster/SD params
    sorted_groups.sort()
    
    # Print raw data in the format requested
    for group_info in sorted_groups:
        day_index, conf_time, direction, cluster, sd_retrace, target_sd, group = group_info
        day = group['day_of_week'].iloc[0]  # Get actual day name
        total_in_group = len(group)
        valid = group[group["trade_entry"].notna()]
        valid_count = len(valid)
        
        if not valid.empty:
            # Use the corrected expectancy calculation
            win_rate, avg_winner, avg_loser, grp_expectancy = calculate_expectancy(valid)
            
            # Calculate additional metrics
            profit_factor = calculate_profit_factor(valid) if params["calculate_profit_factor"] else None
            
            # Also calculate TP win rate for reference
            tp_win_rate = (valid["exit_reason"] == "TP").mean() * 100
            grp_total_R = valid["pnl_R"].sum()
            
            # Highlight profitability with simple markers
            profit_marker = '+' if grp_total_R > 0 else '-' if grp_total_R < 0 else ' '
            
            if cluster is not None:  # Cluster-based approach
                output_line = (f"{day} at {conf_time} ({str(direction).capitalize()}, Cluster: {float(cluster):+.2f}): "
                      f"{total_in_group} sessions, Valid Occurrences: {valid_count}, "
                      f"Profit Win Rate: {win_rate:.2f}% (TP Rate: {tp_win_rate:.2f}%), "
                      f"Total R: {profit_marker}{abs(grp_total_R):.2f}, Expectancy: {grp_expectancy:.2f}")
            else:  # SD-based approach
                output_line = (f"{day} at {conf_time} ({str(direction).capitalize()}, SD Retrace: {float(sd_retrace):.1f}, Target SD: {float(target_sd):.1f}): "
                      f"{total_in_group} sessions, Valid Occurrences: {valid_count}, "
                      f"Profit Win Rate: {win_rate:.2f}% (TP Rate: {tp_win_rate:.2f}%), "
                      f"Total R: {profit_marker}{abs(grp_total_R):.2f}, Expectancy: {grp_expectancy:.2f}")
            
            if profit_factor is not None:
                output_line += f", Profit Factor: {profit_factor:.2f}"
                
            print(output_line)
        else:
            if cluster is not None:  # Cluster-based approach
                print(f"{day} at {conf_time} ({str(direction).capitalize()}, Cluster: {float(cluster):+.2f}): "
                    f"{total_in_group} sessions, Valid Occurrences: {valid_count}, "
                    f"Profit Win Rate: N/A, Total R: N/A, Expectancy: N/A")
            else:  # SD-based approach
                print(f"{day} at {conf_time} ({str(direction).capitalize()}, SD Retrace: {float(sd_retrace):.1f}, Target SD: {float(target_sd):.1f}): "
                    f"{total_in_group} sessions, Valid Occurrences: {valid_count}, "
                    f"Profit Win Rate: N/A, Total R: N/A, Expectancy: N/A")

def display_overall_stats(results_df, filtered_df, params):
    """Display overall strategy statistics"""
    if not params["show_overall_stats"]:
        return
    
    print("\n" + "=" * 100)
    print(" " * 35 + "OVERALL STATISTICS")
    print("=" * 100)
    
    # Create a copy of results_df with unfiltered trade data for comparison
    unfiltered_results_df = results_df.copy()
    if 'unfiltered_trade_entry' in unfiltered_results_df.columns and 'unfiltered_pnl_R' in unfiltered_results_df.columns and 'unfiltered_exit_reason' in unfiltered_results_df.columns:
        # Copy unfiltered data to the columns used for analysis
        mask = unfiltered_results_df['unfiltered_trade_entry'].notna()
        unfiltered_results_df.loc[mask, 'trade_entry'] = unfiltered_results_df.loc[mask, 'unfiltered_trade_entry']
        unfiltered_results_df.loc[mask, 'pnl_R'] = unfiltered_results_df.loc[mask, 'unfiltered_pnl_R']
        unfiltered_results_df.loc[mask, 'exit_reason'] = unfiltered_results_df.loc[mask, 'unfiltered_exit_reason']
    
    # Print stats for unfiltered data
    if params["use_wdr_filter"] or params["use_m7b_filter"]:
        print("\n--- WITHOUT FILTERS ---")
    
    # Unfiltered stats - using the unfiltered data
    unfiltered_valid_trades_df = unfiltered_results_df[unfiltered_results_df["trade_entry"].notna()]
    median_retracement = unfiltered_results_df["retracement_SD"].median()
    median_extension = unfiltered_results_df["extension_SD"].median()
    
    print(f"Total valid session entries: {len(unfiltered_results_df)}")
    print(f"Median Retracement (in SD units): {median_retracement:.2f}")
    print(f"Median Maximum Extension (in SD units): {median_extension:.2f}")

    if not unfiltered_valid_trades_df.empty:
        # Use GPU-accelerated expectancy calculation if available
        if params['use_gpu'] and GPU_AVAILABLE:
            try:
                profit_win_rate, avg_winner, avg_loser, overall_expectancy = calculate_expectancy_gpu(unfiltered_valid_trades_df)
            except Exception as e:
                print(f"GPU calculation failed, falling back to CPU: {e}")
                profit_win_rate, avg_winner, avg_loser, overall_expectancy = calculate_expectancy(unfiltered_valid_trades_df)
        else:
            profit_win_rate, avg_winner, avg_loser, overall_expectancy = calculate_expectancy(unfiltered_valid_trades_df)
        
        # Calculate additional metrics
        if params["calculate_profit_factor"]:
            profit_factor = calculate_profit_factor(unfiltered_valid_trades_df)
        
        if params["analyze_streaks"]:
            max_win_streak, max_loss_streak, avg_win_streak, avg_loss_streak, _ = analyze_streaks(unfiltered_valid_trades_df)
        
        # Also calculate TP hit rate for reference
        tp_win_rate = (unfiltered_valid_trades_df["exit_reason"] == "TP").mean() * 100
        total_R = unfiltered_valid_trades_df["pnl_R"].sum()
        
        print(f"\nValid trades: {len(unfiltered_valid_trades_df)}")
        print(f"Overall Profit Win Rate: {profit_win_rate:.2f}%")
        print(f"Overall TP Hit Rate: {tp_win_rate:.2f}%")
        print(f"Overall Total R (sum of pnl_R): {total_R:.2f}")
        print(f"Overall Average Winner (in R): {avg_winner:.2f}")
        print(f"Overall Average Loser (in R): {avg_loser:.2f}")
        print(f"Overall Expectancy (in R): {overall_expectancy:.2f}")
        
        if params["calculate_profit_factor"]:
            print(f"Overall Profit Factor: {profit_factor:.2f}")
        
        if params["analyze_streaks"]:
            print("\nStreaks Analysis:")
            print(f"Max Winning Streak: {max_win_streak}")
            print(f"Max Losing Streak: {max_loss_streak}")
            print(f"Average Winning Streak: {avg_win_streak:.2f}")
            print(f"Average Losing Streak: {avg_loss_streak:.2f}")
    else:
        print("No trades were simulated in the valid sessions.")
    
    # Print stats for filtered data if filtering is enabled
    if (params["use_wdr_filter"] or params["use_m7b_filter"]) and params["show_overall_stats"]:
        print("\n--- WITH FILTERS ---")
        filtered_valid_trades = filtered_df[filtered_df["trade_entry"].notna()]
        filtered_median_retracement = filtered_df["retracement_SD"].median()
        filtered_median_extension = filtered_df["extension_SD"].median()
        
        filtered_session_count = len(filtered_df)
        if params["use_wdr_filter"] and 'wdr_session_invalid' in filtered_df.columns:
            filtered_session_count -= filtered_df['wdr_session_invalid'].sum()
        
        print(f"Total valid session entries: {filtered_session_count}")
        print(f"Median Retracement (in SD units): {filtered_median_retracement:.2f}")
        print(f"Median Maximum Extension (in SD units): {filtered_median_extension:.2f}")

        if not filtered_valid_trades.empty:
            # Use corrected expectancy calculation
            filtered_profit_win_rate, filtered_avg_winner, filtered_avg_loser, filtered_expectancy = calculate_expectancy(filtered_valid_trades)
            
            # Calculate additional metrics for filtered data
            if params["calculate_profit_factor"]:
                filtered_profit_factor = calculate_profit_factor(filtered_valid_trades)
            
            if params["analyze_streaks"]:
                filtered_max_win_streak, filtered_max_loss_streak, filtered_avg_win_streak, filtered_avg_loss_streak, _ = analyze_streaks(filtered_valid_trades)
            
            # Also calculate TP hit rate for reference
            filtered_tp_win_rate = (filtered_valid_trades["exit_reason"] == "TP").mean() * 100
            filtered_total_R = filtered_valid_trades["pnl_R"].sum()
            
            print(f"\nValid trades: {len(filtered_valid_trades)}")
            print(f"Overall Profit Win Rate: {filtered_profit_win_rate:.2f}%")
            print(f"Overall TP Hit Rate: {filtered_tp_win_rate:.2f}%")
            print(f"Overall Total R (sum of pnl_R): {filtered_total_R:.2f}")
            print(f"Overall Average Winner (in R): {filtered_avg_winner:.2f}")
            print(f"Overall Average Loser (in R): {filtered_avg_loser:.2f}")
            print(f"Overall Expectancy (in R): {filtered_expectancy:.2f}")
            
            if params["calculate_profit_factor"]:
                print(f"Overall Profit Factor: {filtered_profit_factor:.2f}")
            
            if params["analyze_streaks"]:
                print("\nStreaks Analysis (Filtered):")
                print(f"Max Winning Streak: {filtered_max_win_streak}")
                print(f"Max Losing Streak: {filtered_max_loss_streak}")
                print(f"Average Winning Streak: {filtered_avg_win_streak:.2f}")
                print(f"Average Losing Streak: {filtered_avg_loss_streak:.2f}")
            
            # Calculate improvement safely
            try:
                improvement_R = filtered_total_R - total_R
                improvement_R_pct = (improvement_R/abs(total_R))*100 if total_R != 0 else float('inf')
            except:
                improvement_R = 0
                improvement_R_pct = 0
                
            try:
                improvement_win_rate = filtered_profit_win_rate - profit_win_rate
                improvement_win_rate_pct = (improvement_win_rate/profit_win_rate)*100 if profit_win_rate != 0 else float('inf')
            except:
                improvement_win_rate = 0
                improvement_win_rate_pct = 0
                
            try:
                improvement_expectancy = filtered_expectancy - overall_expectancy
                improvement_expectancy_pct = (improvement_expectancy/abs(overall_expectancy))*100 if overall_expectancy != 0 else float('inf')
            except:
                improvement_expectancy = 0
                improvement_expectancy_pct = 0
            
            print("\nImprovement from filtering:")
            print(f"Total R: {improvement_R:.2f} ({improvement_R_pct:.2f}% change)")
            print(f"Win Rate: {improvement_win_rate:.2f}% ({improvement_win_rate_pct:.2f}% change)")
            print(f"Expectancy: {improvement_expectancy:.2f} ({improvement_expectancy_pct:.2f}% change)")
            
            if params["calculate_profit_factor"] and 'profit_factor' in locals() and profit_factor != 0:
                improvement_pf = filtered_profit_factor - profit_factor
                improvement_pf_pct = (improvement_pf/profit_factor)*100
                print(f"Profit Factor: {improvement_pf:.2f} ({improvement_pf_pct:.2f}% change)")
        else:
            print("No trades were simulated in the filtered sessions.")
    
    return unfiltered_valid_trades_df, filtered_valid_trades

def display_day_performance(unfiltered_valid_trades_df, filtered_valid_trades, params):
    """Display performance by day of week"""
    if not params["show_day_performance"]:
        return
    
    # Define weekday order for sorting
    weekday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']
    
    print("\n" + "=" * 100)
    print(" " * 32 + "PERFORMANCE BY DAY OF WEEK")
    print("=" * 100)
    
    # First show unfiltered day of week performance
    if params["use_wdr_filter"] or params["use_m7b_filter"]:
        print("\n--- WITHOUT FILTERS ---")
    
    # Calculate day of week performance for unfiltered data
    if not unfiltered_valid_trades_df.empty:
        # First, handle the case when analysis_mode is "both"
        if params["analysis_mode"] == "both":
            print("\n" + "-" * 50)
            print(" " * 15 + "LONG TRADES")
            print("-" * 50)
            
            # Filter for long trades only
            long_trades_df = unfiltered_valid_trades_df[unfiltered_valid_trades_df['trade_direction'] == "long"]
            
            if not long_trades_df.empty:
                for day in weekday_order:
                    if day in long_trades_df['day_of_week'].values:
                        day_data = long_trades_df[long_trades_df['day_of_week'] == day]
                        
                        trade_count = len(day_data)
                        win_rate, _, _, expectancy = calculate_expectancy(day_data)
                        tp_rate = (day_data["exit_reason"] == "TP").mean() * 100
                        total_r = day_data['pnl_R'].sum()
                        avg_r = day_data['pnl_R'].mean()
                        
                        # Calculate profit factor if requested
                        profit_factor_str = ""
                        if params["calculate_profit_factor"]:
                            profit_factor = calculate_profit_factor(day_data)
                            profit_factor_str = f" | Profit Factor: {profit_factor:6.2f}"
                        
                        # Profit indicator
                        profit_indicator = "+" if total_r > 0 else "-" if total_r < 0 else " "
                        
                        # Format the output line
                        print(f"{day:<10} | Trades: {trade_count:3d} | Profit Rate: {win_rate:6.2f}% | TP Rate: {tp_rate:6.2f}% | "
                              f"Total R: {profit_indicator}{abs(total_r):6.2f} | Avg R: {avg_r:6.2f} | Expectancy: {expectancy:6.2f}{profit_factor_str}")
            else:
                print("No valid long trades to analyze by day of week.")
                
            print("\n" + "-" * 50)
            print(" " * 15 + "SHORT TRADES")
            print("-" * 50)
            
            # Filter for short trades only
            short_trades_df = unfiltered_valid_trades_df[unfiltered_valid_trades_df['trade_direction'] == "short"]
            
            if not short_trades_df.empty:
                for day in weekday_order:
                    if day in short_trades_df['day_of_week'].values:
                        day_data = short_trades_df[short_trades_df['day_of_week'] == day]
                        
                        trade_count = len(day_data)
                        win_rate, _, _, expectancy = calculate_expectancy(day_data)
                        tp_rate = (day_data["exit_reason"] == "TP").mean() * 100
                        total_r = day_data['pnl_R'].sum()
                        avg_r = day_data['pnl_R'].mean()
                        
                        # Calculate profit factor if requested
                        profit_factor_str = ""
                        if params["calculate_profit_factor"]:
                            profit_factor = calculate_profit_factor(day_data)
                            profit_factor_str = f" | Profit Factor: {profit_factor:6.2f}"
                        
                        # Profit indicator
                        profit_indicator = "+" if total_r > 0 else "-" if total_r < 0 else " "
                        
                        # Format the output line
                        print(f"{day:<10} | Trades: {trade_count:3d} | Profit Rate: {win_rate:6.2f}% | TP Rate: {tp_rate:6.2f}% | "
                              f"Total R: {profit_indicator}{abs(total_r):6.2f} | Avg R: {avg_r:6.2f} | Expectancy: {expectancy:6.2f}{profit_factor_str}")
            else:
                print("No valid short trades to analyze by day of week.")
        
        # If analysis_mode is "long" or "short", keep the original logic
        else:
            for day in weekday_order:
                if day in unfiltered_valid_trades_df['day_of_week'].values:
                    day_data = unfiltered_valid_trades_df[unfiltered_valid_trades_df['day_of_week'] == day]
                    
                    trade_count = len(day_data)
                    win_rate, _, _, expectancy = calculate_expectancy(day_data)
                    tp_rate = (day_data["exit_reason"] == "TP").mean() * 100
                    total_r = day_data['pnl_R'].sum()
                    avg_r = day_data['pnl_R'].mean()
                    
                    # Calculate profit factor if requested
                    profit_factor_str = ""
                    if params["calculate_profit_factor"]:
                        profit_factor = calculate_profit_factor(day_data)
                        profit_factor_str = f" | Profit Factor: {profit_factor:6.2f}"
                    
                    # Profit indicator
                    profit_indicator = "+" if total_r > 0 else "-" if total_r < 0 else " "
                    
                    # Format the output line
                    print(f"{day:<10} | Trades: {trade_count:3d} | Profit Rate: {win_rate:6.2f}% | TP Rate: {tp_rate:6.2f}% | "
                          f"Total R: {profit_indicator}{abs(total_r):6.2f} | Avg R: {avg_r:6.2f} | Expectancy: {expectancy:6.2f}{profit_factor_str}")
    else:
        print("No valid trades to analyze by day of week.")
    
    # Then show filtered day of week performance (if enabled)
    if (params["use_wdr_filter"] or params["use_m7b_filter"]) and not filtered_valid_trades.empty:
        print("\n--- WITH FILTERS ---")
        
        # First, handle the case when analysis_mode is "both"
        if params["analysis_mode"] == "both":
            print("\n" + "-" * 50)
            print(" " * 15 + "LONG TRADES (Filtered)")
            print("-" * 50)
            
            # Filter for long trades only
            long_trades_df = filtered_valid_trades[filtered_valid_trades['trade_direction'] == "long"]
            
            if not long_trades_df.empty:
                for day in weekday_order:
                    if day in long_trades_df['day_of_week'].values:
                        day_data = long_trades_df[long_trades_df['day_of_week'] == day]
                        
                        trade_count = len(day_data)
                        win_rate, _, _, expectancy = calculate_expectancy(day_data)
                        tp_rate = (day_data["exit_reason"] == "TP").mean() * 100
                        total_r = day_data['pnl_R'].sum()
                        avg_r = day_data['pnl_R'].mean()
                        
                        # Calculate profit factor if requested
                        profit_factor_str = ""
                        if params["calculate_profit_factor"]:
                            profit_factor = calculate_profit_factor(day_data)
                            profit_factor_str = f" | Profit Factor: {profit_factor:6.2f}"
                        
                        # Profit indicator
                        profit_indicator = "+" if total_r > 0 else "-" if total_r < 0 else " "
                        
                        # Format the output line
                        print(f"{day:<10} | Trades: {trade_count:3d} | Profit Rate: {win_rate:6.2f}% | TP Rate: {tp_rate:6.2f}% | "
                              f"Total R: {profit_indicator}{abs(total_r):6.2f} | Avg R: {avg_r:6.2f} | Expectancy: {expectancy:6.2f}{profit_factor_str}")
            else:
                print("No valid long trades to analyze by day of week with filters.")
                
            print("\n" + "-" * 50)
            print(" " * 15 + "SHORT TRADES (Filtered)")
            print("-" * 50)
            
            # Filter for short trades only
            short_trades_df = filtered_valid_trades[filtered_valid_trades['trade_direction'] == "short"]
            
            if not short_trades_df.empty:
                for day in weekday_order:
                    if day in short_trades_df['day_of_week'].values:
                        day_data = short_trades_df[short_trades_df['day_of_week'] == day]
                        
                        trade_count = len(day_data)
                        win_rate, _, _, expectancy = calculate_expectancy(day_data)
                        tp_rate = (day_data["exit_reason"] == "TP").mean() * 100
                        total_r = day_data['pnl_R'].sum()
                        avg_r = day_data['pnl_R'].mean()
                        
                        # Calculate profit factor if requested
                        profit_factor_str = ""
                        if params["calculate_profit_factor"]:
                            profit_factor = calculate_profit_factor(day_data)
                            profit_factor_str = f" | Profit Factor: {profit_factor:6.2f}"
                        
                        # Profit indicator
                        profit_indicator = "+" if total_r > 0 else "-" if total_r < 0 else " "
                        
                        # Format the output line
                        print(f"{day:<10} | Trades: {trade_count:3d} | Profit Rate: {win_rate:6.2f}% | TP Rate: {tp_rate:6.2f}% | "
                              f"Total R: {profit_indicator}{abs(total_r):6.2f} | Avg R: {avg_r:6.2f} | Expectancy: {expectancy:6.2f}{profit_factor_str}")
            else:
                print("No valid short trades to analyze by day of week with filters.")
        
        # If analysis_mode is "long" or "short", keep the original logic but for filtered data
        else:
            for day in weekday_order:
                if day in filtered_valid_trades['day_of_week'].values:
                    day_data = filtered_valid_trades[filtered_valid_trades['day_of_week'] == day]
                    
                    trade_count = len(day_data)
                    win_rate, _, _, expectancy = calculate_expectancy(day_data)
                    tp_rate = (day_data["exit_reason"] == "TP").mean() * 100
                    total_r = day_data['pnl_R'].sum()
                    avg_r = day_data['pnl_R'].mean()
                    
                    # Calculate profit factor if requested
                    profit_factor_str = ""
                    if params["calculate_profit_factor"]:
                        profit_factor = calculate_profit_factor(day_data)
                        profit_factor_str = f" | Profit Factor: {profit_factor:6.2f}"
                    
                    # Profit indicator
                    profit_indicator = "+" if total_r > 0 else "-" if total_r < 0 else " "
                    
                    # Format the output line
                    print(f"{day:<10} | Trades: {trade_count:3d} | Profit Rate: {win_rate:6.2f}% | TP Rate: {tp_rate:6.2f}% | "
                          f"Total R: {profit_indicator}{abs(total_r):6.2f} | Avg R: {avg_r:6.2f} | Expectancy: {expectancy:6.2f}{profit_factor_str}")

def display_top_configs(results_df, filtered_df, unfiltered_valid_trades_df, filtered_valid_trades, params):
    """Display top configurations"""
    if not params["show_top_configs"]:
        return
    
    print("\n" + "=" * 100)
    print(" " * 30 + f"TOP {params['top_configs']} MOST PROFITABLE CONFIGURATIONS")
    print("=" * 100)
    
    # First show unfiltered top configurations
    if params["use_wdr_filter"] or params["use_m7b_filter"]:
        print("\n--- WITHOUT FILTERS ---")
    
    # Determine if we're using cluster-based or SD-based approach
    if 'candidate_cluster' in results_df.columns:
        # Group for cluster-based statistics
        unfiltered_grouped = unfiltered_results_df.groupby(['day_of_week', 'confirm_time_str', 'trade_direction', 'candidate_cluster'])
    else:
        # Group for SD-based statistics
        unfiltered_grouped = unfiltered_results_df.groupby(['day_of_week', 'confirm_time_str', 'trade_direction', 'sd_retracement', 'target_sd'])
    
    # Calculate metrics for each configuration (unfiltered)
    config_metrics_df = None
    filtered_config_metrics_df = None
    
    if not unfiltered_valid_trades_df.empty:
        config_metrics = []
        
        for name, group in unfiltered_grouped:
            if 'candidate_cluster' in results_df.columns:
                day, conf_time, direction, cluster = name
                grouping_key = {'day': day, 'time': conf_time, 'direction': direction, 'cluster': cluster}
            else:
                day, conf_time, direction, sd_retrace, target_sd = name
                grouping_key = {'day': day, 'time': conf_time, 'direction': direction, 
                                'sd_retracement': sd_retrace, 'target_sd': target_sd}
                
            valid = group[group["trade_entry"].notna()]
            valid_count = len(valid)
            
            if valid_count >= params["min_trades"]:  # Only consider configurations with at least min_trades_for_analysis trades
                # Use corrected expectancy calculation
                win_rate, avg_winner, avg_loser, expectancy = calculate_expectancy(valid)
                total_r = valid["pnl_R"].sum()
                
                # Add profit factor if requested
                if params["calculate_profit_factor"]:
                    profit_factor = calculate_profit_factor(valid)
                else:
                    profit_factor = None
                
                config_entry = grouping_key.copy()
                config_entry.update({
                    'trades': valid_count,
                    'win_rate': win_rate,
                    'total_r': total_r,
                    'expectancy': expectancy,
                    'profit_factor': profit_factor
                })
                config_metrics.append(config_entry)
        
        # Sort by total R (most profitable first)
        config_metrics_df = pd.DataFrame(config_metrics)
        if not config_metrics_df.empty:
            sorted_configs = config_metrics_df.sort_values('total_r', ascending=False).head(params["top_configs"])
            
            # Print the top configurations
            for i, row in sorted_configs.iterrows():
                # Profit indicator
                profit_indicator = "+" if row['total_r'] > 0 else "-" if row['total_r'] < 0 else " "
                exp_indicator = "+" if row['expectancy'] > 0 else "-" if row['expectancy'] < 0 else " "
                
                if 'cluster' in row:  # Cluster-based approach
                    output_line = (f"#{i+1:2d}: {row['day']:<9} at {row['time']} ({row['direction'].capitalize():<5}, "
                          f"Cluster: {float(row['cluster']):+.2f}): "
                          f"Trades: {row['trades']:3d}, Profit Rate: {row['win_rate']:6.2f}%, "
                          f"Total R: {profit_indicator}{abs(row['total_r']):6.2f}, Expectancy: {exp_indicator}{abs(row['expectancy']):5.2f}")
                else:  # SD-based approach
                    output_line = (f"#{i+1:2d}: {row['day']:<9} at {row['time']} ({row['direction'].capitalize():<5}, "
                          f"SD Retrace: {float(row['sd_retracement']):+.2f}, Target SD: {float(row['target_sd']):+.2f}): "
                          f"Trades: {row['trades']:3d}, Profit Rate: {row['win_rate']:6.2f}%, "
                          f"Total R: {profit_indicator}{abs(row['total_r']):6.2f}, Expectancy: {exp_indicator}{abs(row['expectancy']):5.2f}")
                
                if params["calculate_profit_factor"] and 'profit_factor' in row and row['profit_factor'] is not None:
                    output_line += f", Profit Factor: {row['profit_factor']:.2f}"
                
                print(output_line)
        else:
            print("No configurations with sufficient trades to analyze.")
    else:
        print("No valid trades found for analysis.")
    
    # Then show filtered top configurations (if enabled)
    if (params["use_wdr_filter"] or params["use_m7b_filter"]) and filtered_valid_trades is not None and not filtered_valid_trades.empty:
        print("\n--- WITH FILTERS ---")
        
        # Group and calculate metrics for filtered configurations
        if 'candidate_cluster' in filtered_df.columns:
            # Group for cluster-based statistics
            filtered_grouped = filtered_df.groupby(['day_of_week', 'confirm_time_str', 'trade_direction', 'candidate_cluster'])
        else:
            # Group for SD-based statistics
            filtered_grouped = filtered_df.groupby(['day_of_week', 'confirm_time_str', 'trade_direction', 'sd_retracement', 'target_sd'])
            
        filtered_config_metrics = []
        
        for name, group in filtered_grouped:
            if 'candidate_cluster' in filtered_df.columns:
                day, conf_time, direction, cluster = name
                grouping_key = {'day': day, 'time': conf_time, 'direction': direction, 'cluster': cluster}
            else:
                day, conf_time, direction, sd_retrace, target_sd = name
                grouping_key = {'day': day, 'time': conf_time, 'direction': direction, 
                                'sd_retracement': sd_retrace, 'target_sd': target_sd}
                
            valid = group[group["trade_entry"].notna()]
            valid_count = len(valid)
            
            if valid_count >= params["min_trades"]:
                # Use corrected expectancy calculation
                win_rate, avg_winner, avg_loser, expectancy = calculate_expectancy(valid)
                total_r = valid["pnl_R"].sum()
                
                # Add profit factor if requested
                if params["calculate_profit_factor"]:
                    profit_factor = calculate_profit_factor(valid)
                else:
                    profit_factor = None
                
                config_entry = grouping_key.copy()
                config_entry.update({
                    'trades': valid_count,
                    'win_rate': win_rate,
                    'total_r': total_r,
                    'expectancy': expectancy,
                    'profit_factor': profit_factor
                })
                filtered_config_metrics.append(config_entry)
        
        # Sort by total R (most profitable first)
        filtered_config_metrics_df = pd.DataFrame(filtered_config_metrics)
        if not filtered_config_metrics_df.empty:
            filtered_sorted_configs = filtered_config_metrics_df.sort_values('total_r', ascending=False).head(params["top_configs"])
            
            # Print the top filtered configurations
            for i, row in filtered_sorted_configs.iterrows():
                # Profit indicator
                profit_indicator = "+" if row['total_r'] > 0 else "-" if row['total_r'] < 0 else " "
                exp_indicator = "+" if row['expectancy'] > 0 else "-" if row['expectancy'] < 0 else " "
                
                if 'cluster' in row:  # Cluster-based approach
                    output_line = (f"#{i+1:2d}: {row['day']:<9} at {row['time']} ({row['direction'].capitalize():<5}, "
                          f"Cluster: {float(row['cluster']):+.2f}): "
                          f"Trades: {row['trades']:3d}, Profit Rate: {row['win_rate']:6.2f}%, "
                          f"Total R: {profit_indicator}{abs(row['total_r']):6.2f}, Expectancy: {exp_indicator}{abs(row['expectancy']):5.2f}")
                else:  # SD-based approach
                    output_line = (f"#{i+1:2d}: {row['day']:<9} at {row['time']} ({row['direction'].capitalize():<5}, "
                          f"SD Retrace: {float(row['sd_retracement']):+.2f}, Target SD: {float(row['target_sd']):+.2f}): "
                          f"Trades: {row['trades']:3d}, Profit Rate: {row['win_rate']:6.2f}%, "
                          f"Total R: {profit_indicator}{abs(row['total_r']):6.2f}, Expectancy: {exp_indicator}{abs(row['expectancy']):5.2f}")
                
                if params["calculate_profit_factor"] and 'profit_factor' in row and row['profit_factor'] is not None:
                    output_line += f", Profit Factor: {row['profit_factor']:.2f}"
                
                print(output_line)
        else:
            print("No configurations with sufficient trades to analyze after filtering.")
    
    return config_metrics_df, filtered_config_metrics_df

def display_target_configs(config_metrics_df, filtered_config_metrics_df, params):
    """Display configurations meeting target criteria"""
    if not params["show_target_configs"] or (params["target_win_rate"] is None and params["target_expectancy"] is None):
        return
    
    print("\n" + "=" * 100)
    print(" " * 30 + "CONFIGURATIONS MEETING TARGET CRITERIA")
    print("=" * 100)
    print(f"Target Win Rate: {params['target_win_rate']}%, Target Expectancy: {params['target_expectancy']}R")
    
    # First show unfiltered configurations meeting criteria
    if params["use_wdr_filter"] or params["use_m7b_filter"]:
        print("\n--- WITHOUT FILTERS ---")
    
    # Filter configurations meeting criteria (unfiltered)
    if config_metrics_df is not None and not config_metrics_df.empty:
        criteria_configs = config_metrics_df.copy()
        
        if params["target_win_rate"] is not None:
            criteria_configs = criteria_configs[criteria_configs['win_rate'] >= params["target_win_rate"]]
        if params["target_expectancy"] is not None:
            criteria_configs = criteria_configs[criteria_configs['expectancy'] >= params["target_expectancy"]]
        
        if not criteria_configs.empty:
            # Sort by total R
            sorted_criteria = criteria_configs.sort_values('total_r', ascending=False)
            
            for i, row in sorted_criteria.iterrows():
                # Profit indicator
                profit_indicator = "+" if row['total_r'] > 0 else "-" if row['total_r'] < 0 else " "
                exp_indicator = "+" if row['expectancy'] > 0 else "-" if row['expectancy'] < 0 else " "
                
                if 'cluster' in row:  # Cluster-based approach
                    output_line = (f"{row['day']:<9} at {row['time']} ({row['direction'].capitalize():<5}, "
                          f"Cluster: {float(row['cluster']):+.2f}): "
                          f"Trades: {row['trades']:3d}, Profit Rate: {row['win_rate']:6.2f}%, "
                          f"Total R: {profit_indicator}{abs(row['total_r']):6.2f}, Expectancy: {exp_indicator}{abs(row['expectancy']):5.2f}")
                else:  # SD-based approach
                    output_line = (f"{row['day']:<9} at {row['time']} ({row['direction'].capitalize():<5}, "
                          f"SD Retrace: {float(row['sd_retracement']):+.2f}, Target SD: {float(row['target_sd']):+.2f}): "
                          f"Trades: {row['trades']:3d}, Profit Rate: {row['win_rate']:6.2f}%, "
                          f"Total R: {profit_indicator}{abs(row['total_r']):6.2f}, Expectancy: {exp_indicator}{abs(row['expectancy']):5.2f}")
                
                if params["calculate_profit_factor"] and 'profit_factor' in row and row['profit_factor'] is not None:
                    output_line += f", Profit Factor: {row['profit_factor']:.2f}"
                
                print(output_line)
        else:
            print("No configurations meet the target criteria.")
    else:
        print("No configuration metrics available for target filtering.")
    
    # Then show filtered configurations meeting criteria (if enabled)
    if (params["use_wdr_filter"] or params["use_m7b_filter"]) and filtered_config_metrics_df is not None and not filtered_config_metrics_df.empty:
        print("\n--- WITH FILTERS ---")
        
        filtered_criteria = filtered_config_metrics_df.copy()
        
        if params["target_win_rate"] is not None:
            filtered_criteria = filtered_criteria[filtered_criteria['win_rate'] >= params["target_win_rate"]]
        if params["target_expectancy"] is not None:
            filtered_criteria = filtered_criteria[filtered_criteria['expectancy'] >= params["target_expectancy"]]
        
        if not filtered_criteria.empty:
            # Sort by total R
            sorted_filtered_criteria = filtered_criteria.sort_values('total_r', ascending=False)
            
            for i, row in sorted_filtered_criteria.iterrows():
                # Profit indicator
                profit_indicator = "+" if row['total_r'] > 0 else "-" if row['total_r'] < 0 else " "
                exp_indicator = "+" if row['expectancy'] > 0 else "-" if row['expectancy'] < 0 else " "
                
                if 'cluster' in row:  # Cluster-based approach
                    output_line = (f"{row['day']:<9} at {row['time']} ({row['direction'].capitalize():<5}, "
                          f"Cluster: {float(row['cluster']):+.2f}): "
                          f"Trades: {row['trades']:3d}, Profit Rate: {row['win_rate']:6.2f}%, "
                          f"Total R: {profit_indicator}{abs(row['total_r']):6.2f}, Expectancy: {exp_indicator}{abs(row['expectancy']):5.2f}")
                else:  # SD-based approach
                    output_line = (f"{row['day']:<9} at {row['time']} ({row['direction'].capitalize():<5}, "
                          f"SD Retrace: {float(row['sd_retracement']):+.2f}, Target SD: {float(row['target_sd']):+.2f}): "
                          f"Trades: {row['trades']:3d}, Profit Rate: {row['win_rate']:6.2f}%, "
                          f"Total R: {profit_indicator}{abs(row['total_r']):6.2f}, Expectancy: {exp_indicator}{abs(row['expectancy']):5.2f}")
                
                if params["calculate_profit_factor"] and 'profit_factor' in row and row['profit_factor'] is not None:
                    output_line += f", Profit Factor: {row['profit_factor']:.2f}"
                
                print(output_line)
        else:
            print("No configurations meet the target criteria after filtering.")

def display_filter_stats(results_df, params):
    """Display filter statistics"""
    if not params["show_filter_stats"]:
        return
    
    weekday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']
    
    # Print M7B statistics if filter is enabled
    if params["use_m7b_filter"] and 'm7b_invalid' in results_df.columns:
        # Fixed: Use fillna to handle NaN values in boolean filtering
        m7b_counts = results_df[results_df['m7b_invalid'].fillna(False) & results_df['unfiltered_trade_entry'].notna()].shape[0]
        
        print("\n" + "=" * 100)
        print(" " * 35 + "M7B FILTER STATISTICS")
        print("=" * 100)
        
        # Print overall filter count
        unfiltered_total = results_df['unfiltered_trade_entry'].notna().sum()
        filtered_total = results_df['trade_entry'].notna().sum()
        filtered_out = unfiltered_total - filtered_total
        pct_filtered = (filtered_out / unfiltered_total * 100) if unfiltered_total > 0 else 0
        
        print(f"\nM7B filter removed {filtered_out} of {unfiltered_total} trades ({pct_filtered:.2f}%)")
        
        # Print sample filtered trades
        if 'm7b_upper' in results_df.columns and 'm7b_lower' in results_df.columns:
            # Fixed: Use fillna in boolean filtering
            sample_data = results_df[results_df['m7b_invalid'].fillna(False) & results_df['unfiltered_trade_entry'].notna()].head(5)
            if not sample_data.empty:
                print("\nSample of filtered trades (first 5):")
                for _, row in sample_data.iterrows():
                    direction = row['trade_direction']
                    entry = row['unfiltered_trade_entry']
                    if pd.notna(entry):  # Make sure entry is not NaN
                        if direction == 'long':
                            boundary = row['m7b_lower']
                            if pd.notna(boundary):  # Make sure boundary is not NaN
                                print(f"Long trade entry: {entry:.2f}, M7B lower: {boundary:.2f}, Entry < M7B lower: {entry < boundary}")
                        else:
                            boundary = row['m7b_upper']
                            if pd.notna(boundary):  # Make sure boundary is not NaN
                                print(f"Short trade entry: {entry:.2f}, M7B upper: {boundary:.2f}, Entry > M7B upper: {entry > boundary}")
        
        # Print by reason if available
        if 'm7b_reason' in results_df.columns:
            # Count invalidation reasons - Fixed: Use fillna in boolean filtering
            m7b_invalid_trades = results_df[results_df['m7b_invalid'].fillna(False) & results_df['unfiltered_trade_entry'].notna()]
            if not m7b_invalid_trades.empty:
                reason_counts = m7b_invalid_trades['m7b_reason'].value_counts()
                print("\nM7B invalidation reasons:")
                for reason, count in reason_counts.items():
                    if reason and pd.notna(reason) and count > 0:  # Only print non-empty reasons
                        pct = count / m7b_counts * 100 if m7b_counts > 0 else 0
                        print(f"  - {reason}: {count} ({pct:.2f}%)")
        
        # By day of week
        print("\nM7B filtered trades by day of week:")
        # Group by day of week and count trades and filtered trades - Fixed: Use fillna in boolean filtering
        day_counts = {}
        for day in weekday_order:
            day_trades = results_df[(results_df['day_of_week'] == day) & results_df['unfiltered_trade_entry'].notna()]
            # Use fillna to handle NaN values in boolean filtering
            day_filtered = day_trades[day_trades['m7b_invalid'].fillna(False)].shape[0]
            day_total = day_trades.shape[0]
            if day_total > 0:
                day_counts[day] = (day_filtered, day_total, day_filtered/day_total*100)
        
        for day, (filtered, total, pct) in day_counts.items():
            print(f"  - {day}: {filtered}/{total} ({pct:.2f}%)")
        
        # By trade direction
        print("\nM7B filtered trades by direction:")
        # Group by trade direction and count trades and filtered trades - Fixed: Use fillna in boolean filtering
        direction_counts = {}
        for direction in ['long', 'short']:
            dir_trades = results_df[(results_df['trade_direction'] == direction) & results_df['unfiltered_trade_entry'].notna()]
            # Use fillna to handle NaN values in boolean filtering
            dir_filtered = dir_trades[dir_trades['m7b_invalid'].fillna(False)].shape[0]
            dir_total = dir_trades.shape[0]
            if dir_total > 0:
                direction_counts[direction] = (dir_filtered, dir_total, dir_filtered/dir_total*100)
        
        for direction, (filtered, total, pct) in direction_counts.items():
            print(f"  - {direction.capitalize()}: {filtered}/{total} ({pct:.2f}%)")

    # Print WDR statistics if filter is enabled
    if params["use_wdr_filter"] and 'wdr_session_invalid' in results_df.columns:
        session_counts = results_df['wdr_session_invalid'].sum()
        # Fixed: Use fillna in boolean filtering for wdr_cluster_invalid
        cluster_counts = results_df[~results_df['wdr_session_invalid'] & results_df['wdr_cluster_invalid'].fillna(False)].shape[0]
        
        print("\n" + "=" * 100)
        print(" " * 35 + "WDR FILTER STATISTICS")
        print("=" * 100)
        
        # Print by reason if available
        if 'wdr_reason' in results_df.columns:
            # Count session invalidation reasons
            session_invalid = results_df[results_df['wdr_session_invalid']]
            if not session_invalid.empty:
                reason_counts = session_invalid['wdr_reason'].value_counts()
                print("\nSession invalidation reasons:")
                for reason, count in reason_counts.items():
                    if reason and pd.notna(reason) and count > 0:  # Only print non-empty reasons
                        pct = count / session_counts * 100 if session_counts > 0 else 0
                        print(f"  - {reason}: {count} ({pct:.2f}%)")
            
            # Count cluster invalidation reasons - Fixed: Use fillna in boolean filtering
            cluster_invalid = results_df[~results_df['wdr_session_invalid'] & results_df['wdr_cluster_invalid'].fillna(False)]
            if not cluster_invalid.empty:
                cluster_reason_counts = cluster_invalid['wdr_reason'].value_counts()
                print("\nCluster invalidation reasons:")
                for reason, count in cluster_reason_counts.items():
                    if reason and pd.notna(reason) and count > 0:  # Only print non-empty reasons
                        pct = count / cluster_counts * 100 if cluster_counts > 0 else 0
                        print(f"  - {reason}: {count} ({pct:.2f}%)")
        
        # By day of week
        print("\nWDR filtered sessions by day of week:")
        day_total = results_df.groupby('day_of_week').size()
        day_invalid = session_invalid.groupby('day_of_week').size() if 'session_invalid' in locals() and not session_invalid.empty else pd.Series()
        
        for day in weekday_order:
            if day in day_total.index:
                invalid_count = day_invalid.get(day, 0)
                total_count = day_total[day]
                invalid_pct = invalid_count / total_count * 100 if total_count > 0 else 0
                print(f"  - {day}: {invalid_count}/{total_count} ({invalid_pct:.2f}%)")
        
        # By cluster level or SD retracement - Fixed: Use fillna in boolean filtering
        if 'cluster_invalid' in locals() and not cluster_invalid.empty:
            if 'candidate_cluster' in results_df.columns:
                print("\nClusters filtered by level:")
                cluster_configs = cluster_invalid.groupby(['candidate_cluster', 'trade_direction']).size()
                
                for (cluster, direction), count in sorted(cluster_configs.items()):
                    similar_configs = results_df[(results_df['candidate_cluster'] == cluster) & 
                                                (results_df['trade_direction'] == direction)]
                    total_count = len(similar_configs)
                    filter_pct = count / total_count * 100 if total_count > 0 else 0
                    print(f"  - Cluster {cluster:+.1f} ({direction.capitalize()}): {count}/{total_count} ({filter_pct:.2f}%)")
            else:
                print("\nSD Retracement filtered by level:")
                sd_configs = cluster_invalid.groupby(['sd_retracement', 'target_sd', 'trade_direction']).size()
                
                for (sd_retrace, target_sd, direction), count in sorted(sd_configs.items()):
                    similar_configs = results_df[(results_df['sd_retracement'] == sd_retrace) & 
                                                (results_df['target_sd'] == target_sd) &
                                                (results_df['trade_direction'] == direction)]
                    total_count = len(similar_configs)
                    filter_pct = count / total_count * 100 if total_count > 0 else 0
                    print(f"  - SD Retrace {sd_retrace:+.1f}, Target SD {target_sd:+.1f} ({direction.capitalize()}): {count}/{total_count} ({filter_pct:.2f}%)")

# =========================== MAIN FUNCTION ===========================

def main():
    # Get user parameters through interactive prompts
    params = get_user_input()
    
    # Import necessary visualization libraries if needed
    if params['generate_equity_curve'] or params['perform_param_sweep'] or params['perform_walk_forward']:
        if not MATPLOTLIB_AVAILABLE:
            print("Warning: Matplotlib is not available. Visualizations will be disabled.")
            params['generate_equity_curve'] = False
    
    # Create output directory if needed and doesn't exist
    os.makedirs(params['output_dir'], exist_ok=True)
    
    # Data Import & Preprocessing
    print(f"Loading data from {params['data_file']}...")
    cols = ["date_str", "time_str", "open", "high", "low", "close", "volume"]

    try:
        # Use GPU acceleration if enabled
        if params['use_gpu'] and GPU_AVAILABLE:
            print("Using GPU for data loading and preprocessing...")
            # Use cuDF to read the CSV if available, otherwise use CuPy + pandas
            try:
                if CUDF_AVAILABLE:
                    # cuDF version
                    df = cudf.read_csv(params['data_file'], delimiter=";", header=None, names=cols)
                    # Convert timestamp
                    df["timestamp"] = cudf.to_datetime(df["date_str"] + " " + df["time_str"], format="%d/%m/%Y %H:%M")
                    df = df.drop(columns=["date_str", "time_str"])
                    df["date"] = df["timestamp"].dt.date
                    
                    # Convert back to pandas for the rest of the processing since all our functions expect pandas
                    df = df.to_pandas()
                    df["timestamp"] = pd.to_datetime(df["timestamp"])
                    df["timestamp"] = df["timestamp"].dt.tz_localize("America/New_York")
                else:
                    # CuPy + pandas version (CuPy for computation, pandas for dataframes)
                    print("Using pandas for dataframes and CuPy for computations")
                    df = pd.read_csv(params['data_file'], delimiter=";", header=None, names=cols)
                    df["timestamp"] = pd.to_datetime(df["date_str"] + " " + df["time_str"], format="%d/%m/%Y %H:%M")
                    df.drop(columns=["date_str", "time_str"], inplace=True)
                    df["timestamp"] = df["timestamp"].dt.tz_localize("America/New_York")
                    df["date"] = df["timestamp"].dt.date
                    # We'll use CuPy for numerical computations later
            except Exception as e:
                print(f"Error using GPU for data loading: {e}")
                print("Falling back to CPU for data loading...")
                # Fall back to pandas
                df = pd.read_csv(params['data_file'], delimiter=";", header=None, names=cols)
                df["timestamp"] = pd.to_datetime(df["date_str"] + " " + df["time_str"], format="%d/%m/%Y %H:%M")
                df.drop(columns=["date_str", "time_str"], inplace=True)
                df["timestamp"] = df["timestamp"].dt.tz_localize("America/New_York")
                df["date"] = df["timestamp"].dt.date
        else:
            # Standard pandas loading
            df = pd.read_csv(params['data_file'], delimiter=";", header=None, names=cols)
            df["timestamp"] = pd.to_datetime(df["date_str"] + " " + df["time_str"], format="%d/%m/%Y %H:%M")
            df.drop(columns=["date_str", "time_str"], inplace=True)
            df["timestamp"] = df["timestamp"].dt.tz_localize("America/New_York")
            df["date"] = df["timestamp"].dt.date
    except Exception as e:
        print(f"Error loading data file: {e}")
        sys.exit(1)
    
    print(f"Loaded {len(df)} data points across {len(df['date'].unique())} trading days")
    
    # Filter by selected days if specified
    if params['selected_days'] is not None:
        print(f"Filtering for selected days: {params['selected_days']}")
        filtered_dates = []
        for date in df["date"].unique():
            if pd.Timestamp(date).day_name() in params['selected_days']:
                filtered_dates.append(date)
        df = df[df["date"].isin(filtered_dates)]
        print(f"After day filtering: {len(df)} data points across {len(df['date'].unique())} trading days")
    
    # Display strategy parameters
    print("\nStrategy Parameters:")
    if params['use_sd_based_entries']:
        print(f"Analysis Mode: {params['analysis_mode']}")
        print(f"Using SD-based entries")
        print(f"Default SD Retracement: {params['default_sd_retracement']}")
        print(f"Default Target SD: {params['default_target_sd']}")
    else:
        print(f"Analysis Mode: {params['analysis_mode']}")
        print(f"Using cluster-based entries")
        print(f"Cluster Levels: {params['cluster_levels']}")
        print(f"TP Multiple: {params['tp_multiple']}")
    
    print(f"SL Distance: {params['sl_distance']}")
    print(f"SL Extra Ticks: {params['sl_extra_ticks']}")
    print(f"Use WDR Filter: {params['use_wdr_filter']}")
    print(f"Use M7B Filter: {params['use_m7b_filter']}")
    print(f"Long Confirm Window: {params['long_confirm_start']} to {params['long_confirm_end']}")
    print(f"Short Confirm Window: {params['short_confirm_start']} to {params['short_confirm_end']}")
    
    # Choose analysis path based on options
    
    # Run parameter sweep if requested
    if params['perform_param_sweep']:
        top_configs = perform_parameter_sweep(df, params)
    
    # Run walk-forward analysis if requested
    if params['perform_walk_forward']:
        top_configs = None
        if params['perform_param_sweep']:
            # Already have top configs from parameter sweep
            pass
        else:
            # Need to find top configs first
            print("Finding optimal configurations before walk-forward analysis...")
            top_configs = perform_parameter_sweep(df, params)
        
        # Run walk-forward with top configs
        perform_walk_forward_analysis(df, params)
    
    # Run standard backtest if no special analysis requested
    if not params['perform_param_sweep'] and not params['perform_walk_forward']:
        # Process all sessions with current parameters
        print("\nRunning standard backtest with current parameters...")
        results = process_all_sessions(df, params['cluster_levels'], params)
        results_df = pd.DataFrame(results)
        
        if results_df.empty:
            print("No valid sessions were processed.")
            return
        
        # Create a copy of the data with filters applied
        filtered_df = results_df.copy()
        
        # Apply WDR filter if enabled
        if params["use_wdr_filter"] and 'wdr_session_invalid' in filtered_df.columns and 'wdr_cluster_invalid' in filtered_df.columns:
            # Track filtering statistics
            total_sessions = len(results_df)
            sessions_to_filter = filtered_df['wdr_session_invalid'].sum()
            # Use fillna to handle NaN values in boolean filtering
            clusters_to_filter = filtered_df[~filtered_df['wdr_session_invalid'] & filtered_df['wdr_cluster_invalid'].fillna(False)].shape[0]
            
            # Now apply the WDR filtering by setting trade values to NaN based on WDR rules
            session_mask = filtered_df['wdr_session_invalid']
            # Use fillna to handle NaN values in boolean filtering
            cluster_mask = filtered_df['wdr_cluster_invalid'].fillna(False)
            
            # Force these columns to NaN for filtered rows
            for col in ['trade_entry', 'SL', 'trade_exit', 'exit_reason', 'pnl_price', 'pnl_R', 'pnl_ticks']:
                filtered_df.loc[session_mask | cluster_mask, col] = np.nan
            
            # Print initial filtering stats
            print(f"\nWDR Filtering Applied:")
            print(f"  - {sessions_to_filter} sessions invalid ({sessions_to_filter/total_sessions*100 if total_sessions > 0 else 0:.2f}%)")
            print(f"  - {clusters_to_filter} additional clusters invalid")
            
            # Count trades before and after filtering
            unfiltered_trades = results_df['trade_entry'].notna().sum()
            filtered_trades = filtered_df['trade_entry'].notna().sum()
            trades_removed = unfiltered_trades - filtered_trades
            
            print(f"Trades affected: {trades_removed}/{unfiltered_trades} ({trades_removed/unfiltered_trades*100 if unfiltered_trades > 0 else 0:.2f}% removed)")
        
        # Display results based on enabled output sections
        # Note: Some functions return values needed by other functions, so we need to capture them
        unfiltered_valid_trades_df, filtered_valid_trades = display_overall_stats(results_df, filtered_df, params)
        display_day_performance(unfiltered_valid_trades_df, filtered_valid_trades, params)
        config_metrics_df, filtered_config_metrics_df = display_top_configs(results_df, filtered_df, unfiltered_valid_trades_df, filtered_valid_trades, params)
        display_target_configs(config_metrics_df, filtered_config_metrics_df, params)
        display_raw_data(results_df, params)
        display_filter_stats(results_df, params)
        
        # Generate equity curves if requested
        if params['generate_equity_curve']:
            print("\nGenerating equity curves...")
            if not unfiltered_valid_trades_df.empty:
                generate_equity_curve(unfiltered_valid_trades_df, params['output_dir'], "All_Trades_Unfiltered")
                
                # Generate separate curves for long and short trades if in 'both' mode
                if params["analysis_mode"] == "both":
                    long_trades = unfiltered_valid_trades_df[unfiltered_valid_trades_df['trade_direction'] == "long"]
                    short_trades = unfiltered_valid_trades_df[unfiltered_valid_trades_df['trade_direction'] == "short"]
                    
                    if not long_trades.empty:
                        generate_equity_curve(long_trades, params['output_dir'], "Long_Trades_Unfiltered")
                    if not short_trades.empty:
                        generate_equity_curve(short_trades, params['output_dir'], "Short_Trades_Unfiltered")
            
            if (params["use_wdr_filter"] or params["use_m7b_filter"]) and not filtered_valid_trades.empty:
                generate_equity_curve(filtered_valid_trades, params['output_dir'], "All_Trades_Filtered")
                
                # Generate separate curves for long and short trades if in 'both' mode
                if params["analysis_mode"] == "both":
                    long_trades = filtered_valid_trades[filtered_valid_trades['trade_direction'] == "long"]
                    short_trades = filtered_valid_trades[filtered_valid_trades['trade_direction'] == "short"]
                    
                    if not long_trades.empty:
                        generate_equity_curve(long_trades, params['output_dir'], "Long_Trades_Filtered")
                    if not short_trades.empty:
                        generate_equity_curve(short_trades, params['output_dir'], "Short_Trades_Filtered")
            
            # Generate equity curves for top configurations
            if config_metrics_df is not None and not config_metrics_df.empty:
                top_config = config_metrics_df.sort_values('total_r', ascending=False).iloc[0]
                
                if 'day' in top_config and 'time' in top_config and 'direction' in top_config:
                    day, time, direction = top_config['day'], top_config['time'], top_config['direction']
                    
                    if 'cluster' in top_config:  # Old approach
                        cluster = top_config['cluster']
                        
                        top_trades = unfiltered_valid_trades_df[
                            (unfiltered_valid_trades_df['day_of_week'] == day) & 
                            (unfiltered_valid_trades_df['confirm_time_str'] == time) & 
                            (unfiltered_valid_trades_df['trade_direction'] == direction) & 
                            (unfiltered_valid_trades_df['candidate_cluster'] == cluster)
                        ]
                        
                        if not top_trades.empty:
                            generate_equity_curve(top_trades, params['output_dir'], f"Top_Config_{day}_{time}_{direction}_Cluster{cluster}")
                    
                    elif 'sd_retracement' in top_config and 'target_sd' in top_config:  # New approach
                        sd_retracement = top_config['sd_retracement']
                        target_sd = top_config['target_sd']
                        
                        top_trades = unfiltered_valid_trades_df[
                            (unfiltered_valid_trades_df['day_of_week'] == day) & 
                            (unfiltered_valid_trades_df['confirm_time_str'] == time) & 
                            (unfiltered_valid_trades_df['trade_direction'] == direction) & 
                            (unfiltered_valid_trades_df['sd_retracement'] == sd_retracement) &
                            (unfiltered_valid_trades_df['target_sd'] == target_sd)
                        ]
                        
                        if not top_trades.empty:
                            generate_equity_curve(top_trades, params['output_dir'], 
                                               f"Top_Config_{day}_{time}_{direction}_SD{sd_retracement}_Target{target_sd}")
    
    print("\nAnalysis complete!")



# =========================== INTERACTIVE INTERFACE FUNCTIONS ===========================

def get_user_input():
    """Interactive parameter input from the user"""
    print("\n" + "=" * 60)
    print(" " * 15 + "ES TRADING STRATEGY CONFIGURATION")
    print("=" * 60)
    
    params = default_params.copy()
    
    # Main configuration categories
    categories = [
        "Core Strategy Parameters",
        "Filtering Options",
        "Time Windows",
        "Trade Parameters",
        "Analysis Parameters",
        "Output Controls",
        "Advanced Analysis Options",
        "Performance Options",
        "SD Parameter Options",  # NEW category for SD parameters
        "File Options",
        "Run with current settings"
    ]
    
    while True:
        print("\nConfiguration Categories:")
        for i, category in enumerate(categories, 1):
            print(f"{i}. {category}")
        
        try:
            choice = int(input("\nSelect a category (1-11): "))
            if not 1 <= choice <= len(categories):
                print("Invalid selection. Please try again.")
                continue
            
            # If user selects "Run with current settings"
            if choice == len(categories):
                break
                
            # Process each category
            if choice == 1:  # Core Strategy Parameters
                print("\n--- Core Strategy Parameters ---")
                
                # Analysis mode
                print(f"\nCurrent analysis mode: {params['analysis_mode']}")
                mode_choice = input("Select analysis mode (long/short/both) [leave blank to keep current]: ")
                if mode_choice in ["long", "short", "both"]:
                    params["analysis_mode"] = mode_choice
                
                # Cluster levels
                print(f"\nCurrent cluster levels: {params['cluster_levels']}")
                cluster_input = input("Enter cluster levels separated by spaces [leave blank to keep current]: ")
                if cluster_input.strip():
                    try:
                        params["cluster_levels"] = [float(x) for x in cluster_input.split()]
                        print(f"New cluster levels: {params['cluster_levels']}")
                    except ValueError:
                        print("Invalid input. Keeping current cluster levels.")
                
                # Selected days
                weekdays = ["Monday", "Tuesday", "Wednesday", "Thursday", "Friday"]
                current_days = params["selected_days"] if params["selected_days"] else "All days"
                print(f"\nCurrent selected days: {current_days}")
                print("Select days of week to analyze:")
                print("1. All days")
                print("2. Specific days")
                days_choice = input("Enter choice (1-2) [leave blank to keep current]: ")
                
                if days_choice == "1":
                    params["selected_days"] = None
                elif days_choice == "2":
                    selected = []
                    for day in weekdays:
                        include = input(f"Include {day}? (y/n): ").lower()
                        if include == "y":
                            selected.append(day)
                    if selected:
                        params["selected_days"] = selected
                        print(f"Selected days: {selected}")
                    else:
                        print("No days selected. Using all days.")
                        params["selected_days"] = None
                
                # NEW: Ask about using SD-based entries
                print(f"\nCurrent entry method: {'SD-based entries' if params['use_sd_based_entries'] else 'Cluster-based entries'}")
                sd_entry_choice = input("Use SD-based entries instead of cluster-based? (y/n) [leave blank to keep current]: ").lower()
                if sd_entry_choice in ["y", "n"]:
                    params["use_sd_based_entries"] = (sd_entry_choice == "y")
            
            elif choice == 2:  # Filtering Options
                print("\n--- Filtering Options ---")
                
                # WDR filter
                print(f"Current WDR filter status: {'Enabled' if params['use_wdr_filter'] else 'Disabled'}")
                wdr_choice = input("Enable WDR filter? (y/n) [leave blank to keep current]: ").lower()
                if wdr_choice in ["y", "n"]:
                    params["use_wdr_filter"] = (wdr_choice == "y")
                
                # M7B filter
                print(f"Current M7B filter status: {'Enabled' if params['use_m7b_filter'] else 'Disabled'}")
                m7b_choice = input("Enable M7B filter? (y/n) [leave blank to keep current]: ").lower()
                if m7b_choice in ["y", "n"]:
                    params["use_m7b_filter"] = (m7b_choice == "y")
            
            elif choice == 3:  # Time Windows
                print("\n--- Time Windows ---")
                
                # Long confirmation window
                print(f"Current long confirmation window: {params['long_confirm_start']} to {params['long_confirm_end']}")
                long_start = input("Long confirmation start time (HH:MM) [leave blank to keep current]: ")
                if long_start.strip():
                    params["long_confirm_start"] = long_start
                
                long_end = input("Long confirmation end time (HH:MM) [leave blank to keep current]: ")
                if long_end.strip():
                    params["long_confirm_end"] = long_end
                
                # Short confirmation window
                print(f"Current short confirmation window: {params['short_confirm_start']} to {params['short_confirm_end']}")
                short_start = input("Short confirmation start time (HH:MM) [leave blank to keep current]: ")
                if short_start.strip():
                    params["short_confirm_start"] = short_start
                
                short_end = input("Short confirmation end time (HH:MM) [leave blank to keep current]: ")
                if short_end.strip():
                    params["short_confirm_end"] = short_end
            
            elif choice == 4:  # Trade Parameters
                print("\n--- Trade Parameters ---")
                
                if not params["use_sd_based_entries"]:
                    # Original parameters if not using SD-based entries
                    # TP multiple
                    print(f"Current TP multiple: {params['tp_multiple']}")
                    tp_input = input("TP multiple of SD [leave blank to keep current]: ")
                    if tp_input.strip():
                        try:
                            params["tp_multiple"] = float(tp_input)
                        except ValueError:
                            print("Invalid input. Keeping current value.")
                else:
                    # New target SD parameter for SD-based entries
                    print(f"Current Target SD: {params['default_target_sd']}")
                    target_sd_input = input("Target SD value [leave blank to keep current]: ")
                    if target_sd_input.strip():
                        try:
                            params["default_target_sd"] = float(target_sd_input)
                        except ValueError:
                            print("Invalid input. Keeping current value.")
                
                # SL distance
                print(f"Current SL distance: {params['sl_distance']}")
                sl_input = input("SL distance as multiple of SD [leave blank to keep current]: ")
                if sl_input.strip():
                    try:
                        params["sl_distance"] = float(sl_input)
                    except ValueError:
                        print("Invalid input. Keeping current value.")
                
                # SL extra ticks
                print(f"Current SL extra ticks: {params['sl_extra_ticks']}")
                ticks_input = input("Additional ticks to add to stop loss [leave blank to keep current]: ")
                if ticks_input.strip():
                    try:
                        params["sl_extra_ticks"] = int(ticks_input)
                    except ValueError:
                        print("Invalid input. Keeping current value.")
            
            elif choice == 5:  # Analysis Parameters
                print("\n--- Analysis Parameters ---")
                
                # Minimum trades
                print(f"Current minimum trades for config analysis: {params['min_trades']}")
                min_trades_input = input("Minimum trades for a configuration to be considered [leave blank to keep current]: ")
                if min_trades_input.strip():
                    try:
                        params["min_trades"] = int(min_trades_input)
                    except ValueError:
                        print("Invalid input. Keeping current value.")
                
                # Top configs
                print(f"Current number of top configs to show: {params['top_configs']}")
                top_configs_input = input("Number of top configurations to display [leave blank to keep current]: ")
                if top_configs_input.strip():
                    try:
                        params["top_configs"] = int(top_configs_input)
                    except ValueError:
                        print("Invalid input. Keeping current value.")
                
                # NEW: Keep top configs
                print(f"Current number of top configs to keep for each day/time: {params['keep_top_configs']}")
                keep_top_input = input("Number of top configurations to keep for each day/time/direction [leave blank to keep current]: ")
                if keep_top_input.strip():
                    try:
                        params["keep_top_configs"] = int(keep_top_input)
                    except ValueError:
                        print("Invalid input. Keeping current value.")
                
                # Target win rate
                print(f"Current target win rate: {params['target_win_rate']}%")
                win_rate_input = input("Target win rate for filtering (%) [leave blank to keep current]: ")
                if win_rate_input.strip():
                    try:
                        params["target_win_rate"] = float(win_rate_input)
                    except ValueError:
                        print("Invalid input. Keeping current value.")
                
                # Target expectancy
                print(f"Current target expectancy: {params['target_expectancy']}R")
                expectancy_input = input("Target expectancy for filtering (R) [leave blank to keep current]: ")
                if expectancy_input.strip():
                    try:
                        params["target_expectancy"] = float(expectancy_input)
                    except ValueError:
                        print("Invalid input. Keeping current value.")
                
                # NEW: Max R reduction percentage
                print(f"Current maximum allowed R reduction: {params['max_r_reduction_pct']}%")
                max_r_input = input("Maximum allowed R reduction percentage [leave blank to keep current]: ")
                if max_r_input.strip():
                    try:
                        params["max_r_reduction_pct"] = float(max_r_input)
                    except ValueError:
                        print("Invalid input. Keeping current value.")
            
            elif choice == 6:  # Output Controls
                print("\n--- Output Controls ---")
                output_options = [
                    ("show_raw_data", "Show raw configuration data"),
                    ("show_overall_stats", "Show overall statistics"),
                    ("show_day_performance", "Show day of week performance"),
                    ("show_top_configs", "Show top configurations"),
                    ("show_target_configs", "Show configurations meeting target criteria"),
                    ("show_filter_stats", "Show filter statistics")
                ]
                
                print("Toggle output sections:")
                for i, (param_name, description) in enumerate(output_options, 1):
                    status = "Enabled" if params[param_name] else "Disabled"
                    print(f"{i}. {description}: {status}")
                
                option_choice = input("\nSelect option to toggle (1-6) or 'a' for all, 'n' for none [leave blank to keep current]: ")
                if option_choice.strip():
                    if option_choice.lower() == "a":
                        for param_name, _ in output_options:
                            params[param_name] = True
                        print("All output sections enabled.")
                    elif option_choice.lower() == "n":
                        for param_name, _ in output_options:
                            params[param_name] = False
                        print("All output sections disabled.")
                    elif option_choice.isdigit() and 1 <= int(option_choice) <= len(output_options):
                        idx = int(option_choice) - 1
                        param_name = output_options[idx][0]
                        params[param_name] = not params[param_name]
                        status = "Enabled" if params[param_name] else "Disabled"
                        print(f"{output_options[idx][1]}: {status}")
            
            elif choice == 7:  # Advanced Analysis Options
                if not MATPLOTLIB_AVAILABLE and (params['generate_equity_curve'] or params['perform_param_sweep'] or params['perform_walk_forward']):
                    print("\nWARNING: Matplotlib not available. Visualization features will be disabled.")
                
                print("\n--- Advanced Analysis Options ---")
                advanced_options = [
                    ("calculate_profit_factor", "Calculate profit factor (gross profits/gross losses)"),
                    ("analyze_streaks", "Analyze winning and losing streaks"),
                    ("perform_param_sweep", "Perform parameter sweep to find optimal settings"),
                    ("perform_day_by_day_sweep", "Analyze parameter sweep separately for each day"),
                    ("perform_walk_forward", "Perform walk-forward analysis to test strategy robustness"),
                    ("generate_equity_curve", "Generate equity curve charts")
                ]
                
                print("Toggle advanced analysis options:")
                for i, (param_name, description) in enumerate(advanced_options, 1):
                    status = "Enabled" if params[param_name] else "Disabled"
                    print(f"{i}. {description}: {status}")
                
                option_choice = input(f"\nSelect option to toggle (1-{len(advanced_options)}) or 'a' for all, 'n' for none [leave blank to keep current]: ")
                if option_choice.strip():
                    if option_choice.lower() == "a":
                        for param_name, _ in advanced_options:
                            params[param_name] = True
                        print("All advanced analysis options enabled.")
                    elif option_choice.lower() == "n":
                        for param_name, _ in advanced_options:
                            params[param_name] = False
                        print("All advanced analysis options disabled.")
                    elif option_choice.isdigit() and 1 <= int(option_choice) <= len(advanced_options):
                        idx = int(option_choice) - 1
                        param_name = advanced_options[idx][0]
                        params[param_name] = not params[param_name]
                        status = "Enabled" if params[param_name] else "Disabled"
                        print(f"{advanced_options[idx][1]}: {status}")
            
            elif choice == 8:  # Performance Options
                print("\n--- Performance Options ---")
                
                # GPU acceleration
                if GPU_AVAILABLE:
                    print(f"Current GPU acceleration status: {'Enabled' if params['use_gpu'] else 'Disabled'}")
                    gpu_choice = input("Enable GPU acceleration? (y/n) [leave blank to keep current]: ").lower()
                    if gpu_choice in ["y", "n"]:
                        params["use_gpu"] = (gpu_choice == "y")
                else:
                    print("GPU acceleration not available on this system (RAPIDS/CuPy not installed)")
                    params["use_gpu"] = False
                
                # Multiprocessing
                print(f"Current multiprocessing status: {'Enabled' if params['use_multiprocessing'] else 'Disabled'}")
                mp_choice = input("Enable multiprocessing for faster calculations? (y/n) [leave blank to keep current]: ").lower()
                if mp_choice in ["y", "n"]:
                    params["use_multiprocessing"] = (mp_choice == "y")
                    
                # Maximum CPU cores
                print(f"Current max CPU cores: {params['max_cpu_cores']} (system has {mp.cpu_count()} cores)")
                max_cores_input = input(f"Maximum CPU cores to use (1-{mp.cpu_count()}) [leave blank to keep current]: ")
                if max_cores_input.strip():
                    try:
                        max_cores = int(max_cores_input)
                        if 1 <= max_cores <= mp.cpu_count():
                            params["max_cpu_cores"] = max_cores
                        else:
                            print(f"Invalid input. Value must be between 1 and {mp.cpu_count()}.")
                    except ValueError:
                        print("Invalid input. Keeping current value.")
                
                # CPU usage limit
                print(f"Current CPU usage limit: {params['cpu_usage_limit']}%")
                cpu_limit_input = input("Maximum CPU usage percentage (1-100) [leave blank to keep current]: ")
                if cpu_limit_input.strip():
                    try:
                        cpu_limit = int(cpu_limit_input)
                        if 1 <= cpu_limit <= 100:
                            params["cpu_usage_limit"] = cpu_limit
                        else:
                            print("Invalid input. Value must be between 1 and 100.")
                    except ValueError:
                        print("Invalid input. Keeping current value.")
                
                # Temperature limit
                print(f"Current temperature limit: {params['max_temperature']}째C")
                temp_limit_input = input("Maximum temperature in Celsius [leave blank to keep current]: ")
                if temp_limit_input.strip():
                    try:
                        temp_limit = int(temp_limit_input)
                        if 50 <= temp_limit <= 90:  # Safe range
                            params["max_temperature"] = temp_limit
                        else:
                            print("Invalid input. Value must be between 50 and 90.")
                    except ValueError:
                        print("Invalid input. Keeping current value.")
                
                # Temperature monitoring
                print(f"Current temperature monitoring: {'Enabled' if params['enable_temp_monitoring'] else 'Disabled'}")
                temp_monitor = input("Enable temperature monitoring? (y/n) [leave blank to keep current]: ").lower()
                if temp_monitor in ["y", "n"]:
                    params["enable_temp_monitoring"] = (temp_monitor == "y")
                
                # Cooling pause duration
                print(f"Current cooling pause duration: {params['cooling_pause_seconds']} seconds")
                pause_input = input("Cooling pause duration in seconds [leave blank to keep current]: ")
                if pause_input.strip():
                    try:
                        pause_seconds = int(pause_input)
                        if 1 <= pause_seconds <= 60:  # Reasonable range
                            params["cooling_pause_seconds"] = pause_seconds
                        else:
                            print("Invalid input. Value must be between 1 and 60.")
                    except ValueError:
                        print("Invalid input. Keeping current value.")
            
            elif choice == 9:  # NEW: SD Parameter Options
                print("\n--- SD Parameter Options ---")
                
                # SD retracement ranges
                print(f"Current SD retracement ranges: {params['sd_retracement_ranges']}")
                sd_ranges_input = input("Enter SD retracement values separated by spaces [leave blank to keep current]: ")
                if sd_ranges_input.strip():
                    try:
                        params["sd_retracement_ranges"] = [float(x) for x in sd_ranges_input.split()]
                        print(f"New SD retracement ranges: {params['sd_retracement_ranges']}")
                    except ValueError:
                        print("Invalid input. Keeping current values.")
                
                # Target SD ranges
                print(f"Current target SD ranges: {params['target_sd_ranges']}")
                target_ranges_input = input("Enter target SD values separated by spaces [leave blank to keep current]: ")
                if target_ranges_input.strip():
                    try:
                        params["target_sd_ranges"] = [float(x) for x in target_ranges_input.split()]
                        print(f"New target SD ranges: {params['target_sd_ranges']}")
                    except ValueError:
                        print("Invalid input. Keeping current values.")
                
                # Default SD retracement
                print(f"Current default SD retracement: {params['default_sd_retracement']}")
                default_sd_input = input("Default SD retracement value [leave blank to keep current]: ")
                if default_sd_input.strip():
                    try:
                        params["default_sd_retracement"] = float(default_sd_input)
                    except ValueError:
                        print("Invalid input. Keeping current value.")
                
                # Walk-forward using calendar years
                print(f"Current walk-forward method: {'Calendar years' if params['use_calendar_years'] else 'Trading days'}")
                calendar_choice = input("Use calendar years for walk-forward analysis? (y/n) [leave blank to keep current]: ").lower()
                if calendar_choice in ["y", "n"]:
                    params["use_calendar_years"] = (calendar_choice == "y")
                
                # In-sample years
                print(f"Current in-sample years: {params['in_sample_years']}")
                in_sample_input = input("Number of years for in-sample data [leave blank to keep current]: ")
                if in_sample_input.strip():
                    try:
                        params["in_sample_years"] = int(in_sample_input)
                    except ValueError:
                        print("Invalid input. Keeping current value.")
                
                # Out-sample years
                print(f"Current out-sample years: {params['out_sample_years']}")
                out_sample_input = input("Number of years for out-sample data [leave blank to keep current]: ")
                if out_sample_input.strip():
                    try:
                        params["out_sample_years"] = int(out_sample_input)
                    except ValueError:
                        print("Invalid input. Keeping current value.")
            
            elif choice == 10:  # File Options
                print("\n--- File Options ---")
                
                # Data file
                print(f"Current data file: {params['data_file']}")
                data_file = input("Input data file path [leave blank to keep current]: ")
                if data_file.strip():
                    params["data_file"] = data_file
                
                # Output directory
                print(f"Current output directory: {params['output_dir']}")
                output_dir = input("Directory to save output files [leave blank to keep current]: ")
                if output_dir.strip():
                    params["output_dir"] = output_dir
        
        except (ValueError, IndexError):
            print("Invalid input. Please try again.")
    
    # Final confirmation
    print("\n" + "=" * 60)
    print("CURRENT CONFIGURATION SUMMARY:")
    print("=" * 60)
    print(f"Analysis Mode: {params['analysis_mode']}")
    print(f"Selected Days: {params['selected_days'] if params['selected_days'] else 'All days'}")
    
    if params['use_sd_based_entries']:
        print(f"Using SD-based entries (Default SD retracement: {params['default_sd_retracement']}, Target SD: {params['default_target_sd']})")
    else:
        print(f"Using cluster-based entries with levels: {params['cluster_levels']}")
        print(f"TP Multiple: {params['tp_multiple']}")
    
    print(f"SL Distance: {params['sl_distance']}")
    print(f"SL Extra Ticks: {params['sl_extra_ticks']}")
    print(f"WDR Filter: {'Enabled' if params['use_wdr_filter'] else 'Disabled'}")
    print(f"M7B Filter: {'Enabled' if params['use_m7b_filter'] else 'Disabled'}")
    print(f"GPU Acceleration: {'Enabled' if params['use_gpu'] and GPU_AVAILABLE else 'Disabled'}")
    print(f"Multiprocessing: {'Enabled' if params['use_multiprocessing'] else 'Disabled'}")
    print(f"CPU Cores: {params['max_cpu_cores']} of {mp.cpu_count()} available")
    print(f"CPU Usage Limit: {params['cpu_usage_limit']}%")
    print(f"Max Temperature: {params['max_temperature']}째C {'(Monitoring Enabled)' if params['enable_temp_monitoring'] else '(Monitoring Disabled)'}")
    print(f"Long Confirmation Window: {params['long_confirm_start']} to {params['long_confirm_end']}")
    print(f"Short Confirmation Window: {params['short_confirm_start']} to {params['short_confirm_end']}")
    
    if params['perform_param_sweep']:
        print(f"Parameter Sweep: Enabled, keeping top {params['keep_top_configs']} configs per day/time/direction")
    
    if params['perform_walk_forward']:
        print(f"Walk-forward Analysis: Enabled, using {'calendar years' if params['use_calendar_years'] else 'trading days'}")
        print(f"  In-sample: {params['in_sample_years']} years, Out-sample: {params['out_sample_years']} years")
    
    confirm = input("\nRun analysis with these settings? (y/n): ").lower()
    if confirm != "y":
        print("Aborted by user.")
        sys.exit()
    
    return params

# =========================== UTILITY FUNCTIONS ===========================

def calculate_expectancy(valid_trades):
    """
    Calculate expectancy consistently based on profitable vs unprofitable trades.
    
    Parameters:
    valid_trades (DataFrame): DataFrame containing trade results with pnl_R column
    
    Returns:
    tuple: (win_rate, avg_winner, avg_loser, expectancy)
    """
    if valid_trades.empty:
        return 0, 0, 0, 0
    
    # Define winners and losers based on profit/loss
    winners = valid_trades[valid_trades["pnl_R"] > 0]
    losers = valid_trades[valid_trades["pnl_R"] <= 0]
    
    # Calculate win rate based on profitability (not TP hits)
    win_rate = (len(winners) / len(valid_trades)) * 100 if len(valid_trades) > 0 else 0
    
    # Calculate average winner and loser
    avg_winner = winners["pnl_R"].mean() if not winners.empty else 0
    avg_loser = losers["pnl_R"].mean() if not losers.empty else 0
    
    # Proper expectancy calculation
    expectancy = (win_rate/100) * avg_winner + (1 - win_rate/100) * avg_loser
    
    return win_rate, avg_winner, avg_loser, expectancy

def calculate_expectancy_gpu(valid_trades):
    """
    GPU-accelerated version of calculate_expectancy using CuPy
    """
    if valid_trades.empty:
        return 0, 0, 0, 0
    
    # Convert pnl_R to CuPy array for faster computation
    pnl_array = cp.array(valid_trades["pnl_R"].values)
    
    # Define winners and losers
    winners_mask = pnl_array > 0
    losers_mask = ~winners_mask
    
    # Calculate win rate
    win_rate = (cp.sum(winners_mask) / len(pnl_array)) * 100
    
    # Calculate average winner and loser
    winners = pnl_array[winners_mask]
    losers = pnl_array[losers_mask]
    
    avg_winner = cp.mean(winners).item() if len(winners) > 0 else 0
    avg_loser = cp.mean(losers).item() if len(losers) > 0 else 0
    
    # Expectancy calculation
    expectancy = (win_rate/100) * avg_winner + (1 - win_rate/100) * avg_loser
    
    return win_rate.item(), avg_winner, avg_loser, expectancy

def calculate_profit_factor(valid_trades):
    """Calculate profit factor (gross profits / gross losses)"""
    if valid_trades.empty:
        return 0
    
    gross_profits = valid_trades[valid_trades["pnl_R"] > 0]["pnl_R"].sum()
    gross_losses = abs(valid_trades[valid_trades["pnl_R"] < 0]["pnl_R"].sum())
    
    profit_factor = gross_profits / gross_losses if gross_losses != 0 else float('inf')
    return profit_factor

def analyze_streaks(valid_trades):
    """Analyze winning and losing streaks"""
    if valid_trades.empty:
        return None, None, None, None, None
    
    # Sort trades by date and time
    sorted_trades = valid_trades.sort_values(['date', 'confirm_time'])
    
    # Create list of wins (1) and losses (0)
    results = [1 if r > 0 else 0 for r in sorted_trades['pnl_R'].values]
    
    # Find streaks
    win_streaks = []
    loss_streaks = []
    
    current_streak = 1
    current_type = results[0] if len(results) > 0 else None
    
    for i in range(1, len(results)):
        if results[i] == results[i-1]:
            current_streak += 1
        else:
            if current_type == 1:
                win_streaks.append(current_streak)
            else:
                loss_streaks.append(current_streak)
            current_streak = 1
            current_type = results[i]
    
    # Add the last streak
    if current_type == 1:
        win_streaks.append(current_streak)
    elif current_type == 0:
        loss_streaks.append(current_streak)
    
    # Calculate streak statistics
    max_win_streak = max(win_streaks) if win_streaks else 0
    max_loss_streak = max(loss_streaks) if loss_streaks else 0
    avg_win_streak = sum(win_streaks) / len(win_streaks) if win_streaks else 0
    avg_loss_streak = sum(loss_streaks) / len(loss_streaks) if loss_streaks else 0
    
    return max_win_streak, max_loss_streak, avg_win_streak, avg_loss_streak, win_streaks + loss_streaks

def generate_equity_curve(valid_trades, output_dir, title):
    """Generate equity curve chart from trade data"""
    if valid_trades.empty:
        print("No valid trades to generate equity curve")
        return
    
    if not MATPLOTLIB_AVAILABLE:
        print("Matplotlib not available. Cannot generate equity curve.")
        return
    
    # Create output directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)
    
    # Sort trades by date and time
    sorted_trades = valid_trades.sort_values(['date', 'confirm_time'])
    
    # Calculate cumulative R
    sorted_trades['cumulative_R'] = sorted_trades['pnl_R'].cumsum()
    
    # Convert date to datetime for plotting
    sorted_trades['datetime'] = pd.to_datetime(sorted_trades['date']) + pd.to_timedelta(
        sorted_trades['confirm_time'].dt.hour * 3600 + 
        sorted_trades['confirm_time'].dt.minute * 60 + 
        sorted_trades['confirm_time'].dt.second, unit='s')
    
    # Create figure
    plt.figure(figsize=(12, 6))
    plt.plot(sorted_trades['datetime'], sorted_trades['cumulative_R'], 
             linestyle='-', marker='o', markersize=3)
    
    # Add moving average if enough points
    if len(sorted_trades) > 20:
        window = min(20, len(sorted_trades) // 5)
        sorted_trades['ma'] = sorted_trades['cumulative_R'].rolling(window=window).mean()
        plt.plot(sorted_trades['datetime'], sorted_trades['ma'], 'r--', 
                 label=f'{window}-Trade Moving Average')
    
    # Add drawdown
    running_max = sorted_trades['cumulative_R'].expanding().max()
    drawdown = sorted_trades['cumulative_R'] - running_max
    max_drawdown = drawdown.min()
    max_dd_idx = drawdown.idxmin()
    
    # Format the plot
    plt.grid(True, alpha=0.3)
    plt.title(f'Equity Curve - {title}')
    plt.xlabel('Date')
    plt.ylabel('Cumulative R')
    
    # Format x-axis dates
    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))
    plt.gcf().autofmt_xdate()
    
    # Add annotations
    total_r = sorted_trades['pnl_R'].sum()
    profit_factor = calculate_profit_factor(sorted_trades)
    win_rate, _, _, expectancy = calculate_expectancy(sorted_trades)
    
    annotation = (f'Total R: {total_r:.2f}\n'
                  f'Max Drawdown: {max_drawdown:.2f}R\n'
                  f'Profit Factor: {profit_factor:.2f}\n'
                  f'Win Rate: {win_rate:.1f}%\n'
                  f'Expectancy: {expectancy:.2f}R')
    
    plt.annotate(annotation, xy=(0.02, 0.02), xycoords='axes fraction', 
                 bbox=dict(boxstyle="round,pad=0.5", facecolor='white', alpha=0.8))
    
    if len(sorted_trades) > 20:
        plt.legend()
    
    # Mark maximum drawdown
    if max_dd_idx in sorted_trades.index:
        plt.plot(sorted_trades.loc[max_dd_idx, 'datetime'], 
                 sorted_trades.loc[max_dd_idx, 'cumulative_R'], 
                 'rv', markersize=8, label='Max Drawdown')
    
    # Save the chart
    filename = f"{output_dir}/equity_curve_{title.replace(' ', '_')}.png"
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"Equity curve saved to {filename}")
    
    return max_drawdown

# =========================== SD-BASED ENTRY FUNCTIONS ===========================

def get_confirmation_times(params):
    """Extract all possible confirmation times from parameters"""
    confirmation_times = []
    
    # Long confirmation times
    long_start = pd.to_datetime(params["long_confirm_start"]).time()
    long_end = pd.to_datetime(params["long_confirm_end"]).time()
    
    # Short confirmation times
    short_start = pd.to_datetime(params["short_confirm_start"]).time()
    short_end = pd.to_datetime(params["short_confirm_end"]).time()
    
    # Create a list of all possible 5-minute intervals in these ranges
    for hour in range(9, 16):  # 9 AM to 3 PM
        for minute in range(0, 60, 5):  # 5-minute intervals
            time_str = f"{hour:02d}:{minute:02d}"
            time_obj = pd.to_datetime(time_str).time()
            
            # Check if in long confirmation window
            if long_start <= time_obj <= long_end:
                confirmation_times.append(time_str)
            
            # Check if in short confirmation window and not already added
            elif short_start <= time_obj <= short_end and time_str not in confirmation_times:
                confirmation_times.append(time_str)
    
    return sorted(confirmation_times)

def get_long_session_metrics_with_sd_params(session_df, sd_retracement, params):
    """Process long trade setup for a session using SD retracement and Target SD parameters"""
    # Get time windows from parameters
    long_confirm_start = pd.to_datetime(params["long_confirm_start"]).time()
    long_confirm_end = pd.to_datetime(params["long_confirm_end"]).time()
    tick_size = params["tick_size"]
    
    # Get necessary price data
    dr_df = session_df[(session_df["timestamp"].dt.time >= pd.to_datetime("09:30").time()) &
                      (session_df["timestamp"].dt.time <= pd.to_datetime("10:25").time())]
    if dr_df.empty:
        return None
    
    DR_high = dr_df["high"].max()
    DR_low = dr_df["low"].min()
    IDR_high = dr_df["close"].max()
    IDR_low = dr_df["close"].min()
    SD = IDR_high - IDR_low if (IDR_high - IDR_low) != 0 else np.nan
    
    # Get the session date
    session_date = session_df["date"].iloc[0]
    day_of_week = pd.Timestamp(session_date).day_name()
    
    # Check WDR - get info, don't filter yet
    wdr_tuesday = None
    wdr_high = None
    wdr_low = None
    wdr_reason = None
    
    if params["use_wdr_filter"]:
        wdr_tuesday, wdr_info = get_applicable_wdr(session_date, tuesday_dates, tuesday_wdr)
        if wdr_info:
            wdr_high = wdr_info["dr_high"]
            wdr_low = wdr_info["dr_low"]
    
    # Calculate M7B (M7 Box)
    m7b_upper, m7b_lower = calculate_m7b(session_df)
    m7b_invalid = False
    m7b_reason = None
    
    # Check confirmation
    confirm_df = session_df[(session_df["timestamp"].dt.time >= long_confirm_start) &
                            (session_df["timestamp"].dt.time <= long_confirm_end)]
    confirm_signal = confirm_df[confirm_df["close"] > DR_high]
    if confirm_signal.empty:
        return None
    
    confirm_row = confirm_signal.iloc[0]
    confirm_time = confirm_row["timestamp"]
    confirm_time_str = confirm_time.strftime("%H:%M")
    
    # Check if close is above DR_low
    session_close = session_df.iloc[-1]["close"]
    if session_close < DR_low:
        return None
    
    # Calculate entry based on SD retracement
    baseline = IDR_high
    candidate_entry = baseline + sd_retracement * SD
    
    # Calculate TP based on Target SD
    target_sd = params.get("target_sd", params["default_target_sd"])  # Use target_sd if provided, else default
    TP = baseline + target_sd * SD
    
    # Find entry and simulate trade
    entry_window_df = session_df[(session_df["timestamp"].dt.time >= pd.to_datetime("10:40").time()) &
                                (session_df["timestamp"].dt.time <= pd.to_datetime("14:00").time()) &
                                (session_df["timestamp"] > confirm_time)]
    
    if entry_window_df.empty:
        return None
    
    # Find entry
    candidate_entries = entry_window_df[entry_window_df["low"] <= candidate_entry]
    if candidate_entries.empty:
        trade_entry = None
        trade_result = None
    else:
        # Process entry and simulate trade
        candidate_entry_index = candidate_entries.index[0]
        pre_candidate_df = entry_window_df.loc[:candidate_entry_index]
        
        # Check if TP has already been hit before entry
        if not pre_candidate_df.empty and pre_candidate_df["high"].max() >= TP:
            trade_entry = None
            trade_result = None
        else:
            # For long, check if price fell below threshold before entry
            check_df = entry_window_df.loc[entry_window_df.index < candidate_entry_index]
            long_threshold = candidate_entry - params["sl_distance"] * SD
            
            if not check_df.empty and check_df["low"].min() < long_threshold:
                trade_entry = None
                trade_result = None
            else:
                trade_entry = candidate_entry
                SL = trade_entry - params["sl_distance"] * SD - (params["sl_extra_ticks"] * tick_size)
                entry_time = entry_window_df.loc[candidate_entry_index, "timestamp"]
                
                # Check M7B filter if enabled
                if params["use_m7b_filter"] and m7b_lower is not None and candidate_entry < m7b_lower:
                    m7b_invalid = True
                    m7b_reason = "Entry below M7B lower boundary"
                
                # Simulate trade
                post_trade_df = session_df[(session_df["timestamp"] >= entry_time) &
                                          (session_df["timestamp"].dt.time <= pd.to_datetime("15:55").time())]
                
                if post_trade_df.empty:
                    trade_result = None
                else:
                    exit_price, exit_reason = simulate_trade(post_trade_df, trade_entry, TP, SL, "long", tick_size)
                    risk = trade_entry - SL
                    trade_result = {
                        "exit_reason": exit_reason,
                        "pnl_price": exit_price - trade_entry,
                        "pnl_R": (exit_price - trade_entry) / risk if risk != 0 else np.nan,
                        "pnl_ticks": (exit_price - trade_entry) / tick_size
                    }
    
    # Calculate post confirm price movement for analytics
    post_confirm_df = session_df[(session_df["timestamp"] > confirm_time) &
                                (session_df["timestamp"].dt.time <= pd.to_datetime("15:55").time())]
    
    if post_confirm_df.empty:
        return None
        
    max_high = post_confirm_df["high"].max()
    extreme_row = post_confirm_df[post_confirm_df["high"] == max_high].iloc[0]
    extension_value = max_high - baseline
    retracement_window = post_confirm_df[post_confirm_df["timestamp"] < extreme_row["timestamp"]]
    
    if retracement_window.empty:
        return None
        
    retracement_value = retracement_window["low"].min() - baseline
    extension_SD = extension_value / SD if pd.notna(SD) and SD != 0 else np.nan
    retracement_SD = retracement_value / SD if pd.notna(SD) and SD != 0 else np.nan
    
    # Check WDR conditions
    wdr_session_invalid = False
    wdr_cluster_invalid = False
    
    if params["use_wdr_filter"] and wdr_high is not None and wdr_low is not None:
        # Check if both IDR high and low are inside WDR
        if (wdr_low <= IDR_high <= wdr_high) and (wdr_low <= IDR_low <= wdr_high):
            wdr_session_invalid = True
            wdr_reason = "IDR inside WDR"
        # Check if TP is inside WDR
        elif wdr_low <= TP <= wdr_high:
            wdr_session_invalid = True
            wdr_reason = "TP inside WDR"
        # Check if cluster is inside WDR
        elif wdr_low <= candidate_entry <= wdr_high:
            wdr_cluster_invalid = True
            wdr_reason = "Entry inside WDR"
    
    # Build output
    output = {
        "date": session_date,
        "trade_direction": "long",
        "day_of_week": day_of_week,
        "confirm_time": confirm_time,
        "confirm_time_str": confirm_time_str,
        "DR_high": DR_high,
        "DR_low": DR_low,
        "IDR_high": IDR_high,
        "IDR_low": IDR_low,
        "SD": SD,
        "session_close": session_close,
        "baseline": baseline,
        "sd_retracement": sd_retracement,
        "target_sd": target_sd,
        "TP": TP,
        "retracement": retracement_value,
        "retracement_SD": retracement_SD,
        "extension": extension_value,
        "extension_SD": extension_SD,
        "wdr_session_invalid": wdr_session_invalid,
        "wdr_cluster_invalid": wdr_cluster_invalid,
        "wdr_reason": wdr_reason,
    }
    
    # Add M7B information to output
    if m7b_upper is not None and m7b_lower is not None:
        output.update({
            "m7b_upper": m7b_upper,
            "m7b_lower": m7b_lower,
            "m7b_invalid": m7b_invalid,
            "m7b_reason": m7b_reason,
        })
    
    if wdr_tuesday is not None:
        output.update({
            "wdr_tuesday": wdr_tuesday,
            "wdr_high": wdr_high,
            "wdr_low": wdr_low,
        })
    
    # Add trade results
    if trade_result is not None:
        # Store unfiltered values
        output_values = {
            "unfiltered_trade_entry": trade_entry,
            "unfiltered_exit_reason": trade_result["exit_reason"],
            "unfiltered_pnl_price": trade_result["pnl_price"],
            "unfiltered_pnl_R": trade_result["pnl_R"],
            "unfiltered_pnl_ticks": trade_result["pnl_ticks"]
        }
        
        # Store filtered values (NaN if m7b_invalid or wdr invalid)
        if m7b_invalid or wdr_session_invalid or wdr_cluster_invalid:
            output_values.update({
                "trade_entry": np.nan,
                "SL": np.nan,
                "trade_exit": np.nan,
                "exit_reason": np.nan,
                "pnl_price": np.nan,
                "pnl_R": np.nan,
                "pnl_ticks": np.nan
            })
        else:
            output_values.update({
                "trade_entry": trade_entry,
                "SL": SL,
                "trade_exit": exit_price,
                "exit_reason": trade_result["exit_reason"],
                "pnl_price": trade_result["pnl_price"],
                "pnl_R": trade_result["pnl_R"],
                "pnl_ticks": trade_result["pnl_ticks"]
            })
        
        output.update(output_values)
    else:
        output.update({
            "trade_entry": np.nan,
            "SL": np.nan,
            "trade_exit": np.nan,
            "exit_reason": np.nan,
            "pnl_price": np.nan,
            "pnl_R": np.nan,
            "pnl_ticks": np.nan,
            "unfiltered_trade_entry": np.nan,
            "unfiltered_exit_reason": np.nan,
            "unfiltered_pnl_price": np.nan,
            "unfiltered_pnl_R": np.nan,
            "unfiltered_pnl_ticks": np.nan
        })
    
    return output

def get_short_session_metrics(session_df, cluster, params, tuesday_dates, tuesday_wdr):
    """Process short trade setups for a session"""
    # Get time windows from parameters
    short_confirm_start = pd.to_datetime(params["short_confirm_start"]).time()
    short_confirm_end = pd.to_datetime(params["short_confirm_end"]).time()
    tick_size = params["tick_size"]
    
    dr_df = session_df[(session_df["timestamp"].dt.time >= pd.to_datetime("09:30").time()) &
                      (session_df["timestamp"].dt.time <= pd.to_datetime("10:25").time())]
    if dr_df.empty:
        return None
    DR_high = dr_df["high"].max()
    DR_low = dr_df["low"].min()
    IDR_high = dr_df["close"].max()
    IDR_low = dr_df["close"].min()
    SD = IDR_high - IDR_low if (IDR_high - IDR_low) != 0 else np.nan
    
    # Get the session date
    session_date = session_df["date"].iloc[0]
    day_of_week = pd.Timestamp(session_date).day_name()
    
    # Check WDR - just store info, don't filter yet
    wdr_tuesday = None
    wdr_high = None
    wdr_low = None
    wdr_reason = None
    
    if params["use_wdr_filter"]:
        wdr_tuesday, wdr_info = get_applicable_wdr(session_date, tuesday_dates, tuesday_wdr)
        if wdr_info:
            wdr_high = wdr_info["dr_high"]
            wdr_low = wdr_info["dr_low"]
            
    # Calculate M7B (M7 Box)
    m7b_upper, m7b_lower = calculate_m7b(session_df)
    m7b_invalid = False
    m7b_reason = None

    confirm_df = session_df[(session_df["timestamp"].dt.time >= short_confirm_start) &
                            (session_df["timestamp"].dt.time <= short_confirm_end)]
    confirm_signal = confirm_df[confirm_df["close"] < DR_low]
    if confirm_signal.empty:
        return None
    confirm_row = confirm_signal.iloc[0]
    confirm_time = confirm_row["timestamp"]

    session_close = session_df.iloc[-1]["close"]
    if session_close > DR_high:
        return None

    entry_window_df = session_df[(session_df["timestamp"].dt.time >= pd.to_datetime("10:40").time()) &
                                (session_df["timestamp"].dt.time <= pd.to_datetime("14:00").time()) &
                                (session_df["timestamp"] > confirm_time)]
    if entry_window_df.empty:
        return None

    baseline = IDR_low  # For short, baseline = IDR_low.
    candidate_entry = baseline - cluster * SD
    TP = baseline - params["tp_multiple"] * SD  # Use TP multiple from parameters

    # Don't skip trade simulation based on WDR here - just run it normally
    candidate_entries = entry_window_df[entry_window_df["high"] >= candidate_entry]
    if candidate_entries.empty:
        trade_entry = None
        trade_result = None
    else:
        candidate_entry_index = candidate_entries.index[0]
        pre_candidate_df = entry_window_df.loc[:candidate_entry_index]
        if not pre_candidate_df.empty and pre_candidate_df["low"].min() <= TP:
            trade_entry = None
            trade_result = None
        else:
            # For short, candidate valid only if between confirmation and candidate entry,
            # the price never goes above candidate_entry + sl_distance * SD.
            check_df = entry_window_df.loc[entry_window_df.index < candidate_entry_index]
            short_threshold = candidate_entry + params["sl_distance"] * SD  # Use SL distance from parameters
            if not check_df.empty and check_df["high"].max() > short_threshold:
                trade_entry = None
                trade_result = None
            else:
                trade_entry = candidate_entry
                SL = trade_entry + params["sl_distance"] * SD + (params["sl_extra_ticks"] * tick_size)  # Use SL extra ticks from parameters
                entry_time = entry_window_df.loc[candidate_entry_index, "timestamp"]
                
                # Check M7B filter - For SHORT trades, invalidate if entry is above M7B upper boundary
                if params["use_m7b_filter"] and m7b_upper is not None and candidate_entry > m7b_upper:
                    m7b_invalid = True
                    m7b_reason = "Entry above M7B upper boundary"
                
                post_trade_df = session_df[(session_df["timestamp"] >= entry_time) &
                                        (session_df["timestamp"].dt.time <= pd.to_datetime("15:55").time())]
                if post_trade_df.empty:
                    trade_result = None
                else:
                    exit_price, exit_reason = simulate_trade(post_trade_df, trade_entry, TP, SL, "short", tick_size)
                    risk = SL - trade_entry
                    trade_result = {
                        "exit_reason": exit_reason,
                        "pnl_price": trade_entry - exit_price,
                        "pnl_R": (trade_entry - exit_price) / risk if risk != 0 else np.nan,
                        "pnl_ticks": (trade_entry - exit_price) / tick_size
                    }

    post_confirm_df = session_df[(session_df["timestamp"] > confirm_time) &
                                (session_df["timestamp"].dt.time <= pd.to_datetime("15:55").time())]
    if post_confirm_df.empty:
        return None
    min_low = post_confirm_df["low"].min()
    max_high = post_confirm_df["high"].max()
    extension_value = baseline - min_low  # Favorable move (down from baseline).
    retracement_value = max_high - baseline  # Adverse move (up from baseline).
    extension_SD = extension_value / SD if pd.notna(SD) and SD != 0 else np.nan
    retracement_SD = retracement_value / SD if pd.notna(SD) and SD != 0 else np.nan

    confirm_time_str = confirm_time.strftime("%H:%M")
    
    # Now check WDR conditions AFTER all processing is done
    wdr_session_invalid = False
    wdr_cluster_invalid = False
    
    if params["use_wdr_filter"] and wdr_high is not None and wdr_low is not None:
        # Check if both IDR high and low are inside WDR
        if (wdr_low <= IDR_high <= wdr_high) and (wdr_low <= IDR_low <= wdr_high):
            wdr_session_invalid = True
            wdr_reason = "IDR inside WDR"
        # Check if TP is inside WDR
        elif wdr_low <= TP <= wdr_high:
            wdr_session_invalid = True
            wdr_reason = "TP inside WDR"
        # Check if cluster is inside WDR
        elif wdr_low <= candidate_entry <= wdr_high:
            wdr_cluster_invalid = True
            wdr_reason = "Cluster inside WDR"

    output = {
        "date": session_date,
        "trade_direction": "short",
        "day_of_week": day_of_week,
        "confirm_time": confirm_time,
        "confirm_time_str": confirm_time_str,
        "DR_high": DR_high,
        "DR_low": DR_low,
        "IDR_high": IDR_high,
        "IDR_low": IDR_low,
        "SD": SD,
        "session_close": session_close,
        "baseline": baseline,
        "TP": TP,
        "candidate_cluster": cluster,
        "candidate_entry": candidate_entry,
        "retracement": retracement_value,
        "retracement_SD": retracement_SD,
        "extension": extension_value,
        "extension_SD": extension_SD,
        "wdr_session_invalid": wdr_session_invalid,
        "wdr_cluster_invalid": wdr_cluster_invalid,
        "wdr_reason": wdr_reason,
    }
    
    # Add M7B information to output
    if m7b_upper is not None and m7b_lower is not None:
        output.update({
            "m7b_upper": m7b_upper,
            "m7b_lower": m7b_lower,
            "m7b_invalid": m7b_invalid,
            "m7b_reason": m7b_reason,
        })
    
    if wdr_tuesday is not None:
        output.update({
            "wdr_tuesday": wdr_tuesday,
            "wdr_high": wdr_high,
            "wdr_low": wdr_low,
        })
    
    # Store trade results with M7B filter applied
    if trade_result is not None:
        # First store the original unfiltered values
        output_values = {
            "unfiltered_trade_entry": trade_entry,
            "unfiltered_exit_reason": trade_result["exit_reason"],
            "unfiltered_pnl_price": trade_result["pnl_price"],
            "unfiltered_pnl_R": trade_result["pnl_R"],
            "unfiltered_pnl_ticks": trade_result["pnl_ticks"]
        }
        
        # Then store the filtered values (NaN if m7b_invalid is True)
        if m7b_invalid:
            output_values.update({
                "trade_entry": np.nan,
                "SL": np.nan,
                "trade_exit": np.nan,
                "exit_reason": np.nan,
                "pnl_price": np.nan,
                "pnl_R": np.nan,
                "pnl_ticks": np.nan
            })
        else:
            output_values.update({
                "trade_entry": trade_entry,
                "SL": SL,
                "trade_exit": exit_price,
                "exit_reason": trade_result["exit_reason"],
                "pnl_price": trade_result["pnl_price"],
                "pnl_R": trade_result["pnl_R"],
                "pnl_ticks": trade_result["pnl_ticks"]
            })
        
        output.update(output_values)
    else:
        output.update({
            "trade_entry": np.nan,
            "SL": np.nan,
            "trade_exit": np.nan,
            "exit_reason": np.nan,
            "pnl_price": np.nan,
            "pnl_R": np.nan,
            "pnl_ticks": np.nan,
            "unfiltered_trade_entry": np.nan,
            "unfiltered_exit_reason": np.nan,
            "unfiltered_pnl_price": np.nan,
            "unfiltered_pnl_R": np.nan,
            "unfiltered_pnl_ticks": np.nan
        })
        
    return output

# =========================== PARAMETER SWEEP FUNCTIONS ===========================

def process_parameter_combo(combo_args):
    """Process a single parameter combination for SD retracement and Target SD"""
    sd_retrace, target_sd, df, params = combo_args
    
    # Create parameters for this run
    sweep_params = params.copy()
    sweep_params['default_sd_retracement'] = sd_retrace
    sweep_params['default_target_sd'] = target_sd
    
    # Process all combinations of day, confirmation time, and direction
    results = []
    
    # Group by date to process session by session
    for date, session_df in df.groupby('date'):
        day_name = pd.Timestamp(date).day_name()
        
        # For each possible confirmation time
        conf_times = get_confirmation_times(params)
        
        for conf_time in conf_times:
            # Convert string time to datetime.time for filtering
            conf_time_obj = pd.to_datetime(conf_time).time()
            
            # For each direction (if requested)
            directions = []
            if params['analysis_mode'] in ['long', 'both']:
                directions.append('long')
            if params['analysis_mode'] in ['short', 'both']:
                directions.append('short')
            
            for direction in directions:
                # Process this specific combination
                if direction == 'long':
                    # Check if time is in long confirmation window
                    long_start = pd.to_datetime(params["long_confirm_start"]).time()
                    long_end = pd.to_datetime(params["long_confirm_end"]).time()
                    
                    if not (long_start <= conf_time_obj <= long_end):
                        continue
                    
                    metrics = get_long_session_metrics_with_sd_params(
                        session_df, sd_retrace, sweep_params
                    )
                else:
                    # Check if time is in short confirmation window
                    short_start = pd.to_datetime(params["short_confirm_start"]).time()
                    short_end = pd.to_datetime(params["short_confirm_end"]).time()
                    
                    if not (short_start <= conf_time_obj <= short_end):
                        continue
                    
                    metrics = get_short_session_metrics_with_sd_params(
                        session_df, sd_retrace, sweep_params
                    )
                
                if metrics:
                    # Ensure confirm_time_str matches our target
                    if metrics['confirm_time_str'] == conf_time:
                        results.append({
                            'date': metrics['date'],
                            'day': day_name,
                            'confirm_time': conf_time,
                            'direction': direction,
                            'sd_retracement': sd_retrace,
                            'target_sd': target_sd,
                            'trade_entry': metrics.get('trade_entry'),
                            'exit_reason': metrics.get('exit_reason'),
                            'pnl_R': metrics.get('pnl_R')
                        })
    
    return results

def perform_parameter_sweep(df, params):
    """
    Perform parameter sweep with CPU and temperature management
    with enhanced visualization output
    """
    print("\n" + "=" * 100)
    print(" " * 35 + "PARAMETER SWEEP ANALYSIS")
    print("=" * 100)
    
    # Define parameter ranges to sweep
    sd_retracement_ranges = params['sd_retracement_ranges']
    target_sd_ranges = params['target_sd_ranges']
    
    # Print parameter ranges
    print(f"Testing {len(sd_retracement_ranges) * len(target_sd_ranges)} parameter combinations...")
    print(f"SD Retracement values: {sd_retracement_ranges}")
    print(f"Target SD values: {target_sd_ranges}")
    print(f"Fixed SL distance: {params['sl_distance']} SD")
    print(f"Fixed SL extra ticks: {params['sl_extra_ticks']}")
    
    # Create combinations
    from itertools import product
    param_combinations = list(product(sd_retracement_ranges, target_sd_ranges))
    
    total_combinations = len(param_combinations)
    print(f"Testing {total_combinations} parameter combinations...")
    
    # Process combinations with resource management
    sweep_results = []
    
    # Create a list of args for each combination
    combo_args_list = [(sd_retrace, target, df, params) for sd_retrace, target in param_combinations]
    
    # Use the resource management function
    if params['use_multiprocessing']:
        combo_results = process_with_resource_management(
            process_parameter_combo, 
            combo_args_list, 
            params,
            "Processing parameter combinations"
        )
        
        # Flatten the results (since each result might be a list)
        for result in combo_results:
            if result:  # Only add if result is not empty
                sweep_results.extend(result)
    else:
        # Sequential processing with resource management
        for i, combo_args in enumerate(combo_args_list):
            # Check system load periodically
            if i % 5 == 0 and check_system_load(params):
                print(f"Pausing for {params['cooling_pause_seconds']} seconds to cool down...")
                time.sleep(params['cooling_pause_seconds'])
            
            # Process this combination
            result = process_parameter_combo(combo_args)
            if result:  # Only add if result is not empty
                sweep_results.extend(result)
            
            # Update progress
            progress = ((i + 1) / total_combinations) * 100
            sd_retrace, target, _, _ = combo_args
            sys.stdout.write(f"\rProgress: {progress:.1f}% - Testing Retrace={sd_retrace}, Target={target}")
            sys.stdout.flush()
        
        print()  # New line after progress
    
    # Convert to DataFrame for analysis
    sweep_results_df = pd.DataFrame(sweep_results)
    
    if sweep_results_df.empty or len(sweep_results_df) == 0:
        print("No valid parameter combinations found")
        return {}
    
    # Save all raw results
    sweep_results_df.to_csv(f"{params['output_dir']}/all_parameter_sweep_results.csv", index=False)
    print(f"All parameter sweep results saved to {params['output_dir']}/all_parameter_sweep_results.csv")
    
    # Group by day, confirmation time, and direction
    grouped = sweep_results_df.groupby(['day', 'confirm_time', 'direction', 'sd_retracement', 'target_sd'])
    
    # Calculate metrics for each configuration
    config_metrics = []
    
    for name, group in grouped:
        day, conf_time, direction, sd_retrace, target = name
        valid_trades = group[group["trade_entry"].notna()]
        
        if len(valid_trades) >= params["min_trades"]:  # Only consider configurations with enough trades
            win_rate, avg_winner, avg_loser, expectancy = calculate_expectancy(valid_trades)
            total_r = valid_trades["pnl_R"].sum()
            
            if params["calculate_profit_factor"]:
                profit_factor = calculate_profit_factor(valid_trades)
            else:
                profit_factor = None
            
            config_metrics.append({
                'day': day,
                'confirm_time': conf_time,
                'direction': direction,
                'sd_retracement': sd_retrace,
                'target_sd': target,
                'trade_count': len(valid_trades),
                'win_rate': win_rate,
                'total_r': total_r,
                'expectancy': expectancy,
                'profit_factor': profit_factor
            })
    
    # Convert to DataFrame
    config_metrics_df = pd.DataFrame(config_metrics)
    
    if config_metrics_df.empty:
        print("No configurations with sufficient trades to analyze")
        return {}
    
    # Save metrics to CSV
    config_metrics_df.to_csv(f"{params['output_dir']}/parameter_sweep_metrics.csv", index=False)
    print(f"Parameter sweep metrics saved to {params['output_dir']}/parameter_sweep_metrics.csv")
    
    # Find top configurations for each day/time/direction
    top_configs = {}
    
    print("\n" + "=" * 100)
    print(" " * 20 + f"TOP {params['keep_top_configs']} CONFIGURATIONS FOR EACH DAY/TIME/DIRECTION")
    print("=" * 100)
    
    # Group by day, confirmation time, and direction
    day_time_dir_grouped = config_metrics_df.groupby(['day', 'confirm_time', 'direction'])
    
    # Track overall unfiltered total R for each group
    unfiltered_total_r = {}
    
    for group_key, group_df in day_time_dir_grouped:
        day, conf_time, direction = group_key
        
        # Calculate unfiltered total R (all configurations)
        group_total_r = group_df['total_r'].sum()
        unfiltered_total_r[group_key] = group_total_r
        
        # Sort by total_r and get top N
        top_n = min(params['keep_top_configs'], len(group_df))
        top_n_configs = group_df.sort_values('total_r', ascending=False).head(top_n)
        
        # Store in dictionary
        top_configs[group_key] = top_n_configs
        
        # Print the top configurations
        print(f"\n{day} {direction.capitalize()} at {conf_time}:")
        
        filtered_total_r = top_n_configs['total_r'].sum()
        
        for i, row in top_n_configs.iterrows():
            print(f"  #{i+1}: Retracement SD: {row['sd_retracement']:.1f}, Target SD: {row['target_sd']:.1f}")
            print(f"      Trades: {int(row['trade_count'])}, Win Rate: {row['win_rate']:.1f}%, "
                  f"Total R: {row['total_r']:.2f}, Expectancy: {row['expectancy']:.2f}")
        
        # Check total R constraint
        if group_total_r > 0:
            r_reduction = (group_total_r - filtered_total_r) / group_total_r * 100
            if r_reduction > params['max_r_reduction_pct']:
                print(f"  WARNING: Filtering reduced total R by {r_reduction:.1f}%, "
                      f"which exceeds the {params['max_r_reduction_pct']}% limit")
    
    # Save top configurations
    top_configs_list = []
    for group_key, top_df in top_configs.items():
        day, conf_time, direction = group_key
        for _, row in top_df.iterrows():
            config = row.to_dict()
            config['unfiltered_total_r'] = unfiltered_total_r[group_key]
            top_configs_list.append(config)
    
    top_configs_df = pd.DataFrame(top_configs_list)
    top_configs_df.to_csv(f"{params['output_dir']}/top_configurations.csv", index=False)
    
    print(f"\nTop configurations saved to {params['output_dir']}/top_configurations.csv")
    
    # Create heatmap visualizations if matplotlib is available
    if MATPLOTLIB_AVAILABLE:
        try:
            # Create directory for heatmaps
            os.makedirs(f"{params['output_dir']}/heatmaps", exist_ok=True)
            
            # For each day/time/direction combination
            for group_key, group_df in day_time_dir_grouped:
                day, conf_time, direction = group_key
                
                # Skip if not enough data points for a meaningful heatmap
                if len(group_df) < 4:
                    continue
                
                # Create a pivot table for the heatmap
                pivot_data = group_df.pivot_table(
                    index='sd_retracement', 
                    columns='target_sd',
                    values='total_r',
                    aggfunc='sum'
                )
                
                # Create heatmap
                plt.figure(figsize=(10, 8))
                sns_heatmap = plt.imshow(pivot_data, cmap='RdYlGn')
                plt.colorbar(sns_heatmap, label='Total R')
                plt.title(f'{day} {direction.capitalize()} at {conf_time} - Total R')
                
                # Label axes
                plt.xlabel('Target SD')
                plt.ylabel('SD Retracement')
                
                # Set custom tick labels
                plt.xticks(range(len(pivot_data.columns)), labels=pivot_data.columns)
                plt.yticks(range(len(pivot_data.index)), labels=pivot_data.index)
                
                # Save the heatmap
                heatmap_file = f"{params['output_dir']}/heatmaps/{day}_{direction}_{conf_time.replace(':', '')}_heatmap.png"
                plt.savefig(heatmap_file, dpi=300, bbox_inches='tight')
                plt.close()
                
                print(f"Heatmap saved to {heatmap_file}")
            
            # Create overall expectancy heatmap
            all_params_df = config_metrics_df.groupby(['sd_retracement', 'target_sd']).agg({
                'total_r': 'sum',
                'expectancy': 'mean',
                'win_rate': 'mean',
                'trade_count': 'sum'
            }).reset_index()
            
            # Only create if we have enough data points
            if len(all_params_df) >= 4:
                # Total R heatmap
                plt.figure(figsize=(12, 10))
                
                # Create subplot for Total R
                plt.subplot(2, 2, 1)
                total_r_pivot = all_params_df.pivot_table(
                    index='sd_retracement',
                    columns='target_sd',
                    values='total_r'
                )
                sns_heatmap = plt.imshow(total_r_pivot, cmap='RdYlGn')
                plt.colorbar(sns_heatmap, label='Total R')
                plt.title('Total R by Parameter Combination')
                plt.xlabel('Target SD')
                plt.ylabel('SD Retracement')
                plt.xticks(range(len(total_r_pivot.columns)), labels=total_r_pivot.columns)
                plt.yticks(range(len(total_r_pivot.index)), labels=total_r_pivot.index)
                
                # Create subplot for Expectancy
                plt.subplot(2, 2, 2)
                exp_pivot = all_params_df.pivot_table(
                    index='sd_retracement',
                    columns='target_sd',
                    values='expectancy'
                )
                sns_heatmap = plt.imshow(exp_pivot, cmap='Blues')
                plt.colorbar(sns_heatmap, label='Expectancy')
                plt.title('Expectancy by Parameter Combination')
                plt.xlabel('Target SD')
                plt.ylabel('SD Retracement')
                plt.xticks(range(len(exp_pivot.columns)), labels=exp_pivot.columns)
                plt.yticks(range(len(exp_pivot.index)), labels=exp_pivot.index)
                
                # Create subplot for Win Rate
                plt.subplot(2, 2, 3)
                win_rate_pivot = all_params_df.pivot_table(
                    index='sd_retracement',
                    columns='target_sd',
                    values='win_rate'
                )
                sns_heatmap = plt.imshow(win_rate_pivot, cmap='Greens')
                plt.colorbar(sns_heatmap, label='Win Rate (%)')
                plt.title('Win Rate by Parameter Combination')
                plt.xlabel('Target SD')
                plt.ylabel('SD Retracement')
                plt.xticks(range(len(win_rate_pivot.columns)), labels=win_rate_pivot.columns)
                plt.yticks(range(len(win_rate_pivot.index)), labels=win_rate_pivot.index)
                
                # Create subplot for Trade Count
                plt.subplot(2, 2, 4)
                trade_count_pivot = all_params_df.pivot_table(
                    index='sd_retracement',
                    columns='target_sd',
                    values='trade_count'
                )
                sns_heatmap = plt.imshow(trade_count_pivot, cmap='Purples')
                plt.colorbar(sns_heatmap, label='Trade Count')
                plt.title('Trade Count by Parameter Combination')
                plt.xlabel('Target SD')
                plt.ylabel('SD Retracement')
                plt.xticks(range(len(trade_count_pivot.columns)), labels=trade_count_pivot.columns)
                plt.yticks(range(len(trade_count_pivot.index)), labels=trade_count_pivot.index)
                
                plt.tight_layout()
                
                # Save the overall heatmap
                overall_heatmap_file = f"{params['output_dir']}/parameter_sweep_heatmaps.png"
                plt.savefig(overall_heatmap_file, dpi=300, bbox_inches='tight')
                plt.close()
                
                print(f"Overall parameter sweep heatmaps saved to {overall_heatmap_file}")
        
        except Exception as e:
            print(f"Error creating heatmaps: {e}")
    
    return top_configs

def perform_walk_forward_analysis(df, params):
    """
    Perform walk-forward analysis using calendar years instead of trading days,
    and apply the top configurations for each day/time/direction.
    """
    print("\n" + "=" * 100)
    print(" " * 35 + "WALK FORWARD ANALYSIS")
    print("=" * 100)
    
    # Get all unique dates
    all_dates = sorted(df["date"].unique())
    start_date = min(all_dates)
    end_date = max(all_dates)
    
    # Calculate time span in years
    start_timestamp = pd.Timestamp(start_date)
    end_timestamp = pd.Timestamp(end_date)
    years_span = (end_timestamp - start_timestamp).days / 365.25
    
    print(f"Dataset spans from {start_date} to {end_date} ({years_span:.1f} years)")
    
    # Check if we have enough years
    required_years = params['in_sample_years'] + params['out_sample_years']
    if years_span < required_years:
        print(f"Not enough data for walk-forward analysis (need at least {required_years} years)")
        print(f"Current dataset spans {years_span:.1f} years")
        return
    
    # Define walk-forward windows using calendar years
    windows = []
    current_start = start_timestamp
    
    while current_start + pd.DateOffset(years=required_years) <= end_timestamp:
        in_sample_start = current_start
        in_sample_end = current_start + pd.DateOffset(years=params['in_sample_years'])
        out_sample_start = in_sample_end
        out_sample_end = out_sample_start + pd.DateOffset(years=params['out_sample_years'])
        
        windows.append({
            'in_sample_start': in_sample_start.date(),  # Store as datetime.date objects, not strings
            'in_sample_end': in_sample_end.date(),
            'out_sample_start': out_sample_start.date(),
            'out_sample_end': out_sample_end.date()
        })
        
        # Move forward by 1 year for next window
        current_start = current_start + pd.DateOffset(years=1)
    
    print(f"Created {len(windows)} walk-forward windows")
    
    # Store results
    wf_results = []
    all_out_sample_trades = []
    
    for i, window in enumerate(windows):
        # Periodically check system load
        if check_system_load(params):
            print(f"Pausing for {params['cooling_pause_seconds']} seconds to cool down...")
            time.sleep(params['cooling_pause_seconds'])
            
        print(f"\nProcessing Window {i+1}/{len(windows)}")
        print(f"In-sample: {window['in_sample_start']} to {window['in_sample_end']}")
        print(f"Out-sample: {window['out_sample_start']} to {window['out_sample_end']}")
        
        # Filter data for in-sample period
        in_sample_df = df[(df["date"] >= window['in_sample_start']) & 
                          (df["date"] <= window['in_sample_end'])]
        
        # Find optimal parameters using in-sample data
        top_configs = perform_parameter_sweep(in_sample_df, params)
        
        if not top_configs:
            print("No optimal parameters found for this window, skipping...")
            continue
        
        # Filter data for out-of-sample period
        out_sample_df = df[(df["date"] >= window['out_sample_start']) & 
                           (df["date"] <= window['out_sample_end'])]
        
        # Apply top configurations to out-sample data
        all_results = []
        
        # Process each day/time/direction with its optimal parameters
        for group_key, top_df in top_configs.items():
            day, conf_time, direction = group_key
            
            # Filter out-sample data for this day
            day_out_df = out_sample_df[pd.to_datetime(out_sample_df['date']).dt.day_name() == day]
            
            if day_out_df.empty:
                print(f"No out-sample data for {day}, skipping")
                continue
            
            # For each of the top configurations
            for _, config in top_df.iterrows():
                # Set configuration parameters
                config_params = params.copy()
                config_params['default_sd_retracement'] = config['sd_retracement']
                config_params['default_target_sd'] = config['target_sd']
                
                # Apply this configuration to get results
                if direction == 'long':
                    # Process long trades
                    for date, session_df in day_out_df.groupby('date'):
                        metrics = get_long_session_metrics_with_sd_params(
                            session_df, config['sd_retracement'], config_params
                        )
                        if metrics and metrics['confirm_time_str'] == conf_time:
                            all_results.append(metrics)
                else:
                    # Process short trades
                    for date, session_df in day_out_df.groupby('date'):
                        metrics = get_short_session_metrics_with_sd_params(
                            session_df, config['sd_retracement'], config_params
                        )
                        if metrics and metrics['confirm_time_str'] == conf_time:
                            all_results.append(metrics)
        
        # Convert to DataFrame
        out_results_df = pd.DataFrame(all_results)
        
        if not out_results_df.empty:
            filtered_trades = out_results_df[out_results_df["trade_entry"].notna()]
            
            if not filtered_trades.empty:
                # Store these trades for combined equity curve
                filtered_trades['window'] = i+1
                all_out_sample_trades.append(filtered_trades)
                
                # Calculate metrics
                win_rate, avg_winner, avg_loser, expectancy = calculate_expectancy(filtered_trades)
                total_r = filtered_trades["pnl_R"].sum()
                profit_factor = calculate_profit_factor(filtered_trades)
                trade_count = len(filtered_trades)
                
                # Calculate max drawdown
                cumulative_r = filtered_trades['pnl_R'].cumsum()
                running_max = cumulative_r.expanding().max()
                drawdown = cumulative_r - running_max
                max_drawdown = drawdown.min() if not drawdown.empty else 0
                
                # Store results for this window
                wf_results.append({
                    'window': i+1,
                    'in_sample_start': window['in_sample_start'],
                    'in_sample_end': window['in_sample_end'],
                    'out_sample_start': window['out_sample_start'],
                    'out_sample_end': window['out_sample_end'],
                    'trade_count': trade_count,
                    'win_rate': win_rate,
                    'total_r': total_r,
                    'avg_r': total_r / trade_count if trade_count > 0 else 0,
                    'expectancy': expectancy,
                    'profit_factor': profit_factor,
                    'max_drawdown': max_drawdown
                })
                
                print(f"Out-sample results: {trade_count} trades, {win_rate:.2f}% win rate, "
                     f"{total_r:.2f} total R, {expectancy:.2f} expectancy")
            else:
                print("No valid trades in out-sample period")
        else:
            print("No valid trade setups in out-sample period")
    
    # Overall walk-forward results
    if wf_results:
        wf_df = pd.DataFrame(wf_results)
        
        print("\nWalk-Forward Analysis Summary:")
        print(f"Total Windows: {len(wf_df)}")
        print(f"Total Trades: {wf_df['trade_count'].sum()}")
        print(f"Total R: {wf_df['total_r'].sum():.2f}")
        print(f"Average Win Rate: {wf_df['win_rate'].mean():.2f}%")
        print(f"Average Expectancy: {wf_df['expectancy'].mean():.2f}R")
        
        # Count profitable windows
        profitable_windows = len(wf_df[wf_df['total_r'] > 0])
        print(f"Profitable Windows: {profitable_windows}/{len(wf_df)} ({profitable_windows/len(wf_df)*100:.2f}%)")
        
        # Save results
        wf_df.to_csv(f"{params['output_dir']}/walk_forward_results.csv", index=False)
        print(f"Walk-forward results saved to {params['output_dir']}/walk_forward_results.csv")
        
        # Generate equity curve if requested
        if params['generate_equity_curve'] and all_out_sample_trades:
            print("\nGenerating combined equity curve for all out-sample periods...")
            all_trades_df = pd.concat(all_out_sample_trades)
            generate_equity_curve(all_trades_df, params['output_dir'], "Combined_Walk_Forward")
            
            # Equity curves by strategy type if requested
            if params['analysis_mode'] == 'both':
                # Long trades
                long_trades = all_trades_df[all_trades_df['trade_direction'] == 'long']
                if not long_trades.empty:
                    generate_equity_curve(long_trades, params['output_dir'], "Combined_Walk_Forward_Long")
                
                # Short trades
                short_trades = all_trades_df[all_trades_df['trade_direction'] == 'short']
                if not short_trades.empty:
                    generate_equity_curve(short_trades, params['output_dir'], "Combined_Walk_Forward_Short")
        
        # Create performance charts if matplotlib available
        if MATPLOTLIB_AVAILABLE:
            plt.figure(figsize=(12, 6))
            plt.bar(range(1, len(wf_df) + 1), wf_df['total_r'], color=['g' if r > 0 else 'r' for r in wf_df['total_r']])
            plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)
            plt.xlabel('Window')
            plt.ylabel('Total R')
            plt.title('Walk-Forward Analysis - Window Performance')
            plt.grid(axis='y', alpha=0.3)
            
            # Add labels
            for i, r in enumerate(wf_df['total_r']):
                plt.text(i + 1, r + (0.5 if r > 0 else -0.5), f"{r:.1f}R", 
                        ha='center', va='center', fontweight='bold')
            
            plt.tight_layout()
            plt.savefig(f"{params['output_dir']}/walk_forward_performance.png", dpi=300, bbox_inches='tight')
            plt.close()
            
            print(f"Walk-forward performance chart saved to {params['output_dir']}/walk_forward_performance.png")
    else:
        print("No valid walk-forward results")
    
    return wf_results

# =========================== CORE STRATEGY FUNCTIONS ===========================

def get_applicable_wdr(session_date, tuesday_dates, tuesday_wdr):
    """
    Find the most recent Tuesday's WDR that applies to this session date.
    For Tuesday sessions, use the previous Tuesday's WDR.
    """
    if pd.Timestamp(session_date).day_name() == 'Tuesday':
        # For Tuesday, find the previous Tuesday
        prev_tuesdays = [date for date in tuesday_dates if date < session_date]
        if prev_tuesdays:
            prev_tuesday = max(prev_tuesdays)  # Most recent previous Tuesday
            if prev_tuesday in tuesday_wdr:
                return prev_tuesday, tuesday_wdr[prev_tuesday]
    else:
        # For other days, find the most recent Tuesday on or before this date
        applicable_tuesdays = [date for date in tuesday_dates if date <= session_date]
        if applicable_tuesdays:
            most_recent_tuesday = max(applicable_tuesdays)  # Most recent Tuesday
            if most_recent_tuesday in tuesday_wdr:
                return most_recent_tuesday, tuesday_wdr[most_recent_tuesday]
    
    # No applicable WDR found
    return None, None

def calculate_m7b(session_df):
    """
    Calculate the M7 Box (M7B) for a session.
    M7B is defined as the range between the open price at 9:30 and close price at 10:25.
    """
    # Get the 9:30 candle (market open)
    market_open_df = session_df[session_df["timestamp"].dt.time == pd.to_datetime("09:30").time()]
    
    # Get the 10:25 candle (end of DR period)
    dr_end_df = session_df[session_df["timestamp"].dt.time == pd.to_datetime("10:25").time()]
    
    # Debug print if we can't find these candles
    if market_open_df.empty or dr_end_df.empty:
        session_date = session_df["date"].iloc[0] if not session_df.empty else "unknown"
        return None, None
    
    # Get the open price at 9:30 and close price at 10:25
    open_price = market_open_df.iloc[0]["open"]
    close_price = dr_end_df.iloc[0]["close"]
    
    # Define M7B upper and lower boundaries
    m7b_upper = max(open_price, close_price)
    m7b_lower = min(open_price, close_price)
    
    return m7b_upper, m7b_lower

def simulate_trade(post_trade_df, entry_price, TP, SL, direction, tick_size):
    """
    Simulate a trade from the entry candle until 15:55.
    For long trades:
       - If candle's low <= SL, then SL is triggered.
       - If candle's high >= TP, then TP is triggered.
    For short trades:
       - If candle's high >= SL, then SL is triggered.
       - If candle's low <= TP, then TP is triggered.
    If both occur in the same candle, assume stop loss.
    """
    for idx, row in post_trade_df.iterrows():
        if direction == "long":
            if row["low"] <= SL and row["high"] >= TP:
                return SL, "SL"
            elif row["low"] <= SL:
                return SL, "SL"
            elif row["high"] >= TP:
                return TP, "TP"
        elif direction == "short":
            if row["high"] >= SL and row["low"] <= TP:
                return SL, "SL"
            elif row["high"] >= SL:
                return SL, "SL"
            elif row["low"] <= TP:
                return TP, "TP"
    return post_trade_df.iloc[-1]["close"], "EXIT"

def get_long_session_metrics(session_df, cluster, params, tuesday_dates, tuesday_wdr):
    """Process long trade setups for a session"""
    # Get time windows from parameters
    long_confirm_start = pd.to_datetime(params["long_confirm_start"]).time()
    long_confirm_end = pd.to_datetime(params["long_confirm_end"]).time()
    tick_size = params["tick_size"]
    
    dr_df = session_df[(session_df["timestamp"].dt.time >= pd.to_datetime("09:30").time()) &
                      (session_df["timestamp"].dt.time <= pd.to_datetime("10:25").time())]
    if dr_df.empty:
        return None
    DR_high = dr_df["high"].max()
    DR_low = dr_df["low"].min()
    IDR_high = dr_df["close"].max()
    IDR_low = dr_df["close"].min()
    SD = IDR_high - IDR_low if (IDR_high - IDR_low) != 0 else np.nan
    
    # Get the session date
    session_date = session_df["date"].iloc[0]
    day_of_week = pd.Timestamp(session_date).day_name()
    
    # Check WDR - just store info, don't filter yet
    wdr_tuesday = None
    wdr_high = None
    wdr_low = None
    wdr_reason = None
    
    if params["use_wdr_filter"]:
        wdr_tuesday, wdr_info = get_applicable_wdr(session_date, tuesday_dates, tuesday_wdr)
        if wdr_info:
            wdr_high = wdr_info["dr_high"]
            wdr_low = wdr_info["dr_low"]
    
    # Calculate M7B (M7 Box)
    m7b_upper, m7b_lower = calculate_m7b(session_df)
    m7b_invalid = False
    m7b_reason = None

    confirm_df = session_df[(session_df["timestamp"].dt.time >= long_confirm_start) &
                            (session_df["timestamp"].dt.time <= long_confirm_end)]
    confirm_signal = confirm_df[confirm_df["close"] > DR_high]
    if confirm_signal.empty:
        return None
    confirm_row = confirm_signal.iloc[0]
    confirm_time = confirm_row["timestamp"]

    session_close = session_df.iloc[-1]["close"]
    if session_close < DR_low:
        return None

    entry_window_df = session_df[(session_df["timestamp"].dt.time >= pd.to_datetime("10:40").time()) &
                                (session_df["timestamp"].dt.time <= pd.to_datetime("14:00").time()) &
                                (session_df["timestamp"] > confirm_time)]
    if entry_window_df.empty:
        return None

    baseline = IDR_high
    candidate_entry = baseline + cluster * SD
    TP = baseline + params["tp_multiple"] * SD  # Use TP multiple from parameters

    # Don't skip trade simulation based on WDR here - just run it normally
    candidate_entries = entry_window_df[entry_window_df["low"] <= candidate_entry]
    if candidate_entries.empty:
        trade_entry = None
        trade_result = None
    else:
        candidate_entry_index = candidate_entries.index[0]
        pre_candidate_df = entry_window_df.loc[:candidate_entry_index]
        if not pre_candidate_df.empty and pre_candidate_df["high"].max() >= TP:
            trade_entry = None
            trade_result = None
        else:
            # For long, ensure that between confirmation and candidate entry,
            # the price never falls below candidate_entry - sl_distance * SD.
            check_df = entry_window_df.loc[entry_window_df.index < candidate_entry_index]
            long_threshold = candidate_entry - params["sl_distance"] * SD  # Use SL distance from parameters
            if not check_df.empty and check_df["low"].min() < long_threshold:
                trade_entry = None
                trade_result = None
            else:
                trade_entry = candidate_entry
                SL = trade_entry - params["sl_distance"] * SD - (params["sl_extra_ticks"] * tick_size)  # Use SL extra ticks from parameters
                entry_time = entry_window_df.loc[candidate_entry_index, "timestamp"]
                
                # Check M7B filter - For LONG trades, invalidate if entry is below M7B lower boundary
                if params["use_m7b_filter"] and m7b_lower is not None and candidate_entry < m7b_lower:
                    m7b_invalid = True
                    m7b_reason = "Entry below M7B lower boundary"
                
                post_trade_df = session_df[(session_df["timestamp"] >= entry_time) &
                                        (session_df["timestamp"].dt.time <= pd.to_datetime("15:55").time())]
                if post_trade_df.empty:
                    trade_result = None
                else:
                    exit_price, exit_reason = simulate_trade(post_trade_df, trade_entry, TP, SL, "long", tick_size)
                    risk = trade_entry - SL
                    trade_result = {
                        "exit_reason": exit_reason,
                        "pnl_price": exit_price - trade_entry,
                        "pnl_R": (exit_price - trade_entry) / risk if risk != 0 else np.nan,
                        "pnl_ticks": (exit_price - trade_entry) / tick_size
                    }

    post_confirm_df = session_df[(session_df["timestamp"] > confirm_time) &
                                (session_df["timestamp"].dt.time <= pd.to_datetime("15:55").time())]
    if post_confirm_df.empty:
        return None
    max_high = post_confirm_df["high"].max()
    extreme_row = post_confirm_df[post_confirm_df["high"] == max_high].iloc[0]
    extension_value = max_high - baseline
    retracement_window = post_confirm_df[post_confirm_df["timestamp"] < extreme_row["timestamp"]]
    if retracement_window.empty:
        return None
    retracement_value = retracement_window["low"].min() - baseline
    extension_SD = extension_value / SD if pd.notna(SD) and SD != 0 else np.nan
    retracement_SD = retracement_value / SD if pd.notna(SD) and SD != 0 else np.nan

    confirm_time_str = confirm_time.strftime("%H:%M")

    # Now check WDR conditions AFTER all processing is done
    wdr_session_invalid = False
    wdr_cluster_invalid = False
    
    if params["use_wdr_filter"] and wdr_high is not None and wdr_low is not None:
        # Check if both IDR high and low are inside WDR
        if (wdr_low <= IDR_high <= wdr_high) and (wdr_low <= IDR_low <= wdr_high):
            wdr_session_invalid = True
            wdr_reason = "IDR inside WDR"
        # Check if TP is inside WDR
        elif wdr_low <= TP <= wdr_high:
            wdr_session_invalid = True
            wdr_reason = "TP inside WDR"
        # Check if cluster is inside WDR
        elif wdr_low <= candidate_entry <= wdr_high:
            wdr_cluster_invalid = True
            wdr_reason = "Cluster inside WDR"

    output = {
        "date": session_date,
        "trade_direction": "long",
        "day_of_week": day_of_week,
        "confirm_time": confirm_time,
        "confirm_time_str": confirm_time_str,
        "DR_high": DR_high,
        "DR_low": DR_low,
        "IDR_high": IDR_high,
        "IDR_low": IDR_low,
        "SD": SD,
        "session_close": session_close,
        "baseline": baseline,
        "TP": TP,
        "candidate_cluster": cluster,
        "candidate_entry": candidate_entry,
        "retracement": retracement_value,
        "retracement_SD": retracement_SD,
        "extension": extension_value,
        "extension_SD": extension_SD,
        "wdr_session_invalid": wdr_session_invalid,
        "wdr_cluster_invalid": wdr_cluster_invalid,
        "wdr_reason": wdr_reason,
    }
    
    # Add M7B information to output
    if m7b_upper is not None and m7b_lower is not None:
        output.update({
            "m7b_upper": m7b_upper,
            "m7b_lower": m7b_lower,
            "m7b_invalid": m7b_invalid,
            "m7b_reason": m7b_reason,
        })
    
    if wdr_tuesday is not None:
        output.update({
            "wdr_tuesday": wdr_tuesday,
            "wdr_high": wdr_high,
            "wdr_low": wdr_low,
        })
    
    # Store trade results with M7B filter applied
    if trade_result is not None:
        # First store the original unfiltered values
        output_values = {
            "unfiltered_trade_entry": trade_entry,
            "unfiltered_exit_reason": trade_result["exit_reason"],
            "unfiltered_pnl_price": trade_result["pnl_price"],
            "unfiltered_pnl_R": trade_result["pnl_R"],
            "unfiltered_pnl_ticks": trade_result["pnl_ticks"]
        }
        
        # Then store the filtered values (NaN if m7b_invalid is True)
        if m7b_invalid:
            output_values.update({
                "trade_entry": np.nan,
                "SL": np.nan,
                "trade_exit": np.nan,
                "exit_reason": np.nan,
                "pnl_price": np.nan,
                "pnl_R": np.nan,
                "pnl_ticks": np.nan
            })
        else:
            output_values.update({
                "trade_entry": trade_entry,
                "SL": SL,
                "trade_exit": exit_price,
                "exit_reason": trade_result["exit_reason"],
                "pnl_price": trade_result["pnl_price"],
                "pnl_R": trade_result["pnl_R"],
                "pnl_ticks": trade_result["pnl_ticks"]
            })
        
        output.update(output_values)
    else:
        output.update({
            "trade_entry": np.nan,
            "SL": np.nan,
            "trade_exit": np.nan,
            "exit_reason": np.nan,
            "pnl_price": np.nan,
            "pnl_R": np.nan,
            "pnl_ticks": np.nan,
            "unfiltered_trade_entry": np.nan,
            "unfiltered_exit_reason": np.nan,
            "unfiltered_pnl_price": np.nan,
            "unfiltered_pnl_R": np.nan,
            "unfiltered_pnl_ticks": np.nan
        })
    
    return output

if __name__ == "__main__":
    main()
