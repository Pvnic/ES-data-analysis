import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import os
import sys
import time
import multiprocessing as mp
from functools import partial
import psutil

# Try to import temperature monitoring libraries
try:
    # Optional module for temperature monitoring
    import py3nvml.py3nvml as nvml
    NVML_AVAILABLE = True
except ImportError:
    NVML_AVAILABLE = False

# Try to import GPU acceleration libraries
try:
    import cupy as cp
    GPU_AVAILABLE = True
    CUDF_AVAILABLE = False
    print("GPU acceleration enabled using CuPy")
    
    # Check if cuDF is also available, but don't require it
    try:
        import cudf
        CUDF_AVAILABLE = True
        print("RAPIDS cuDF library also available")
    except ImportError:
        CUDF_AVAILABLE = False
        print("CuPy available, but RAPIDS cuDF library not available. Using pandas for dataframes.")
except ImportError:
    GPU_AVAILABLE = False
    CUDF_AVAILABLE = False
    print("GPU acceleration libraries (CuPy) not available, using CPU")

try:
    import matplotlib.pyplot as plt
    import matplotlib.dates as mdates
    from matplotlib.ticker import FuncFormatter
    MATPLOTLIB_AVAILABLE = True
    
    # Try to import seaborn for enhanced visualizations
    try:
        import seaborn as sns
        SEABORN_AVAILABLE = True
    except ImportError:
        SEABORN_AVAILABLE = False
except ImportError:
    MATPLOTLIB_AVAILABLE = False
    SEABORN_AVAILABLE = False
    
try:
    import itertools
    from tqdm import tqdm
except ImportError:
    pass

# Default parameters - UPDATED with standardized terminology and clear separation
default_params = {
    # Core Strategy Parameters
    "sd_retracement_levels": [0.2, 0.1, 0, -0.1, -0.2, -0.3, -0.4, -0.5, -0.6, -0.7, -0.8, -0.9],
    "analysis_mode": "long",
    "selected_days": None,
    "default_target_sd": 1.0,       # Default target SD multiple
    
    # Filtering Options
    "use_wdr_filter": False,
    "use_m7b_filter": True,
    
    # Time Windows
    "long_confirm_start": "10:30",
    "long_confirm_end": "10:55",
    "short_confirm_start": "10:30",
    "short_confirm_end": "10:50",
    
    # Trade Parameters
    "tp_multiple": 1.0,
    "sl_distance": 0.1,
    "sl_extra_ticks": 2,
    
    # Analysis Parameters
    "min_trades": 5,
    "top_configs": 10,
    "target_win_rate": 50.0,
    "target_expectancy": 1.5,
    "keep_top_configs": 2,  # Number of top configurations to keep
    
    # Output Controls
    "show_raw_data": True,
    "show_overall_stats": True,
    "show_day_performance": True,
    "show_top_configs": True,
    "show_target_configs": True,
    "show_filter_stats": True,
    
    # Additional Analysis Options
    "calculate_profit_factor": False,
    "analyze_streaks": False,
    "perform_param_sweep": True,  # Default to TRUE for better user experience
    "perform_day_by_day_sweep": True,
    
    # Analysis Periods - Added clear period definitions
    "backtest_start_date": None,  # Will be set based on data if not provided
    "backtest_end_date": None,    # Will be set based on data if not provided
    "walkforward_start_date": None,  # Will be set based on data if not provided
    "walkforward_end_date": None,    # Will be set based on data if not provided
    "perform_walk_forward": False,
    "generate_equity_curve": False,
    
    # Performance Options
    "use_gpu": True,
    "use_multiprocessing": True,
    "file_options": {
        "data_file": "es_data.txt",
        "output_dir": "results"
    },
    
    # Target SD Options - Only defined once (for consistency)
    "tick_size": 0.25,
    "target_sd_ranges": [0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.5, 1.8, 2.0],
    "max_r_reduction_pct": 30.0,  # Maximum allowed R reduction percentage
    
    # CPU/GPU usage management
    "max_cpu_cores": 4,              # Limit CPU cores (default to 4 instead of all 16)
    "enable_temp_monitoring": True,  # Enable temperature monitoring
    "max_temperature": 70,           # Maximum temperature in Celsius
    "cpu_usage_limit": 70,           # Maximum CPU usage percentage
    "cooling_pause_seconds": 5,      # Pause duration for cooling when temp is high
}

# =========================== CPU/TEMPERATURE MONITORING FUNCTIONS ===========================

def check_system_load(params):
    """Check CPU temperature and usage, return True if we should pause processing"""
    
    # Check CPU usage
    cpu_usage = psutil.cpu_percent(interval=0.5)
    if cpu_usage > params['cpu_usage_limit']:
        print(f"\nCPU usage ({cpu_usage}%) exceeds limit ({params['cpu_usage_limit']}%). Pausing for cooling...")
        return True
    
    # Check temperature if monitoring is enabled
    if params['enable_temp_monitoring']:
        try:
            # Try to get CPU temperature (this is platform-specific)
            if hasattr(psutil, "sensors_temperatures"):
                temps = psutil.sensors_temperatures()
                if temps:
                    # Get the highest temperature from any CPU core
                    max_temp = 0
                    for name, entries in temps.items():
                        if name.lower().startswith(('cpu', 'core', 'k10temp')):
                            for entry in entries:
                                if entry.current > max_temp:
                                    max_temp = entry.current
                    
                    if max_temp > params['max_temperature']:
                        print(f"\nCPU temperature ({max_temp}째C) exceeds limit ({params['max_temperature']}째C). Pausing for cooling...")
                        return True
            
            # Try NVIDIA GPU temperature if available
            if NVML_AVAILABLE:
                try:
                    nvml.nvmlInit()
                    device_count = nvml.nvmlDeviceGetCount()
                    for i in range(device_count):
                        handle = nvml.nvmlDeviceGetHandleByIndex(i)
                        temp = nvml.nvmlDeviceGetTemperature(handle, nvml.NVML_TEMPERATURE_GPU)
                        if temp > params['max_temperature']:
                            print(f"\nGPU temperature ({temp}째C) exceeds limit ({params['max_temperature']}째C). Pausing for cooling...")
                            return True
                except Exception:
                    pass  # Ignore NVML errors
        
        except Exception as e:
            print(f"Warning: Error monitoring temperature: {e}")
    
    # All checks passed, no need to pause
    return False

def process_with_resource_management(func, item_list, params, desc="Processing"):
    """Process items with multiprocessing while managing CPU resources"""
    
    results = []
    total_items = len(item_list)
    
    if params['use_multiprocessing']:
        # Calculate number of cores to use (respect max_cpu_cores parameter)
        num_processes = min(params['max_cpu_cores'], mp.cpu_count(), total_items)
        print(f"Using {num_processes} of {mp.cpu_count()} available CPU cores")
        
        # Process in smaller batches to allow temperature management
        batch_size = max(1, total_items // (num_processes * 2))
        
        processed = 0
        with mp.Pool(processes=num_processes) as pool:
            for i in range(0, total_items, batch_size):
                # Check system load before processing each batch
                if check_system_load(params):
                    print(f"Pausing for {params['cooling_pause_seconds']} seconds to cool down...")
                    time.sleep(params['cooling_pause_seconds'])
                
                # Process a batch
                end_idx = min(i + batch_size, total_items)
                batch = item_list[i:end_idx]
                batch_results = list(pool.map(func, batch))
                results.extend(batch_results)
                
                # Update progress
                processed += len(batch)
                progress = (processed / total_items) * 100
                sys.stdout.write(f"\r{desc}: {progress:.1f}% - Processed {processed}/{total_items}")
                sys.stdout.flush()
            
            print()  # New line after progress bar
    else:
        # Sequential processing
        for i, item in enumerate(item_list):
            # Check system load periodically
            if i % 10 == 0 and check_system_load(params):
                print(f"Pausing for {params['cooling_pause_seconds']} seconds to cool down...")
                time.sleep(params['cooling_pause_seconds'])
            
            # Process item
            result = func(item)
            results.append(result)
            
            # Update progress
            progress = ((i + 1) / total_items) * 100
            sys.stdout.write(f"\r{desc}: {progress:.1f}% - Processed {i+1}/{total_items}")
            sys.stdout.flush()
        
        print()  # New line after progress bar
    
    return results

# =========================== UTILITY FUNCTIONS ===========================

def calculate_expectancy(valid_trades):
    """
    Calculate expectancy consistently based on profitable vs unprofitable trades.
    
    Parameters:
    valid_trades (DataFrame): DataFrame containing trade results with pnl_R column
    
    Returns:
    tuple: (win_rate, avg_winner, avg_loser, expectancy)
    """
    if valid_trades.empty:
        return 0, 0, 0, 0
    
    # Define winners and losers based on profit/loss
    winners = valid_trades[valid_trades["pnl_R"] > 0]
    losers = valid_trades[valid_trades["pnl_R"] <= 0]
    
    # Calculate win rate based on profitability (not TP hits)
    win_rate = (len(winners) / len(valid_trades)) * 100 if len(valid_trades) > 0 else 0
    
    # Calculate average winner and loser
    avg_winner = winners["pnl_R"].mean() if not winners.empty else 0
    avg_loser = losers["pnl_R"].mean() if not losers.empty else 0
    
    # Proper expectancy calculation
    expectancy = (win_rate/100) * avg_winner + (1 - win_rate/100) * avg_loser
    
    return win_rate, avg_winner, avg_loser, expectancy

def calculate_expectancy_gpu(valid_trades):
    """
    GPU-accelerated version of calculate_expectancy using CuPy
    """
    if valid_trades.empty:
        return 0, 0, 0, 0
    
    # Convert pnl_R to CuPy array for faster computation
    pnl_array = cp.array(valid_trades["pnl_R"].values)
    
    # Define winners and losers
    winners_mask = pnl_array > 0
    losers_mask = ~winners_mask
    
    # Calculate win rate
    win_rate = (cp.sum(winners_mask) / len(pnl_array)) * 100
    
    # Calculate average winner and loser
    winners = pnl_array[winners_mask]
    losers = pnl_array[losers_mask]
    
    avg_winner = cp.mean(winners).item() if len(winners) > 0 else 0
    avg_loser = cp.mean(losers).item() if len(losers) > 0 else 0
    
    # Expectancy calculation
    expectancy = (win_rate/100) * avg_winner + (1 - win_rate/100) * avg_loser
    
    return win_rate.item(), avg_winner, avg_loser, expectancy

def calculate_profit_factor(valid_trades):
    """Calculate profit factor (gross profits / gross losses)"""
    if valid_trades.empty:
        return 0
    
    gross_profits = valid_trades[valid_trades["pnl_R"] > 0]["pnl_R"].sum()
    gross_losses = abs(valid_trades[valid_trades["pnl_R"] < 0]["pnl_R"].sum())
    
    profit_factor = gross_profits / gross_losses if gross_losses != 0 else float('inf')
    return profit_factor

def analyze_streaks(valid_trades):
    """Analyze winning and losing streaks"""
    if valid_trades.empty:
        return None, None, None, None, None
    
    # Sort trades by date and time
    sorted_trades = valid_trades.sort_values(['date', 'confirm_time'])
    
    # Create list of wins (1) and losses (0)
    results = [1 if r > 0 else 0 for r in sorted_trades['pnl_R'].values]
    
    # Find streaks
    win_streaks = []
    loss_streaks = []
    
    current_streak = 1
    current_type = results[0] if len(results) > 0 else None
    
    for i in range(1, len(results)):
        if results[i] == results[i-1]:
            current_streak += 1
        else:
            if current_type == 1:
                win_streaks.append(current_streak)
            else:
                loss_streaks.append(current_streak)
            current_streak = 1
            current_type = results[i]
    
    # Add the last streak
    if current_type == 1:
        win_streaks.append(current_streak)
    elif current_type == 0:
        loss_streaks.append(current_streak)
    
    # Calculate streak statistics
    max_win_streak = max(win_streaks) if win_streaks else 0
    max_loss_streak = max(loss_streaks) if loss_streaks else 0
    avg_win_streak = sum(win_streaks) / len(win_streaks) if win_streaks else 0
    avg_loss_streak = sum(loss_streaks) / len(loss_streaks) if loss_streaks else 0
    
    return max_win_streak, max_loss_streak, avg_win_streak, avg_loss_streak, win_streaks + loss_streaks

def generate_equity_curve(valid_trades, output_dir, title, period_type=""):
    """
    Generate equity curve chart from trade data with improved labeling
    
    Parameters:
    valid_trades (DataFrame): DataFrame containing trade results
    output_dir (str): Directory to save the chart
    title (str): Title for the chart
    period_type (str): "Backtest" or "Walk-Forward" to indicate the period type
    """
    if valid_trades.empty:
        print(f"No valid trades to generate equity curve for {period_type}")
        return
    
    if not MATPLOTLIB_AVAILABLE:
        print("Matplotlib not available. Cannot generate equity curve.")
        return
    
    # Create output directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)
    
    # Sort trades by date and time
    sorted_trades = valid_trades.sort_values(['date', 'confirm_time'])
    
    # Calculate cumulative R
    sorted_trades['cumulative_R'] = sorted_trades['pnl_R'].cumsum()
    
    # Convert date to datetime for plotting
    sorted_trades['datetime'] = pd.to_datetime(sorted_trades['date']) + pd.to_timedelta(
        sorted_trades['confirm_time'].dt.hour * 3600 + 
        sorted_trades['confirm_time'].dt.minute * 60 + 
        sorted_trades['confirm_time'].dt.second, unit='s')
    
    # Create figure
    plt.figure(figsize=(12, 6))
    
    # Generate a subtitle to indicate period type
    full_title = title
    if period_type:
        full_title = f"{period_type}: {title}"
    
    plt.plot(sorted_trades['datetime'], sorted_trades['cumulative_R'], 
             linestyle='-', marker='o', markersize=3)
    
    # Add moving average if enough points
    if len(sorted_trades) > 20:
        window = min(20, len(sorted_trades) // 5)
        sorted_trades['ma'] = sorted_trades['cumulative_R'].rolling(window=window).mean()
        plt.plot(sorted_trades['datetime'], sorted_trades['ma'], 'r--', 
                 label=f'{window}-Trade Moving Average')
    
    # Add drawdown
    running_max = sorted_trades['cumulative_R'].expanding().max()
    drawdown = sorted_trades['cumulative_R'] - running_max
    max_drawdown = drawdown.min()
    max_dd_idx = drawdown.idxmin()
    
    # Format the plot
    plt.grid(True, alpha=0.3)
    plt.title(full_title)
    plt.xlabel('Date')
    plt.ylabel('Cumulative R')
    
    # Format x-axis dates
    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))
    plt.gcf().autofmt_xdate()
    
    # Add annotations
    total_r = sorted_trades['pnl_R'].sum()
    profit_factor = calculate_profit_factor(sorted_trades)
    win_rate, _, _, expectancy = calculate_expectancy(sorted_trades)
    
    annotation = (f'Total R: {total_r:.2f}\n'
                  f'Max Drawdown: {max_drawdown:.2f}R\n'
                  f'Profit Factor: {profit_factor:.2f}\n'
                  f'Win Rate: {win_rate:.1f}%\n'
                  f'Expectancy: {expectancy:.2f}R\n'
                  f'Trades: {len(sorted_trades)}')
    
    plt.annotate(annotation, xy=(0.02, 0.02), xycoords='axes fraction', 
                 bbox=dict(boxstyle="round,pad=0.5", facecolor='white', alpha=0.8))
    
    if len(sorted_trades) > 20:
        plt.legend()
    
    # Mark maximum drawdown
    if max_dd_idx in sorted_trades.index:
        plt.plot(sorted_trades.loc[max_dd_idx, 'datetime'], 
                 sorted_trades.loc[max_dd_idx, 'cumulative_R'], 
                 'rv', markersize=8, label='Max Drawdown')
    
    # Save the chart
    period_tag = f"_{period_type.lower().replace(' ', '_')}" if period_type else ""
    filename = f"{output_dir}/equity_curve_{title.replace(' ', '_')}{period_tag}.png"
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"Equity curve saved to {filename}")
    
    return max_drawdown

# =========================== TRADE SIMULATION FUNCTIONS ===========================

def get_applicable_wdr(session_date, tuesday_dates, tuesday_wdr):
    """
    Find the most recent Tuesday's WDR that applies to this session date.
    For Tuesday sessions, use the previous Tuesday's WDR.
    """
    if pd.Timestamp(session_date).day_name() == 'Tuesday':
        # For Tuesday, find the previous Tuesday
        prev_tuesdays = [date for date in tuesday_dates if date < session_date]
        if prev_tuesdays:
            prev_tuesday = max(prev_tuesdays)  # Most recent previous Tuesday
            if prev_tuesday in tuesday_wdr:
                return prev_tuesday, tuesday_wdr[prev_tuesday]
    else:
        # For other days, find the most recent Tuesday on or before this date
        applicable_tuesdays = [date for date in tuesday_dates if date <= session_date]
        if applicable_tuesdays:
            most_recent_tuesday = max(applicable_tuesdays)  # Most recent Tuesday
            if most_recent_tuesday in tuesday_wdr:
                return most_recent_tuesday, tuesday_wdr[most_recent_tuesday]
    
    # No applicable WDR found
    return None, None

def calculate_m7b(session_df):
    """
    Calculate the M7 Box (M7B) for a session.
    M7B is defined as the range between the open price at 9:30 and close price at 10:25.
    """
    # Ensure timestamp column exists
    if "timestamp" not in session_df.columns:
        print(f"Error: 'timestamp' column not found in calculate_m7b for session date {session_df['date'].iloc[0] if 'date' in session_df.columns else 'unknown'}")
        return None, None
    
    try:
        # Get the 9:30 candle (market open)
        market_open_df = session_df[session_df["timestamp"].dt.time == pd.to_datetime("09:30").time()]
        
        # Get the 10:25 candle (end of DR period)
        dr_end_df = session_df[session_df["timestamp"].dt.time == pd.to_datetime("10:25").time()]
        
        # Debug print if we can't find these candles
        if market_open_df.empty or dr_end_df.empty:
            session_date = session_df["date"].iloc[0] if not session_df.empty else "unknown"
            return None, None
        
        # Get the open price at 9:30 and close price at 10:25
        open_price = market_open_df.iloc[0]["open"]
        close_price = dr_end_df.iloc[0]["close"]
        
        # Define M7B upper and lower boundaries
        m7b_upper = max(open_price, close_price)
        m7b_lower = min(open_price, close_price)
        
        return m7b_upper, m7b_lower
    except Exception as e:
        print(f"Error in calculate_m7b: {e}")
        return None, None

def simulate_trade(post_trade_df, entry_price, TP, SL, direction, tick_size):
    """
    Simulate a trade from the entry candle until 15:55.
    For long trades:
       - If candle's low <= SL, then SL is triggered.
       - If candle's high >= TP, then TP is triggered.
    For short trades:
       - If candle's high >= SL, then SL is triggered.
       - If candle's low <= TP, then TP is triggered.
    If both occur in the same candle, assume stop loss.
    """
    for idx, row in post_trade_df.iterrows():
        if direction == "long":
            if row["low"] <= SL and row["high"] >= TP:
                return SL, "SL"
            elif row["low"] <= SL:
                return SL, "SL"
            elif row["high"] >= TP:
                return TP, "TP"
        elif direction == "short":
            if row["high"] >= SL and row["low"] <= TP:
                return SL, "SL"
            elif row["high"] >= SL:
                return SL, "SL"
            elif row["low"] <= TP:
                return TP, "TP"
    return post_trade_df.iloc[-1]["close"], "EXIT"

# =========================== SESSION ANALYSIS FUNCTIONS ===========================

def get_long_session_metrics_with_sd_retracement(session_df, sd_retracement, params, tuesday_dates=None, tuesday_wdr=None):
    """Process long trade setup for a session using SD retracement and Target SD parameters"""
    # Check if the required columns exist
    if "timestamp" not in session_df.columns:
        print(f"Error: 'timestamp' column not found in the DataFrame for session date {session_df['date'].iloc[0] if 'date' in session_df.columns else 'unknown'}")
        print(f"Available columns: {session_df.columns}")
        return None
    
    # Ensure timestamp is a datetime type
    if not pd.api.types.is_datetime64_dtype(session_df["timestamp"]):
        try:
            session_df = session_df.copy()
            session_df["timestamp"] = pd.to_datetime(session_df["timestamp"])
        except Exception as e:
            print(f"Error converting timestamp to datetime: {e}")
            return None
    
    # Get time windows from parameters
    long_confirm_start = pd.to_datetime(params["long_confirm_start"]).time()
    long_confirm_end = pd.to_datetime(params["long_confirm_end"]).time()
    tick_size = params["tick_size"]
    
    # Get necessary price data
    try:
        dr_df = session_df[(session_df["timestamp"].dt.time >= pd.to_datetime("09:30").time()) &
                          (session_df["timestamp"].dt.time <= pd.to_datetime("10:25").time())]
        if dr_df.empty:
            return None
    except Exception as e:
        print(f"Error filtering DR window: {e}")
        print(f"timestamp dtype: {session_df['timestamp'].dtype}")
        print(f"timestamp sample: {session_df['timestamp'].head()}")
        return None
    
    DR_high = dr_df["high"].max()
    DR_low = dr_df["low"].min()
    IDR_high = dr_df["close"].max()
    IDR_low = dr_df["close"].min()
    SD = IDR_high - IDR_low if (IDR_high - IDR_low) != 0 else np.nan
    
    # Get the session date
    session_date = session_df["date"].iloc[0]
    day_of_week = pd.Timestamp(session_date).day_name()
    
    # Check WDR - get info, don't filter yet
    wdr_tuesday = None
    wdr_high = None
    wdr_low = None
    wdr_reason = None
    
    if params["use_wdr_filter"] and tuesday_dates is not None and tuesday_wdr is not None:
        wdr_tuesday, wdr_info = get_applicable_wdr(session_date, tuesday_dates, tuesday_wdr)
        if wdr_info:
            wdr_high = wdr_info["dr_high"]
            wdr_low = wdr_info["dr_low"]
    
    # Calculate M7B (M7 Box)
    m7b_upper, m7b_lower = calculate_m7b(session_df)
    m7b_invalid = False
    m7b_reason = None
    
    # Check confirmation
    confirm_df = session_df[(session_df["timestamp"].dt.time >= long_confirm_start) &
                            (session_df["timestamp"].dt.time <= long_confirm_end)]
    confirm_signal = confirm_df[confirm_df["close"] > DR_high]
    if confirm_signal.empty:
        return None
    
    confirm_row = confirm_signal.iloc[0]
    confirm_time = confirm_row["timestamp"]
    confirm_time_str = confirm_time.strftime("%H:%M")
    
    # Check if close is above DR_low
    session_close = session_df.iloc[-1]["close"]
    if session_close < DR_low:
        return None
    
    # Calculate entry based on SD retracement
    baseline = IDR_high
    candidate_entry = baseline + sd_retracement * SD
    
    # Calculate TP based on Target SD
    target_sd = params["default_target_sd"]  # Use the default target SD
    TP = baseline + target_sd * SD
    
    # Find entry and simulate trade
    entry_window_df = session_df[(session_df["timestamp"].dt.time >= pd.to_datetime("10:40").time()) &
                                (session_df["timestamp"].dt.time <= pd.to_datetime("14:00").time()) &
                                (session_df["timestamp"] > confirm_time)]
    
    if entry_window_df.empty:
        return None
    
    # Find entry
    candidate_entries = entry_window_df[entry_window_df["low"] <= candidate_entry]
    if candidate_entries.empty:
        trade_entry = None
        trade_result = None
    else:
        # Process entry and simulate trade
        candidate_entry_index = candidate_entries.index[0]
        pre_candidate_df = entry_window_df.loc[:candidate_entry_index]
        
        # Check if TP has already been hit before entry
        if not pre_candidate_df.empty and pre_candidate_df["high"].max() >= TP:
            trade_entry = None
            trade_result = None
        else:
            # For long, check if price fell below threshold before entry
            check_df = entry_window_df.loc[entry_window_df.index < candidate_entry_index]
            long_threshold = candidate_entry - params["sl_distance"] * SD
            
            if not check_df.empty and check_df["low"].min() < long_threshold:
                trade_entry = None
                trade_result = None
            else:
                trade_entry = candidate_entry
                SL = trade_entry - params["sl_distance"] * SD - (params["sl_extra_ticks"] * tick_size)
                entry_time = entry_window_df.loc[candidate_entry_index, "timestamp"]
                
                # Check M7B filter if enabled
                if params["use_m7b_filter"] and m7b_lower is not None and candidate_entry < m7b_lower:
                    m7b_invalid = True
                    m7b_reason = "Entry below M7B lower boundary"
                
                # Simulate trade
                post_trade_df = session_df[(session_df["timestamp"] >= entry_time) &
                                          (session_df["timestamp"].dt.time <= pd.to_datetime("15:55").time())]
                
                if post_trade_df.empty:
                    trade_result = None
                else:
                    exit_price, exit_reason = simulate_trade(post_trade_df, trade_entry, TP, SL, "long", tick_size)
                    risk = trade_entry - SL
                    trade_result = {
                        "exit_reason": exit_reason,
                        "pnl_price": exit_price - trade_entry,
                        "pnl_R": (exit_price - trade_entry) / risk if risk != 0 else np.nan,
                        "pnl_ticks": (exit_price - trade_entry) / tick_size
                    }
    
    # Calculate post confirm price movement for analytics
    post_confirm_df = session_df[(session_df["timestamp"] > confirm_time) &
                                (session_df["timestamp"].dt.time <= pd.to_datetime("15:55").time())]
    
    if post_confirm_df.empty:
        return None
        
    max_high = post_confirm_df["high"].max()
    extreme_row = post_confirm_df[post_confirm_df["high"] == max_high].iloc[0]
    extension_value = max_high - baseline
    retracement_window = post_confirm_df[post_confirm_df["timestamp"] < extreme_row["timestamp"]]
    
    if retracement_window.empty:
        return None
        
    retracement_value = retracement_window["low"].min() - baseline
    extension_SD = extension_value / SD if pd.notna(SD) and SD != 0 else np.nan
    retracement_SD = retracement_value / SD if pd.notna(SD) and SD != 0 else np.nan
    
    # Check WDR conditions
    wdr_session_invalid = False
    wdr_cluster_invalid = False
    
    if params["use_wdr_filter"] and wdr_high is not None and wdr_low is not None:
        # Check if both IDR high and low are inside WDR
        if (wdr_low <= IDR_high <= wdr_high) and (wdr_low <= IDR_low <= wdr_high):
            wdr_session_invalid = True
            wdr_reason = "IDR inside WDR"
        # Check if TP is inside WDR
        elif wdr_low <= TP <= wdr_high:
            wdr_session_invalid = True
            wdr_reason = "TP inside WDR"
        # Check if entry is inside WDR
        elif wdr_low <= candidate_entry <= wdr_high:
            wdr_cluster_invalid = True
            wdr_reason = "Entry inside WDR"
    
    # Build output
    output = {
        "date": session_date,
        "trade_direction": "long",
        "day_of_week": day_of_week,
        "confirm_time": confirm_time,
        "confirm_time_str": confirm_time_str,
        "DR_high": DR_high,
        "DR_low": DR_low,
        "IDR_high": IDR_high,
        "IDR_low": IDR_low,
        "SD": SD,
        "session_close": session_close,
        "baseline": baseline,
        "sd_retracement": sd_retracement,
        "target_sd": target_sd,
        "TP": TP,
        "retracement": retracement_value,
        "retracement_SD": retracement_SD,
        "extension": extension_value,
        "extension_SD": extension_SD,
        "wdr_session_invalid": wdr_session_invalid,
        "wdr_cluster_invalid": wdr_cluster_invalid,
        "wdr_reason": wdr_reason,
    }
    
    # Add M7B information to output
    if m7b_upper is not None and m7b_lower is not None:
        output.update({
            "m7b_upper": m7b_upper,
            "m7b_lower": m7b_lower,
            "m7b_invalid": m7b_invalid,
            "m7b_reason": m7b_reason,
        })
    
    if wdr_tuesday is not None:
        output.update({
            "wdr_tuesday": wdr_tuesday,
            "wdr_high": wdr_high,
            "wdr_low": wdr_low,
        })
    
    # Add trade results
    if trade_result is not None:
        # Store unfiltered values
        output_values = {
            "unfiltered_trade_entry": trade_entry,
            "unfiltered_exit_reason": trade_result["exit_reason"],
            "unfiltered_pnl_price": trade_result["pnl_price"],
            "unfiltered_pnl_R": trade_result["pnl_R"],
            "unfiltered_pnl_ticks": trade_result["pnl_ticks"]
        }
        
        # Store filtered values (NaN if m7b_invalid or wdr invalid)
        if m7b_invalid or wdr_session_invalid or wdr_cluster_invalid:
            output_values.update({
                "trade_entry": np.nan,
                "SL": np.nan,
                "trade_exit": np.nan,
                "exit_reason": np.nan,
                "pnl_price": np.nan,
                "pnl_R": np.nan,
                "pnl_ticks": np.nan
            })
        else:
            output_values.update({
                "trade_entry": trade_entry,
                "SL": SL,
                "trade_exit": exit_price,
                "exit_reason": trade_result["exit_reason"],
                "pnl_price": trade_result["pnl_price"],
                "pnl_R": trade_result["pnl_R"],
                "pnl_ticks": trade_result["pnl_ticks"]
            })
        
        output.update(output_values)
    else:
        output.update({
            "trade_entry": np.nan,
            "SL": np.nan,
            "trade_exit": np.nan,
            "exit_reason": np.nan,
            "pnl_price": np.nan,
            "pnl_R": np.nan,
            "pnl_ticks": np.nan,
            "unfiltered_trade_entry": np.nan,
            "unfiltered_exit_reason": np.nan,
            "unfiltered_pnl_price": np.nan,
            "unfiltered_pnl_R": np.nan,
            "unfiltered_pnl_ticks": np.nan
        })
    
    return output

def get_short_session_metrics_with_sd_retracement(session_df, sd_retracement, params, tuesday_dates=None, tuesday_wdr=None):
    """Process short trade setup for a session using SD retracement and Target SD parameters"""
    # Check if the required columns exist
    if "timestamp" not in session_df.columns:
        print(f"Error: 'timestamp' column not found in the DataFrame for session date {session_df['date'].iloc[0] if 'date' in session_df.columns else 'unknown'}")
        print(f"Available columns: {session_df.columns}")
        return None
    
    # Ensure timestamp is a datetime type
    if not pd.api.types.is_datetime64_dtype(session_df["timestamp"]):
        try:
            session_df = session_df.copy()
            session_df["timestamp"] = pd.to_datetime(session_df["timestamp"])
        except Exception as e:
            print(f"Error converting timestamp to datetime: {e}")
            return None
    
    # Get time windows from parameters
    short_confirm_start = pd.to_datetime(params["short_confirm_start"]).time()
    short_confirm_end = pd.to_datetime(params["short_confirm_end"]).time()
    tick_size = params["tick_size"]
    
    # Get necessary price data
    try:
        dr_df = session_df[(session_df["timestamp"].dt.time >= pd.to_datetime("09:30").time()) &
                          (session_df["timestamp"].dt.time <= pd.to_datetime("10:25").time())]
        if dr_df.empty:
            return None
    except Exception as e:
        print(f"Error filtering DR window: {e}")
        print(f"timestamp dtype: {session_df['timestamp'].dtype}")
        print(f"timestamp sample: {session_df['timestamp'].head()}")
        return None
    
    DR_high = dr_df["high"].max()
    DR_low = dr_df["low"].min()
    IDR_high = dr_df["close"].max()
    IDR_low = dr_df["close"].min()
    SD = IDR_high - IDR_low if (IDR_high - IDR_low) != 0 else np.nan
    
    # Get the session date
    session_date = session_df["date"].iloc[0]
    day_of_week = pd.Timestamp(session_date).day_name()
    
    # Check WDR - get info, don't filter yet
    wdr_tuesday = None
    wdr_high = None
    wdr_low = None
    wdr_reason = None
    
    if params["use_wdr_filter"] and tuesday_dates is not None and tuesday_wdr is not None:
        wdr_tuesday, wdr_info = get_applicable_wdr(session_date, tuesday_dates, tuesday_wdr)
        if wdr_info:
            wdr_high = wdr_info["dr_high"]
            wdr_low = wdr_info["dr_low"]
    
    # Calculate M7B (M7 Box)
    m7b_upper, m7b_lower = calculate_m7b(session_df)
    m7b_invalid = False
    m7b_reason = None
    
    # Check confirmation
    confirm_df = session_df[(session_df["timestamp"].dt.time >= short_confirm_start) &
                            (session_df["timestamp"].dt.time <= short_confirm_end)]
    confirm_signal = confirm_df[confirm_df["close"] < DR_low]
    if confirm_signal.empty:
        return None
    
    confirm_row = confirm_signal.iloc[0]
    confirm_time = confirm_row["timestamp"]
    confirm_time_str = confirm_time.strftime("%H:%M")
    
    # Check if close is below DR_high
    session_close = session_df.iloc[-1]["close"]
    if session_close > DR_high:
        return None
    
    # Calculate entry based on SD retracement
    baseline = IDR_low
    candidate_entry = baseline - sd_retracement * SD
    
    # Calculate TP based on Target SD
    target_sd = params.get("target_sd", params["default_target_sd"])  # Use target_sd if provided, else default
    TP = baseline - target_sd * SD
    
    # Find entry and simulate trade
    entry_window_df = session_df[(session_df["timestamp"].dt.time >= pd.to_datetime("10:40").time()) &
                                (session_df["timestamp"].dt.time <= pd.to_datetime("14:00").time()) &
                                (session_df["timestamp"] > confirm_time)]
    
    if entry_window_df.empty:
        return None
    
    # Find entry
    candidate_entries = entry_window_df[entry_window_df["high"] >= candidate_entry]
    if candidate_entries.empty:
        trade_entry = None
        trade_result = None
    else:
        # Process entry and simulate trade
        candidate_entry_index = candidate_entries.index[0]
        pre_candidate_df = entry_window_df.loc[:candidate_entry_index]
        
        # Check if TP has already been hit before entry
        if not pre_candidate_df.empty and pre_candidate_df["low"].min() <= TP:
            trade_entry = None
            trade_result = None
        else:
            # For short, check if price rose above threshold before entry
            check_df = entry_window_df.loc[entry_window_df.index < candidate_entry_index]
            short_threshold = candidate_entry + params["sl_distance"] * SD
            
            if not check_df.empty and check_df["high"].max() > short_threshold:
                trade_entry = None
                trade_result = None
            else:
                trade_entry = candidate_entry
                SL = trade_entry + params["sl_distance"] * SD + (params["sl_extra_ticks"] * tick_size)
                entry_time = entry_window_df.loc[candidate_entry_index, "timestamp"]
                
                # Check M7B filter if enabled
                if params["use_m7b_filter"] and m7b_upper is not None and candidate_entry > m7b_upper:
                    m7b_invalid = True
                    m7b_reason = "Entry above M7B upper boundary"
                
                # Simulate trade
                post_trade_df = session_df[(session_df["timestamp"] >= entry_time) &
                                          (session_df["timestamp"].dt.time <= pd.to_datetime("15:55").time())]
                
                if post_trade_df.empty:
                    trade_result = None
                else:
                    exit_price, exit_reason = simulate_trade(post_trade_df, trade_entry, TP, SL, "short", tick_size)
                    risk = SL - trade_entry
                    trade_result = {
                        "exit_reason": exit_reason,
                        "pnl_price": trade_entry - exit_price,
                        "pnl_R": (trade_entry - exit_price) / risk if risk != 0 else np.nan,
                        "pnl_ticks": (trade_entry - exit_price) / tick_size
                    }
    
    # Calculate post confirm price movement for analytics
    post_confirm_df = session_df[(session_df["timestamp"] > confirm_time) &
                                (session_df["timestamp"].dt.time <= pd.to_datetime("15:55").time())]
    
    if post_confirm_df.empty:
        return None
        
    min_low = post_confirm_df["low"].min()
    extreme_row = post_confirm_df[post_confirm_df["low"] == min_low].iloc[0]
    extension_value = baseline - min_low  # Favorable move down
    retracement_window = post_confirm_df[post_confirm_df["timestamp"] < extreme_row["timestamp"]]
    
    if retracement_window.empty:
        return None
        
    retracement_value = retracement_window["high"].max() - baseline  # Adverse move up
    extension_SD = extension_value / SD if pd.notna(SD) and SD != 0 else np.nan
    retracement_SD = retracement_value / SD if pd.notna(SD) and SD != 0 else np.nan
    
    # Check WDR conditions
    wdr_session_invalid = False
    wdr_cluster_invalid = False
    
    if params["use_wdr_filter"] and wdr_high is not None and wdr_low is not None:
        # Check if both IDR high and low are inside WDR
        if (wdr_low <= IDR_high <= wdr_high) and (wdr_low <= IDR_low <= wdr_high):
            wdr_session_invalid = True
            wdr_reason = "IDR inside WDR"
        # Check if TP is inside WDR
        elif wdr_low <= TP <= wdr_high:
            wdr_session_invalid = True
            wdr_reason = "TP inside WDR"
        # Check if entry is inside WDR
        elif wdr_low <= candidate_entry <= wdr_high:
            wdr_cluster_invalid = True
            wdr_reason = "Entry inside WDR"
    
    # Build output
    output = {
        "date": session_date,
        "trade_direction": "short",
        "day_of_week": day_of_week,
        "confirm_time": confirm_time,
        "confirm_time_str": confirm_time_str,
        "DR_high": DR_high,
        "DR_low": DR_low,
        "IDR_high": IDR_high,
        "IDR_low": IDR_low,
        "SD": SD,
        "session_close": session_close,
        "baseline": baseline,
        "sd_retracement": sd_retracement,
        "target_sd": target_sd,
        "TP": TP,
        "retracement": retracement_value,
        "retracement_SD": retracement_SD,
        "extension": extension_value,
        "extension_SD": extension_SD,
        "wdr_session_invalid": wdr_session_invalid,
        "wdr_cluster_invalid": wdr_cluster_invalid,
        "wdr_reason": wdr_reason,
    }
    
    # Add M7B information to output
    if m7b_upper is not None and m7b_lower is not None:
        output.update({
            "m7b_upper": m7b_upper,
            "m7b_lower": m7b_lower,
            "m7b_invalid": m7b_invalid,
            "m7b_reason": m7b_reason,
        })
    
    if wdr_tuesday is not None:
        output.update({
            "wdr_tuesday": wdr_tuesday,
            "wdr_high": wdr_high,
            "wdr_low": wdr_low,
        })
    
    # Add trade results
    if trade_result is not None:
        # Store unfiltered values
        output_values = {
            "unfiltered_trade_entry": trade_entry,
            "unfiltered_exit_reason": trade_result["exit_reason"],
            "unfiltered_pnl_price": trade_result["pnl_price"],
            "unfiltered_pnl_R": trade_result["pnl_R"],
            "unfiltered_pnl_ticks": trade_result["pnl_ticks"]
        }
        
        # Store filtered values (NaN if m7b_invalid or wdr invalid)
        if m7b_invalid or wdr_session_invalid or wdr_cluster_invalid:
            output_values.update({
                "trade_entry": np.nan,
                "SL": np.nan,
                "trade_exit": np.nan,
                "exit_reason": np.nan,
                "pnl_price": np.nan,
                "pnl_R": np.nan,
                "pnl_ticks": np.nan
            })
        else:
            output_values.update({
                "trade_entry": trade_entry,
                "SL": SL,
                "trade_exit": exit_price,
                "exit_reason": trade_result["exit_reason"],
                "pnl_price": trade_result["pnl_price"],
                "pnl_R": trade_result["pnl_R"],
                "pnl_ticks": trade_result["pnl_ticks"]
            })
        
        output.update(output_values)
    else:
        output.update({
            "trade_entry": np.nan,
            "SL": np.nan,
            "trade_exit": np.nan,
            "exit_reason": np.nan,
            "pnl_price": np.nan,
            "pnl_R": np.nan,
            "pnl_ticks": np.nan,
            "unfiltered_trade_entry": np.nan,
            "unfiltered_exit_reason": np.nan,
            "unfiltered_pnl_price": np.nan,
            "unfiltered_pnl_R": np.nan,
            "unfiltered_pnl_ticks": np.nan
        })
    
    return output

def process_all_sessions(df, sd_retracement_levels, params, tuesday_dates=None, tuesday_wdr=None):
    """Process all sessions in the dataframe"""
    results = []
    
    # Group by date to process session by session
    for date, group_df in df.groupby('date'):
        day_name = pd.Timestamp(date).day_name()
        
        # Reset index to ensure columns access
        session_df = group_df.reset_index(drop=True)
        
        # Check if we have the timestamp column
        if "timestamp" not in session_df.columns:
            print(f"Warning: timestamp column missing for date {date}")
            print(f"Available columns: {session_df.columns}")
            continue
            
        # Skip if selected_days is specified and this day is not in it
        if params['selected_days'] is not None and day_name not in params['selected_days']:
            continue
        
        # Process each SD retracement level for this session
        for sd_retracement in sd_retracement_levels:
            # Process long trade if requested
            if params['analysis_mode'] in ['long', 'both']:
                metrics = get_long_session_metrics_with_sd_retracement(
                    session_df, sd_retracement, params, tuesday_dates, tuesday_wdr
                )
                if metrics:
                    results.append(metrics)
            
            # Process short trade if requested
            if params['analysis_mode'] in ['short', 'both']:
                metrics = get_short_session_metrics_with_sd_retracement(
                    session_df, sd_retracement, params, tuesday_dates, tuesday_wdr
                )
                if metrics:
                    results.append(metrics)
    
    return results

# =========================== PARAMETER SWEEP FUNCTIONS ===========================

def process_parameter_combo(combo_args):
    """Process a single parameter combination for SD retracement and Target SD"""
    sd_retrace, target_sd, df, params = combo_args
    
    # Create parameters for this run
    sweep_params = params.copy()
    sweep_params['default_sd_retracement'] = sd_retrace
    sweep_params['default_target_sd'] = target_sd
    
    # Process all combinations of day, confirmation time, and direction
    results = []
    
    # Group by date to process session by session
    for date, group_df in df.groupby('date'):
        # Reset index to ensure column access
        session_df = group_df.reset_index(drop=True)
        
        # Check if timestamp column exists
        if "timestamp" not in session_df.columns:
            print(f"Warning: timestamp column missing in parameter sweep for date {date}")
            print(f"Available columns: {session_df.columns}")
            continue
            
        day_name = pd.Timestamp(date).day_name()
        
        # For each possible confirmation time
        conf_times = get_confirmation_times(params)
        
        for conf_time in conf_times:
            # Convert string time to datetime.time for filtering
            conf_time_obj = pd.to_datetime(conf_time).time()
            
            # For each direction (if requested)
            directions = []
            if params['analysis_mode'] in ['long', 'both']:
                directions.append('long')
            if params['analysis_mode'] in ['short', 'both']:
                directions.append('short')
            
            for direction in directions:
                # Process this specific combination
                if direction == 'long':
                    # Check if time is in long confirmation window
                    long_start = pd.to_datetime(params["long_confirm_start"]).time()
                    long_end = pd.to_datetime(params["long_confirm_end"]).time()
                    
                    if not (long_start <= conf_time_obj <= long_end):
                        continue
                    
                    metrics = get_long_session_metrics_with_sd_retracement(
                        session_df, sd_retrace, sweep_params
                    )
                else:
                    # Check if time is in short confirmation window
                    short_start = pd.to_datetime(params["short_confirm_start"]).time()
                    short_end = pd.to_datetime(params["short_confirm_end"]).time()
                    
                    if not (short_start <= conf_time_obj <= short_end):
                        continue
                    
                    metrics = get_short_session_metrics_with_sd_retracement(
                        session_df, sd_retrace, sweep_params
                    )
                
                if metrics:
                    # Ensure confirm_time_str matches our target
                    if metrics['confirm_time_str'] == conf_time:
                        results.append({
                            'date': metrics['date'],
                            'day': day_name,
                            'confirm_time': conf_time,
                            'direction': direction,
                            'sd_retracement': sd_retrace,
                            'target_sd': target_sd,
                            'trade_entry': metrics.get('trade_entry'),
                            'exit_reason': metrics.get('exit_reason'),
                            'pnl_R': metrics.get('pnl_R')
                        })
    
    return results

def get_confirmation_times(params):
    """Extract all possible confirmation times from parameters"""
    confirmation_times = []
    
    # Long confirmation times
    long_start = pd.to_datetime(params["long_confirm_start"]).time()
    long_end = pd.to_datetime(params["long_confirm_end"]).time()
    
    # Short confirmation times
    short_start = pd.to_datetime(params["short_confirm_start"]).time()
    short_end = pd.to_datetime(params["short_confirm_end"]).time()
    
    # Create a list of all possible 5-minute intervals in these ranges
    for hour in range(9, 16):  # 9 AM to 3 PM
        for minute in range(0, 60, 5):  # 5-minute intervals
            time_str = f"{hour:02d}:{minute:02d}"
            time_obj = pd.to_datetime(time_str).time()
            
            # Check if in long confirmation window
            if long_start <= time_obj <= long_end:
                confirmation_times.append(time_str)
            
            # Check if in short confirmation window and not already added
            elif short_start <= time_obj <= short_end and time_str not in confirmation_times:
                confirmation_times.append(time_str)
    
    return sorted(confirmation_times)

def perform_parameter_sweep(df, params):
    """
    Perform parameter sweep with CPU and temperature management
    with enhanced visualization output
    """
    print("\n" + "=" * 100)
    print(" " * 35 + "PARAMETER SWEEP ANALYSIS")
    print("=" * 100)
    
    # Use ALL the SD retracement levels for parameter sweep
    sd_retracement_ranges = params['sd_retracement_levels']  # Use all values, not just positive ones
    target_sd_ranges = params['target_sd_ranges']
    
    # Print parameter ranges
    print(f"Testing {len(sd_retracement_ranges) * len(target_sd_ranges)} parameter combinations...")
    print(f"SD Retracement values: {sd_retracement_ranges}")
    print(f"Target SD values: {target_sd_ranges}")
    print(f"Fixed SL distance: {params['sl_distance']} SD")
    print(f"Fixed SL extra ticks: {params['sl_extra_ticks']}")
    
    # Create combinations
    from itertools import product
    param_combinations = list(product(sd_retracement_ranges, target_sd_ranges))
    
    total_combinations = len(param_combinations)
    print(f"Testing {total_combinations} parameter combinations...")
    
    # Process combinations with resource management
    sweep_results = []
    
    # Create a list of args for each combination
    combo_args_list = [(sd_retrace, target, df, params) for sd_retrace, target in param_combinations]
    
    # Use the resource management function
    if params['use_multiprocessing']:
        combo_results = process_with_resource_management(
            process_parameter_combo, 
            combo_args_list, 
            params,
            "Processing parameter combinations"
        )
        
        # Flatten the results (since each result might be a list)
        for result in combo_results:
            if result:  # Only add if result is not empty
                sweep_results.extend(result)
    else:
        # Sequential processing with resource management
        for i, combo_args in enumerate(combo_args_list):
            # Check system load periodically
            if i % 5 == 0 and check_system_load(params):
                print(f"Pausing for {params['cooling_pause_seconds']} seconds to cool down...")
                time.sleep(params['cooling_pause_seconds'])
            
            # Process this combination
            result = process_parameter_combo(combo_args)
            if result:  # Only add if result is not empty
                sweep_results.extend(result)
            
            # Update progress
            progress = ((i + 1) / total_combinations) * 100
            sd_retrace, target, _, _ = combo_args
            sys.stdout.write(f"\rProgress: {progress:.1f}% - Testing Retrace={sd_retrace}, Target={target}")
            sys.stdout.flush()
        
        print()  # New line after progress
    
    # Convert to DataFrame for analysis
    sweep_results_df = pd.DataFrame(sweep_results)
    
    if sweep_results_df.empty or len(sweep_results_df) == 0:
        print("No valid parameter combinations found")
        return {}
    
    # Save all raw results
    output_dir = params['file_options']['output_dir']
    os.makedirs(output_dir, exist_ok=True)
    sweep_results_df.to_csv(f"{output_dir}/all_parameter_sweep_results.csv", index=False)
    print(f"All parameter sweep results saved to {output_dir}/all_parameter_sweep_results.csv")
    
    # Group by day, confirmation time, and direction
    grouped = sweep_results_df.groupby(['day', 'confirm_time', 'direction', 'sd_retracement', 'target_sd'])
    
    # Calculate metrics for each configuration
    config_metrics = []
    
    for name, group in grouped:
        day, conf_time, direction, sd_retrace, target = name
        valid_trades = group[group["trade_entry"].notna()]
        
        if len(valid_trades) >= params["min_trades"]:  # Only consider configurations with enough trades
            win_rate, avg_winner, avg_loser, expectancy = calculate_expectancy(valid_trades)
            total_r = valid_trades["pnl_R"].sum()
            
            if params["calculate_profit_factor"]:
                profit_factor = calculate_profit_factor(valid_trades)
            else:
                profit_factor = None
            
            config_entry = {
                'day': day,
                'confirm_time': conf_time,
                'direction': direction,
                'sd_retracement': sd_retrace,
                'target_sd': target,
                'trade_count': len(valid_trades),
                'win_rate': win_rate,
                'total_r': total_r,
                'expectancy': expectancy,
                'profit_factor': profit_factor
            }
            config_metrics.append(config_entry)
    
    # Convert to DataFrame
    config_metrics_df = pd.DataFrame(config_metrics)
    
    if config_metrics_df.empty:
        print("No configurations with sufficient trades to analyze")
        return {}
    
    # Save metrics to CSV
    config_metrics_df.to_csv(f"{output_dir}/parameter_sweep_metrics.csv", index=False)
    print(f"Parameter sweep metrics saved to {output_dir}/parameter_sweep_metrics.csv")
    
    # Find top configurations for each day/time/direction
    top_configs = {}
    
    print("\n" + "=" * 100)
    print(" " * 20 + f"TOP {params['keep_top_configs']} CONFIGURATIONS FOR EACH DAY/TIME/DIRECTION")
    print("=" * 100)
    
    # Group by day, confirmation time, and direction
    day_time_dir_grouped = config_metrics_df.groupby(['day', 'confirm_time', 'direction'])
    
    # Track overall unfiltered total R for each group
    unfiltered_total_r = {}
    
    for group_key, group_df in day_time_dir_grouped:
        day, conf_time, direction = group_key
        
        # Calculate unfiltered total R (all configurations)
        group_total_r = group_df['total_r'].sum()
        unfiltered_total_r[group_key] = group_total_r
        
        # Create a combined score using Total R and Win Rate (with equal weighting)
        group_df['combined_score'] = (
            # Normalize Total R and Win Rate to a 0-1 scale within this group
            (group_df['total_r'] - group_df['total_r'].min()) / (group_df['total_r'].max() - group_df['total_r'].min() + 1e-10) + 
            (group_df['win_rate'] - group_df['win_rate'].min()) / (group_df['win_rate'].max() - group_df['win_rate'].min() + 1e-10)
        )
        
        # Sort by the combined score and get top N
        top_n = min(params['keep_top_configs'], len(group_df))
        top_n_configs = group_df.sort_values('combined_score', ascending=False).head(top_n)
        
        # Store in dictionary
        top_configs[group_key] = top_n_configs
        
        # Print the top configurations
        print(f"\n{day} {direction.capitalize()} at {conf_time}:")
        
        filtered_total_r = top_n_configs['total_r'].sum()
        
        for i, row in top_n_configs.iterrows():
            print(f"  #{i+1}: Retracement SD: {row['sd_retracement']:.1f}, Target SD: {row['target_sd']:.1f}")
            print(f"      Trades: {int(row['trade_count'])}, Win Rate: {row['win_rate']:.1f}%, "
                  f"Total R: {row['total_r']:.2f}, Expectancy: {row['expectancy']:.2f}")
        
        # Check total R constraint
        if group_total_r > 0:
            r_reduction = (group_total_r - filtered_total_r) / group_total_r * 100
            if r_reduction > params['max_r_reduction_pct']:
                print(f"  WARNING: Filtering reduced total R by {r_reduction:.1f}%, "
                      f"which exceeds the {params['max_r_reduction_pct']}% limit")
    
    # Save top configurations
    top_configs_list = []
    for group_key, top_df in top_configs.items():
        day, conf_time, direction = group_key
        for _, row in top_df.iterrows():
            config = row.to_dict()
            config['unfiltered_total_r'] = unfiltered_total_r[group_key]
            top_configs_list.append(config)
    
    top_configs_df = pd.DataFrame(top_configs_list)
    top_configs_df.to_csv(f"{output_dir}/top_configurations.csv", index=False)
    
    print(f"\nTop configurations saved to {output_dir}/top_configurations.csv")
    
    # Create heatmap visualizations if matplotlib is available
    if MATPLOTLIB_AVAILABLE and SEABORN_AVAILABLE:
        try:
            # Create directory for heatmaps
            os.makedirs(f"{output_dir}/heatmaps", exist_ok=True)
            
            # For each day/time/direction combination
            for group_key, group_df in day_time_dir_grouped:
                day, conf_time, direction = group_key
                
                # Skip if not enough data points for a meaningful heatmap
                if len(group_df) < 4:
                    continue
                
                # Create a pivot table for the heatmap
                pivot_data = group_df.pivot_table(
                    index='sd_retracement', 
                    columns='target_sd',
                    values='total_r',
                    aggfunc='sum'
                )
                
                # Create heatmap
                plt.figure(figsize=(10, 8))
                sns.heatmap(pivot_data, cmap='RdYlGn', annot=True, fmt=".2f")
                plt.title(f'{day} {direction.capitalize()} at {conf_time} - Total R')
                
                # Label axes
                plt.xlabel('Target SD')
                plt.ylabel('SD Retracement')
                
                # Save the heatmap
                heatmap_file = f"{output_dir}/heatmaps/{day}_{direction}_{conf_time.replace(':', '')}_heatmap.png"
                plt.savefig(heatmap_file, dpi=300, bbox_inches='tight')
                plt.close()
                
                print(f"Heatmap saved to {heatmap_file}")
            
            # Create overall heatmaps for metrics
            all_params_df = config_metrics_df.groupby(['sd_retracement', 'target_sd']).agg({
                'total_r': 'sum',
                'expectancy': 'mean',
                'win_rate': 'mean',
                'trade_count': 'sum'
            }).reset_index()
            
            # Only create if we have enough data points
            if len(all_params_df) >= 4:
                # Create a figure with subplots for each metric
                fig, axes = plt.subplots(2, 2, figsize=(16, 14))
                
                # Total R heatmap
                total_r_pivot = all_params_df.pivot_table(
                    index='sd_retracement',
                    columns='target_sd',
                    values='total_r'
                )
                sns.heatmap(total_r_pivot, cmap='RdYlGn', annot=True, fmt=".2f", ax=axes[0, 0])
                axes[0, 0].set_title('Total R by Parameter Combination')
                axes[0, 0].set_xlabel('Target SD')
                axes[0, 0].set_ylabel('SD Retracement')
                
                # Expectancy heatmap
                exp_pivot = all_params_df.pivot_table(
                    index='sd_retracement',
                    columns='target_sd',
                    values='expectancy'
                )
                sns.heatmap(exp_pivot, cmap='Blues', annot=True, fmt=".2f", ax=axes[0, 1])
                axes[0, 1].set_title('Expectancy by Parameter Combination')
                axes[0, 1].set_xlabel('Target SD')
                axes[0, 1].set_ylabel('SD Retracement')
                
                # Win Rate heatmap
                win_rate_pivot = all_params_df.pivot_table(
                    index='sd_retracement',
                    columns='target_sd',
                    values='win_rate'
                )
                sns.heatmap(win_rate_pivot, cmap='Greens', annot=True, fmt=".1f", ax=axes[1, 0])
                axes[1, 0].set_title('Win Rate (%) by Parameter Combination')
                axes[1, 0].set_xlabel('Target SD')
                axes[1, 0].set_ylabel('SD Retracement')
                
                # Trade Count heatmap
                trade_count_pivot = all_params_df.pivot_table(
                    index='sd_retracement',
                    columns='target_sd',
                    values='trade_count'
                )
                sns.heatmap(trade_count_pivot, cmap='Purples', annot=True, fmt="d", ax=axes[1, 1])
                axes[1, 1].set_title('Trade Count by Parameter Combination')
                axes[1, 1].set_xlabel('Target SD')
                axes[1, 1].set_ylabel('SD Retracement')
                
                plt.tight_layout()
                
                # Save the overall heatmap
                overall_heatmap_file = f"{output_dir}/parameter_sweep_heatmaps.png"
                plt.savefig(overall_heatmap_file, dpi=300, bbox_inches='tight')
                plt.close()
                
                print(f"Overall parameter sweep heatmaps saved to {overall_heatmap_file}")
        
        except Exception as e:
            print(f"Error creating heatmaps: {str(e)}")
    
    return top_configs

# =========================== WALK FORWARD ANALYSIS FUNCTIONS ===========================

def perform_walk_forward_analysis(df, params, top_configs=None):
    """
    Perform walk-forward analysis with clear separation between backtest and walk-forward periods.
    The function now uses explicit start and end dates for both periods.
    
    Parameters:
    df (DataFrame): DataFrame containing price data
    params (dict): Dictionary containing parameters
    top_configs (dict, optional): Dictionary containing top configurations from parameter sweep
    
    Returns:
    dict: Walk-forward analysis results
    """
    print("\n" + "=" * 100)
    print(" " * 35 + "WALK FORWARD ANALYSIS")
    print("=" * 100)
    
    # Check if we have backtest and walk-forward dates
    backtest_start = params.get('backtest_start_date')
    backtest_end = params.get('backtest_end_date')
    walkforward_start = params.get('walkforward_start_date')
    walkforward_end = params.get('walkforward_end_date')
    
    # If dates not provided, use the first 80% of data for backtest and the last 20% for walk-forward
    if not all([backtest_start, backtest_end, walkforward_start, walkforward_end]):
        all_dates = sorted(df["date"].unique())
        split_idx = int(len(all_dates) * 0.8)
        
        backtest_start = all_dates[0]
        backtest_end = all_dates[split_idx - 1]
        walkforward_start = all_dates[split_idx]
        walkforward_end = all_dates[-1]
        
        print(f"Using date range from data: {backtest_start} to {walkforward_end}")
    
    print(f"Backtest period: {backtest_start} to {backtest_end}")
    print(f"Walk-forward period: {walkforward_start} to {walkforward_end}")
    
    # Filter data for backtest period
    backtest_df = df[(df["date"] >= backtest_start) & (df["date"] <= backtest_end)]
    
    # Filter data for walk-forward period
    walkforward_df = df[(df["date"] >= walkforward_start) & (df["date"] <= walkforward_end)]
    
    # Find optimal parameters using backtest data if not provided
    if top_configs is None:
        print("\nFinding optimal configurations using backtest period...")
        top_configs = perform_parameter_sweep(backtest_df, params)
        
        if not top_configs:
            print("No optimal configurations found in backtest period. Aborting walk-forward analysis.")
            return None
    
    # Apply top configurations to walk-forward data
    print("\nApplying optimal configurations to walk-forward period...")
    
    all_backtest_results = []
    all_walkforward_results = []
    
    # Process each day/time/direction with its optimal parameters
    for group_key, top_df in top_configs.items():
        day, conf_time, direction = group_key
        
        print(f"\nProcessing {day} {direction} at {conf_time}:")
        
        # For each of the top configurations
        for _, config in top_df.iterrows():
            # Set configuration parameters
            config_params = params.copy()
            config_params['default_sd_retracement'] = config['sd_retracement']
            config_params['default_target_sd'] = config['target_sd']
            
            # Process BACKTEST trades first for comparison
            backtest_day_df = backtest_df[pd.to_datetime(backtest_df['date']).dt.day_name() == day]
            backtest_results = []
            
            if not backtest_day_df.empty:
                print(f"  Backtest: SD Retracement={config['sd_retracement']:.1f}, Target SD={config['target_sd']:.1f}")
                
                # Apply this configuration to get backtest results
                if direction == 'long':
                    for date, group_df in backtest_day_df.groupby('date'):
                        # Important: Reset index to ensure all columns are accessible as regular columns
                        session_df = group_df.reset_index(drop=True)
                        
                        if "timestamp" not in session_df.columns:
                            print(f"Warning: timestamp column missing in backtest data for {date}")
                            print(f"Available columns: {session_df.columns}")
                            continue
                        
                        metrics = get_long_session_metrics_with_sd_retracement(
                            session_df, config['sd_retracement'], config_params
                        )
                        if metrics and metrics['confirm_time_str'] == conf_time:
                            metrics['config_key'] = f"{day}_{conf_time}_{direction}"
                            metrics['sd_retracement'] = config['sd_retracement']
                            metrics['target_sd'] = config['target_sd']
                            backtest_results.append(metrics)
                else:
                    for date, group_df in backtest_day_df.groupby('date'):
                        # Important: Reset index to ensure all columns are accessible as regular columns
                        session_df = group_df.reset_index(drop=True)
                        
                        if "timestamp" not in session_df.columns:
                            print(f"Warning: timestamp column missing in backtest data for {date}")
                            print(f"Available columns: {session_df.columns}")
                            continue
                        
                        metrics = get_short_session_metrics_with_sd_retracement(
                            session_df, config['sd_retracement'], config_params
                        )
                        if metrics and metrics['confirm_time_str'] == conf_time:
                            metrics['config_key'] = f"{day}_{conf_time}_{direction}"
                            metrics['sd_retracement'] = config['sd_retracement']
                            metrics['target_sd'] = config['target_sd']
                            backtest_results.append(metrics)
                
                # Print backtest results
                if backtest_results:
                    backtest_df = pd.DataFrame(backtest_results)
                    valid_trades = backtest_df[backtest_df["trade_entry"].notna()]
                    
                    if not valid_trades.empty:
                        win_rate, _, _, expectancy = calculate_expectancy(valid_trades)
                        total_r = valid_trades["pnl_R"].sum()
                        
                        print(f"    Backtest: {len(valid_trades)} trades, Win Rate: {win_rate:.2f}%, "
                              f"Total R: {total_r:.2f}, Expectancy: {expectancy:.2f}")
                    else:
                        print(f"    Backtest: No valid trades")
                else:
                    print(f"    Backtest: No valid setups")
            
            # Now process WALK-FORWARD trades
            walkforward_day_df = walkforward_df[pd.to_datetime(walkforward_df['date']).dt.day_name() == day]
            walkforward_results = []
            
            if not walkforward_day_df.empty:
                # Apply this configuration to get walk-forward results
                if direction == 'long':
                    for date, group_df in walkforward_day_df.groupby('date'):
                        # Important: Reset index to ensure all columns are accessible as regular columns
                        session_df = group_df.reset_index(drop=True)
                        
                        if "timestamp" not in session_df.columns:
                            print(f"Warning: timestamp column missing in walk-forward data for {date}")
                            print(f"Available columns: {session_df.columns}")
                            continue
                        
                        try:
                            metrics = get_long_session_metrics_with_sd_retracement(
                                session_df, config['sd_retracement'], config_params
                            )
                            if metrics and metrics['confirm_time_str'] == conf_time:
                                metrics['config_key'] = f"{day}_{conf_time}_{direction}"
                                metrics['sd_retracement'] = config['sd_retracement']
                                metrics['target_sd'] = config['target_sd']
                                walkforward_results.append(metrics)
                        except Exception as e:
                            print(f"Error processing walk-forward data for {date}: {e}")
                            print(f"DataFrame shape: {session_df.shape}")
                            print(f"DataFrame columns: {session_df.columns}")
                            continue
                else:
                    for date, group_df in walkforward_day_df.groupby('date'):
                        # Important: Reset index to ensure all columns are accessible as regular columns
                        session_df = group_df.reset_index(drop=True)
                        
                        if "timestamp" not in session_df.columns:
                            print(f"Warning: timestamp column missing in walk-forward data for {date}")
                            print(f"Available columns: {session_df.columns}")
                            continue
                        
                        try:
                            metrics = get_short_session_metrics_with_sd_retracement(
                                session_df, config['sd_retracement'], config_params
                            )
                            if metrics and metrics['confirm_time_str'] == conf_time:
                                metrics['config_key'] = f"{day}_{conf_time}_{direction}"
                                metrics['sd_retracement'] = config['sd_retracement']
                                metrics['target_sd'] = config['target_sd']
                                walkforward_results.append(metrics)
                        except Exception as e:
                            print(f"Error processing walk-forward data for {date}: {e}")
                            print(f"DataFrame shape: {session_df.shape}")
                            print(f"DataFrame columns: {session_df.columns}")
                            continue
                
                # Print walk-forward results
                if walkforward_results:
                    walkforward_result_df = pd.DataFrame(walkforward_results)
                    valid_trades = walkforward_result_df[walkforward_result_df["trade_entry"].notna()]
                    
                    if not valid_trades.empty:
                        win_rate, _, _, expectancy = calculate_expectancy(valid_trades)
                        total_r = valid_trades["pnl_R"].sum()
                        
                        print(f"    Walk-Forward: {len(valid_trades)} trades, Win Rate: {win_rate:.2f}%, "
                              f"Total R: {total_r:.2f}, Expectancy: {expectancy:.2f}")
                    else:
                        print(f"    Walk-Forward: No valid trades")
                else:
                    print(f"    Walk-Forward: No valid setups")
            
            # Append results to overall lists
            all_backtest_results.extend(backtest_results)
            all_walkforward_results.extend(walkforward_results)
    
    # Convert to DataFrames
    all_backtest_df = pd.DataFrame(all_backtest_results) if all_backtest_results else pd.DataFrame()
    all_walkforward_df = pd.DataFrame(all_walkforward_results) if all_walkforward_results else pd.DataFrame()
    
    # Save results
    output_dir = params['file_options']['output_dir']
    os.makedirs(output_dir, exist_ok=True)
    
    if not all_backtest_df.empty:
        all_backtest_df.to_csv(f"{output_dir}/backtest_results.csv", index=False)
        print(f"\nBacktest results saved to {output_dir}/backtest_results.csv")
    
    if not all_walkforward_df.empty:
        all_walkforward_df.to_csv(f"{output_dir}/walkforward_results.csv", index=False)
        print(f"Walk-forward results saved to {output_dir}/walkforward_results.csv")
    
    # Analyze and print overall results
    print("\n" + "=" * 100)
    print(" " * 30 + "WALK-FORWARD ANALYSIS RESULTS")
    print("=" * 100)
    
    # Backtest summary
    print("\n--- BACKTEST PERIOD SUMMARY ---")
    if not all_backtest_df.empty:
        backtest_valid_trades = all_backtest_df[all_backtest_df["trade_entry"].notna()]
        
        if not backtest_valid_trades.empty:
            win_rate, avg_winner, avg_loser, expectancy = calculate_expectancy(backtest_valid_trades)
            total_r = backtest_valid_trades["pnl_R"].sum()
            profit_factor = calculate_profit_factor(backtest_valid_trades)
            
            print(f"Total trades: {len(backtest_valid_trades)}")
            print(f"Win Rate: {win_rate:.2f}%")
            print(f"Average Winner: {avg_winner:.2f}R")
            print(f"Average Loser: {avg_loser:.2f}R")
            print(f"Expectancy: {expectancy:.2f}R")
            print(f"Total R: {total_r:.2f}")
            print(f"Profit Factor: {profit_factor:.2f}")
            
            # Generate equity curve if requested
            if params['generate_equity_curve']:
                generate_equity_curve(backtest_valid_trades, output_dir, "Equity_Curve", "Backtest")
        else:
            print("No valid trades in backtest period")
    else:
        print("No setups in backtest period")
    
    # Walk-forward summary
    print("\n--- WALK-FORWARD PERIOD SUMMARY ---")
    if not all_walkforward_df.empty:
        walkforward_valid_trades = all_walkforward_df[all_walkforward_df["trade_entry"].notna()]
        
        if not walkforward_valid_trades.empty:
            win_rate, avg_winner, avg_loser, expectancy = calculate_expectancy(walkforward_valid_trades)
            total_r = walkforward_valid_trades["pnl_R"].sum()
            profit_factor = calculate_profit_factor(walkforward_valid_trades)
            
            print(f"Total trades: {len(walkforward_valid_trades)}")
            print(f"Win Rate: {win_rate:.2f}%")
            print(f"Average Winner: {avg_winner:.2f}R")
            print(f"Average Loser: {avg_loser:.2f}R")
            print(f"Expectancy: {expectancy:.2f}R")
            print(f"Total R: {total_r:.2f}")
            print(f"Profit Factor: {profit_factor:.2f}")
            
            # Generate equity curve if requested
            if params['generate_equity_curve']:
                generate_equity_curve(walkforward_valid_trades, output_dir, "Equity_Curve", "Walk-Forward")
        else:
            print("No valid trades in walk-forward period")
    else:
        print("No setups in walk-forward period")
    
    # Create comparison by configuration
    if not all_backtest_df.empty and not all_walkforward_df.empty:
        print("\n--- CONFIGURATION COMPARISON ---")
        
        # Group by configuration
        config_groups = [
            ('backtest', all_backtest_df[all_backtest_df["trade_entry"].notna()]),
            ('walkforward', all_walkforward_df[all_walkforward_df["trade_entry"].notna()])
        ]
        
        combined_results = []
        
        # For each configuration
        for period_name, period_df in config_groups:
            if not period_df.empty:
                # Group by config_key, sd_retracement, target_sd
                for config_key, config_group in period_df.groupby(['config_key', 'sd_retracement', 'target_sd']):
                    key, sd_retrace, target_sd = config_key
                    
                    # Calculate metrics
                    win_rate, _, _, expectancy = calculate_expectancy(config_group)
                    total_r = config_group["pnl_R"].sum()
                    trade_count = len(config_group)
                    
                    combined_results.append({
                        'period': period_name,
                        'config_key': key,
                        'sd_retracement': sd_retrace,
                        'target_sd': target_sd,
                        'trade_count': trade_count,
                        'win_rate': win_rate,
                        'total_r': total_r,
                        'expectancy': expectancy
                    })
        
        # Convert to DataFrame and pivot to compare periods
        if combined_results:
            combined_df = pd.DataFrame(combined_results)
            pivot_df = combined_df.pivot_table(
                index=['config_key', 'sd_retracement', 'target_sd'],
                columns='period',
                values=['trade_count', 'win_rate', 'total_r', 'expectancy'],
                aggfunc='first'
            )
            
            # Save comparison
            pivot_df.to_csv(f"{output_dir}/period_comparison.csv")
            print(f"Period comparison saved to {output_dir}/period_comparison.csv")
            
            # Print comparison
            for config_idx, config_row in pivot_df.iterrows():
                config_key, sd_retrace, target_sd = config_idx
                day, time, direction = config_key.split('_')
                
                print(f"\n{day} {direction.capitalize()} at {time}, Retracement SD: {sd_retrace}, Target SD: {target_sd}")
                
                # Extract metrics for both periods
                bt_trades = config_row['trade_count'].get('backtest', 0)
                wf_trades = config_row['trade_count'].get('walkforward', 0)
                
                bt_win_rate = config_row['win_rate'].get('backtest', 0)
                wf_win_rate = config_row['win_rate'].get('walkforward', 0)
                
                bt_total_r = config_row['total_r'].get('backtest', 0)
                wf_total_r = config_row['total_r'].get('walkforward', 0)
                
                bt_expectancy = config_row['expectancy'].get('backtest', 0)
                wf_expectancy = config_row['expectancy'].get('walkforward', 0)
                
                # Calculate percent changes
                win_rate_change = ((wf_win_rate - bt_win_rate) / bt_win_rate * 100) if bt_win_rate != 0 else float('inf')
                expectancy_change = ((wf_expectancy - bt_expectancy) / bt_expectancy * 100) if bt_expectancy != 0 else float('inf')
                
                # Print comparison
                print(f"  Trades: Backtest={bt_trades}, Walk-Forward={wf_trades}")
                print(f"  Win Rate: Backtest={bt_win_rate:.2f}%, Walk-Forward={wf_win_rate:.2f}% (Change: {win_rate_change:.2f}%)")
                print(f"  Total R: Backtest={bt_total_r:.2f}, Walk-Forward={wf_total_r:.2f}")
                print(f"  Expectancy: Backtest={bt_expectancy:.2f}R, Walk-Forward={wf_expectancy:.2f}R (Change: {expectancy_change:.2f}%)")
    
    # Create performance visualization if matplotlib is available
    if MATPLOTLIB_AVAILABLE and (not all_backtest_df.empty and not all_walkforward_df.empty):
        try:
            # Create directory for visualizations
            os.makedirs(f"{output_dir}/visualizations", exist_ok=True)
            
            # Extract valid trades for both periods
            backtest_valid = all_backtest_df[all_backtest_df["trade_entry"].notna()]
            walkforward_valid = all_walkforward_df[all_walkforward_df["trade_entry"].notna()]
            
            if not backtest_valid.empty and not walkforward_valid.empty:
                # Create a combined equity curve
                plt.figure(figsize=(14, 7))
                
                # Calculate cumulative R for each period
                backtest_trades = backtest_valid.sort_values('date')
                backtest_trades['cumulative_r'] = backtest_trades['pnl_R'].cumsum()
                
                walkforward_trades = walkforward_valid.sort_values('date')
                walkforward_trades['cumulative_r'] = walkforward_trades['pnl_R'].cumsum()
                
                # Plot with different colors
                plt.plot(range(len(backtest_trades)), backtest_trades['cumulative_r'], 
                         label=f'Backtest ({backtest_trades["pnl_R"].sum():.2f}R)', color='blue')
                
                # Connect the two periods with a dotted line
                plt.plot([len(backtest_trades)-1, len(backtest_trades)], 
                         [backtest_trades['cumulative_r'].iloc[-1], backtest_trades['cumulative_r'].iloc[-1] + walkforward_trades['cumulative_r'].iloc[0]],
                         'k--', alpha=0.5)
                
                # Plot walk-forward period
                plt.plot(range(len(backtest_trades), len(backtest_trades) + len(walkforward_trades)), 
                         backtest_trades['cumulative_r'].iloc[-1] + walkforward_trades['cumulative_r'], 
                         label=f'Walk-Forward ({walkforward_trades["pnl_R"].sum():.2f}R)', color='green')

                # Add vertical line to mark the separation
                plt.axvline(x=len(backtest_trades), color='r', linestyle='--', alpha=0.7, 
                            label=f'Period Split (Backtest End: {backtest_end}, Walk-Forward Start: {walkforward_start})')
                
                # Format the plot
                plt.grid(True, alpha=0.3)
                plt.title('Combined Equity Curve: Backtest vs Walk-Forward')
                plt.xlabel('Number of Trades')
                plt.ylabel('Cumulative R')
                plt.legend()
                
                # Save plot
                plt.tight_layout()
                plt.savefig(f"{output_dir}/visualizations/combined_equity_curve.png", dpi=300)
                plt.close()
                
                print(f"Combined equity curve saved to {output_dir}/visualizations/combined_equity_curve.png")
                
                # Create a bar chart comparing metrics
                if 'combined_df' in locals() and not combined_df.empty:
                    # Group by configuration and calculate metrics
                    config_means = combined_df.groupby('period').agg({
                        'win_rate': 'mean',
                        'expectancy': 'mean',
                        'total_r': 'sum',
                        'trade_count': 'sum'
                    }).reset_index()
                    
                    # Create subplots for each metric
                    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
                    
                    # Win Rate
                    axes[0, 0].bar(config_means['period'], config_means['win_rate'], color=['blue', 'green'])
                    axes[0, 0].set_title('Average Win Rate (%)')
                    axes[0, 0].set_ylim(0, max(config_means['win_rate']) * 1.2)
                    for i, v in enumerate(config_means['win_rate']):
                        axes[0, 0].text(i, v + 1, f"{v:.2f}%", ha='center')
                    
                    # Expectancy
                    axes[0, 1].bar(config_means['period'], config_means['expectancy'], color=['blue', 'green'])
                    axes[0, 1].set_title('Average Expectancy (R)')
                    axes[0, 1].set_ylim(0, max(config_means['expectancy']) * 1.2)
                    for i, v in enumerate(config_means['expectancy']):
                        axes[0, 1].text(i, v + 0.05, f"{v:.2f}R", ha='center')
                    
                    # Total R
                    axes[1, 0].bar(config_means['period'], config_means['total_r'], color=['blue', 'green'])
                    axes[1, 0].set_title('Total R')
                    for i, v in enumerate(config_means['total_r']):
                        axes[1, 0].text(i, v + (0.05 * abs(v)), f"{v:.2f}R", ha='center')
                    
                    # Trade Count
                    axes[1, 1].bar(config_means['period'], config_means['trade_count'], color=['blue', 'green'])
                    axes[1, 1].set_title('Number of Trades')
                    axes[1, 1].set_ylim(0, max(config_means['trade_count']) * 1.2)
                    for i, v in enumerate(config_means['trade_count']):
                        axes[1, 1].text(i, v + 1, str(v), ha='center')
                    
                    plt.tight_layout()
                    plt.savefig(f"{output_dir}/visualizations/period_comparison.png", dpi=300)
                    plt.close()
                    
                    print(f"Period comparison chart saved to {output_dir}/visualizations/period_comparison.png")
        
        except Exception as e:
            print(f"Error creating visualizations: {str(e)}")
    
    # Return the results
    return {
        'backtest_results': all_backtest_df,
        'walkforward_results': all_walkforward_df
    }

# =========================== DISPLAY FUNCTIONS ===========================

def display_raw_data(results_df, params):
    """Display raw configuration data"""
    if not params["show_raw_data"]:
        return
    
    # Define weekday order for sorting
    weekday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']
    
    print("\n" + "=" * 100)
    print(" " * 30 + "RAW DATA - ALL CONFIGURATIONS")
    print("=" * 100)
    
    # Group by day_of_week, confirmation time, trade_direction, and sd_retracement + target_sd
    grouped = results_df.groupby(['day_of_week', 'confirm_time_str', 'trade_direction', 'sd_retracement', 'target_sd'])
    
    # Sort days of week in proper order
    sorted_groups = []
    
    for name, group in grouped:
        day, conf_time, direction, sd_retrace, target_sd = name
        day_index = weekday_order.index(day) if day in weekday_order else 999
        sorted_groups.append((day_index, conf_time, direction, sd_retrace, target_sd, group))
    
    # Sort by day, time, direction, and SD params
    sorted_groups.sort()
    
    # Print raw data in the format requested
    for group_info in sorted_groups:
        day_index, conf_time, direction, sd_retrace, target_sd, group = group_info
        day = group['day_of_week'].iloc[0]  # Get actual day name
        total_in_group = len(group)
        valid = group[group["trade_entry"].notna()]
        valid_count = len(valid)
        
        if not valid.empty:
            # Use the corrected expectancy calculation
            win_rate, avg_winner, avg_loser, grp_expectancy = calculate_expectancy(valid)
            
            # Calculate additional metrics
            profit_factor = calculate_profit_factor(valid) if params["calculate_profit_factor"] else None
            
            # Also calculate TP win rate for reference
            tp_win_rate = (valid["exit_reason"] == "TP").mean() * 100
            grp_total_R = valid["pnl_R"].sum()
            
            # Highlight profitability with simple markers
            profit_marker = '+' if grp_total_R > 0 else '-' if grp_total_R < 0 else ' '
            
            # SD-based approach
            output_line = (f"{day} at {conf_time} ({str(direction).capitalize()}, SD Retrace: {float(sd_retrace):.1f}, Target SD: {float(target_sd):.1f}): "
                  f"{total_in_group} sessions, Valid Occurrences: {valid_count}, "
                  f"Profit Win Rate: {win_rate:.2f}% (TP Rate: {tp_win_rate:.2f}%), "
                  f"Total R: {profit_marker}{abs(grp_total_R):.2f}, Expectancy: {grp_expectancy:.2f}")
            
            if profit_factor is not None:
                output_line += f", Profit Factor: {profit_factor:.2f}"
                
            print(output_line)
        else:
            # SD-based approach with no valid trades
            print(f"{day} at {conf_time} ({str(direction).capitalize()}, SD Retrace: {float(sd_retrace):.1f}, Target SD: {float(target_sd):.1f}): "
                f"{total_in_group} sessions, Valid Occurrences: {valid_count}, "
                f"Profit Win Rate: N/A, Total R: N/A, Expectancy: N/A")
                
def display_overall_stats(results_df, filtered_df, params):
    """Display overall strategy statistics"""
    if not params["show_overall_stats"]:
        return
    
    print("\n" + "=" * 100)
    print(" " * 35 + "OVERALL STATISTICS")
    print("=" * 100)
    
    # Create a copy of results_df with unfiltered trade data for comparison
    unfiltered_results_df = results_df.copy()
    if 'unfiltered_trade_entry' in unfiltered_results_df.columns and 'unfiltered_pnl_R' in unfiltered_results_df.columns and 'unfiltered_exit_reason' in unfiltered_results_df.columns:
        # Copy unfiltered data to the columns used for analysis
        mask = unfiltered_results_df['unfiltered_trade_entry'].notna()
        unfiltered_results_df.loc[mask, 'trade_entry'] = unfiltered_results_df.loc[mask, 'unfiltered_trade_entry']
        unfiltered_results_df.loc[mask, 'pnl_R'] = unfiltered_results_df.loc[mask, 'unfiltered_pnl_R']
        unfiltered_results_df.loc[mask, 'exit_reason'] = unfiltered_results_df.loc[mask, 'unfiltered_exit_reason']
    
    # Print stats for unfiltered data
    if params["use_wdr_filter"] or params["use_m7b_filter"]:
        print("\n--- WITHOUT FILTERS ---")
    
    # Unfiltered stats - using the unfiltered data
    unfiltered_valid_trades_df = unfiltered_results_df[unfiltered_results_df["trade_entry"].notna()]
    median_retracement = unfiltered_results_df["retracement_SD"].median()
    median_extension = unfiltered_results_df["extension_SD"].median()
    
    print(f"Total valid session entries: {len(unfiltered_results_df)}")
    print(f"Median Retracement (in SD units): {median_retracement:.2f}")
    print(f"Median Maximum Extension (in SD units): {median_extension:.2f}")

    if not unfiltered_valid_trades_df.empty:
        # Use GPU-accelerated expectancy calculation if available
        if params['use_gpu'] and GPU_AVAILABLE:
            try:
                profit_win_rate, avg_winner, avg_loser, overall_expectancy = calculate_expectancy_gpu(unfiltered_valid_trades_df)
            except Exception as e:
                print(f"GPU calculation failed, falling back to CPU: {e}")
                profit_win_rate, avg_winner, avg_loser, overall_expectancy = calculate_expectancy(unfiltered_valid_trades_df)
        else:
            profit_win_rate, avg_winner, avg_loser, overall_expectancy = calculate_expectancy(unfiltered_valid_trades_df)
        
        # Calculate additional metrics
        if params["calculate_profit_factor"]:
            profit_factor = calculate_profit_factor(unfiltered_valid_trades_df)
        
        if params["analyze_streaks"]:
            max_win_streak, max_loss_streak, avg_win_streak, avg_loss_streak, _ = analyze_streaks(unfiltered_valid_trades_df)
        
        # Also calculate TP hit rate for reference
        tp_win_rate = (unfiltered_valid_trades_df["exit_reason"] == "TP").mean() * 100
        total_R = unfiltered_valid_trades_df["pnl_R"].sum()
        
        print(f"\nValid trades: {len(unfiltered_valid_trades_df)}")
        print(f"Overall Profit Win Rate: {profit_win_rate:.2f}%")
        print(f"Overall TP Hit Rate: {tp_win_rate:.2f}%")
        print(f"Overall Total R (sum of pnl_R): {total_R:.2f}")
        print(f"Overall Average Winner (in R): {avg_winner:.2f}")
        print(f"Overall Average Loser (in R): {avg_loser:.2f}")
        print(f"Overall Expectancy (in R): {overall_expectancy:.2f}")
        
        if params["calculate_profit_factor"]:
            print(f"Overall Profit Factor: {profit_factor:.2f}")
        
        if params["analyze_streaks"]:
            print("\nStreaks Analysis:")
            print(f"Max Winning Streak: {max_win_streak}")
            print(f"Max Losing Streak: {max_loss_streak}")
            print(f"Average Winning Streak: {avg_win_streak:.2f}")
            print(f"Average Losing Streak: {avg_loss_streak:.2f}")
    else:
        print("No trades were simulated in the valid sessions.")
    
    # Print stats for filtered data if filtering is enabled
    if (params["use_wdr_filter"] or params["use_m7b_filter"]) and params["show_overall_stats"]:
        print("\n--- WITH FILTERS ---")
        filtered_valid_trades = filtered_df[filtered_df["trade_entry"].notna()]
        filtered_median_retracement = filtered_df["retracement_SD"].median()
        filtered_median_extension = filtered_df["extension_SD"].median()
        
        filtered_session_count = len(filtered_df)
        if params["use_wdr_filter"] and 'wdr_session_invalid' in filtered_df.columns:
            filtered_session_count -= filtered_df['wdr_session_invalid'].sum()
        
        print(f"Total valid session entries: {filtered_session_count}")
        print(f"Median Retracement (in SD units): {filtered_median_retracement:.2f}")
        print(f"Median Maximum Extension (in SD units): {filtered_median_extension:.2f}")

        if not filtered_valid_trades.empty:
            # Use corrected expectancy calculation
            filtered_profit_win_rate, filtered_avg_winner, filtered_avg_loser, filtered_expectancy = calculate_expectancy(filtered_valid_trades)
            
            # Calculate additional metrics for filtered data
            if params["calculate_profit_factor"]:
                filtered_profit_factor = calculate_profit_factor(filtered_valid_trades)
            
            if params["analyze_streaks"]:
                filtered_max_win_streak, filtered_max_loss_streak, filtered_avg_win_streak, filtered_avg_loss_streak, _ = analyze_streaks(filtered_valid_trades)
            
            # Also calculate TP hit rate for reference
            filtered_tp_win_rate = (filtered_valid_trades["exit_reason"] == "TP").mean() * 100
            filtered_total_R = filtered_valid_trades["pnl_R"].sum()
            
            print(f"\nValid trades: {len(filtered_valid_trades)}")
            print(f"Overall Profit Win Rate: {filtered_profit_win_rate:.2f}%")
            print(f"Overall TP Hit Rate: {filtered_tp_win_rate:.2f}%")
            print(f"Overall Total R (sum of pnl_R): {filtered_total_R:.2f}")
            print(f"Overall Average Winner (in R): {filtered_avg_winner:.2f}")
            print(f"Overall Average Loser (in R): {filtered_avg_loser:.2f}")
            print(f"Overall Expectancy (in R): {filtered_expectancy:.2f}")
            
            if params["calculate_profit_factor"]:
                print(f"Overall Profit Factor: {filtered_profit_factor:.2f}")
            
            if params["analyze_streaks"]:
                print("\nStreaks Analysis (Filtered):")
                print(f"Max Winning Streak: {filtered_max_win_streak}")
                print(f"Max Losing Streak: {filtered_max_loss_streak}")
                print(f"Average Winning Streak: {filtered_avg_win_streak:.2f}")
                print(f"Average Losing Streak: {filtered_avg_loss_streak:.2f}")
            
            # Calculate improvement safely
            try:
                improvement_R = filtered_total_R - total_R
                improvement_R_pct = (improvement_R/abs(total_R))*100 if total_R != 0 else float('inf')
            except:
                improvement_R = 0
                improvement_R_pct = 0
                
            try:
                improvement_win_rate = filtered_profit_win_rate - profit_win_rate
                improvement_win_rate_pct = (improvement_win_rate/profit_win_rate)*100 if profit_win_rate != 0 else float('inf')
            except:
                improvement_win_rate = 0
                improvement_win_rate_pct = 0
                
            try:
                improvement_expectancy = filtered_expectancy - overall_expectancy
                improvement_expectancy_pct = (improvement_expectancy/abs(overall_expectancy))*100 if overall_expectancy != 0 else float('inf')
            except:
                improvement_expectancy = 0
                improvement_expectancy_pct = 0
            
            print("\nImprovement from filtering:")
            print(f"Total R: {improvement_R:.2f} ({improvement_R_pct:.2f}% change)")
            print(f"Win Rate: {improvement_win_rate:.2f}% ({improvement_win_rate_pct:.2f}% change)")
            print(f"Expectancy: {improvement_expectancy:.2f} ({improvement_expectancy_pct:.2f}% change)")
            
            if params["calculate_profit_factor"] and 'profit_factor' in locals() and profit_factor != 0:
                improvement_pf = filtered_profit_factor - profit_factor
                improvement_pf_pct = (improvement_pf/profit_factor)*100
                print(f"Profit Factor: {improvement_pf:.2f} ({improvement_pf_pct:.2f}% change)")
        else:
            print("No trades were simulated in the filtered sessions.")
    
    return unfiltered_valid_trades_df, filtered_valid_trades

def display_day_performance(unfiltered_valid_trades_df, filtered_valid_trades, params):
    """Display performance by day of week"""
    if not params["show_day_performance"]:
        return
    
    # Define weekday order for sorting
    weekday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']
    
    print("\n" + "=" * 100)
    print(" " * 32 + "PERFORMANCE BY DAY OF WEEK")
    print("=" * 100)
    
    # First show unfiltered day of week performance
    if params["use_wdr_filter"] or params["use_m7b_filter"]:
        print("\n--- WITHOUT FILTERS ---")
    
    # Calculate day of week performance for unfiltered data
    if not unfiltered_valid_trades_df.empty:
        # First, handle the case when analysis_mode is "both"
        if params["analysis_mode"] == "both":
            print("\n" + "-" * 50)
            print(" " * 15 + "LONG TRADES")
            print("-" * 50)
            
            # Filter for long trades only
            long_trades_df = unfiltered_valid_trades_df[unfiltered_valid_trades_df['trade_direction'] == "long"]
            
            if not long_trades_df.empty:
                for day in weekday_order:
                    if day in long_trades_df['day_of_week'].values:
                        day_data = long_trades_df[long_trades_df['day_of_week'] == day]
                        
                        trade_count = len(day_data)
                        win_rate, _, _, expectancy = calculate_expectancy(day_data)
                        tp_rate = (day_data["exit_reason"] == "TP").mean() * 100
                        total_r = day_data['pnl_R'].sum()
                        avg_r = day_data['pnl_R'].mean()
                        
                        # Calculate profit factor if requested
                        profit_factor_str = ""
                        if params["calculate_profit_factor"]:
                            profit_factor = calculate_profit_factor(day_data)
                            profit_factor_str = f" | Profit Factor: {profit_factor:6.2f}"
                        
                        # Profit indicator
                        profit_indicator = "+" if total_r > 0 else "-" if total_r < 0 else " "
                        
                        # Format the output line
                        print(f"{day:<10} | Trades: {trade_count:3d} | Profit Rate: {win_rate:6.2f}% | TP Rate: {tp_rate:6.2f}% | "
                              f"Total R: {profit_indicator}{abs(total_r):6.2f} | Avg R: {avg_r:6.2f} | Expectancy: {expectancy:6.2f}{profit_factor_str}")
            else:
                print("No valid long trades to analyze by day of week.")
                
            print("\n" + "-" * 50)
            print(" " * 15 + "SHORT TRADES")
            print("-" * 50)
            
            # Filter for short trades only
            short_trades_df = unfiltered_valid_trades_df[unfiltered_valid_trades_df['trade_direction'] == "short"]
            
            if not short_trades_df.empty:
                for day in weekday_order:
                    if day in short_trades_df['day_of_week'].values:
                        day_data = short_trades_df[short_trades_df['day_of_week'] == day]
                        
                        trade_count = len(day_data)
                        win_rate, _, _, expectancy = calculate_expectancy(day_data)
                        tp_rate = (day_data["exit_reason"] == "TP").mean() * 100
                        total_r = day_data['pnl_R'].sum()
                        avg_r = day_data['pnl_R'].mean()
                        
                        # Calculate profit factor if requested
                        profit_factor_str = ""
                        if params["calculate_profit_factor"]:
                            profit_factor = calculate_profit_factor(day_data)
                            profit_factor_str = f" | Profit Factor: {profit_factor:6.2f}"
                        
                        # Profit indicator
                        profit_indicator = "+" if total_r > 0 else "-" if total_r < 0 else " "
                        
                        # Format the output line
                        print(f"{day:<10} | Trades: {trade_count:3d} | Profit Rate: {win_rate:6.2f}% | TP Rate: {tp_rate:6.2f}% | "
                              f"Total R: {profit_indicator}{abs(total_r):6.2f} | Avg R: {avg_r:6.2f} | Expectancy: {expectancy:6.2f}{profit_factor_str}")
            else:
                print("No valid short trades to analyze by day of week.")
        
        # If analysis_mode is "long" or "short", keep the original logic
        else:
            for day in weekday_order:
                if day in unfiltered_valid_trades_df['day_of_week'].values:
                    day_data = unfiltered_valid_trades_df[unfiltered_valid_trades_df['day_of_week'] == day]
                    
                    trade_count = len(day_data)
                    win_rate, _, _, expectancy = calculate_expectancy(day_data)
                    tp_rate = (day_data["exit_reason"] == "TP").mean() * 100
                    total_r = day_data['pnl_R'].sum()
                    avg_r = day_data['pnl_R'].mean()
                    
                    # Calculate profit factor if requested
                    profit_factor_str = ""
                    if params["calculate_profit_factor"]:
                        profit_factor = calculate_profit_factor(day_data)
                        profit_factor_str = f" | Profit Factor: {profit_factor:6.2f}"
                    
                    # Profit indicator
                    profit_indicator = "+" if total_r > 0 else "-" if total_r < 0 else " "
                    
                    # Format the output line
                    print(f"{day:<10} | Trades: {trade_count:3d} | Profit Rate: {win_rate:6.2f}% | TP Rate: {tp_rate:6.2f}% | "
                          f"Total R: {profit_indicator}{abs(total_r):6.2f} | Avg R: {avg_r:6.2f} | Expectancy: {expectancy:6.2f}{profit_factor_str}")
    else:
        print("No valid trades to analyze by day of week.")
    
    # Then show filtered day of week performance (if enabled)
    if (params["use_wdr_filter"] or params["use_m7b_filter"]) and not filtered_valid_trades.empty:
        print("\n--- WITH FILTERS ---")
        
        # First, handle the case when analysis_mode is "both"
        if params["analysis_mode"] == "both":
            print("\n" + "-" * 50)
            print(" " * 15 + "LONG TRADES (Filtered)")
            print("-" * 50)
            
            # Filter for long trades only
            long_trades_df = filtered_valid_trades[filtered_valid_trades['trade_direction'] == "long"]
            
            if not long_trades_df.empty:
                for day in weekday_order:
                    if day in long_trades_df['day_of_week'].values:
                        day_data = long_trades_df[long_trades_df['day_of_week'] == day]
                        
                        trade_count = len(day_data)
                        win_rate, _, _, expectancy = calculate_expectancy(day_data)
                        tp_rate = (day_data["exit_reason"] == "TP").mean() * 100
                        total_r = day_data['pnl_R'].sum()
                        avg_r = day_data['pnl_R'].mean()
                        
                        # Calculate profit factor if requested
                        profit_factor_str = ""
                        if params["calculate_profit_factor"]:
                            profit_factor = calculate_profit_factor(day_data)
                            profit_factor_str = f" | Profit Factor: {profit_factor:6.2f}"
                        
                        # Profit indicator
                        profit_indicator = "+" if total_r > 0 else "-" if total_r < 0 else " "
                        
                        # Format the output line
                        print(f"{day:<10} | Trades: {trade_count:3d} | Profit Rate: {win_rate:6.2f}% | TP Rate: {tp_rate:6.2f}% | "
                              f"Total R: {profit_indicator}{abs(total_r):6.2f} | Avg R: {avg_r:6.2f} | Expectancy: {expectancy:6.2f}{profit_factor_str}")
            else:
                print("No valid long trades to analyze by day of week with filters.")
                
            print("\n" + "-" * 50)
            print(" " * 15 + "SHORT TRADES (Filtered)")
            print("-" * 50)
            
            # Filter for short trades only
            short_trades_df = filtered_valid_trades[filtered_valid_trades['trade_direction'] == "short"]
            
            if not short_trades_df.empty:
                for day in weekday_order:
                    if day in short_trades_df['day_of_week'].values:
                        day_data = short_trades_df[short_trades_df['day_of_week'] == day]
                        
                        trade_count = len(day_data)
                        win_rate, _, _, expectancy = calculate_expectancy(day_data)
                        tp_rate = (day_data["exit_reason"] == "TP").mean() * 100
                        total_r = day_data['pnl_R'].sum()
                        avg_r = day_data['pnl_R'].mean()
                        
                        # Calculate profit factor if requested
                        profit_factor_str = ""
                        if params["calculate_profit_factor"]:
                            profit_factor = calculate_profit_factor(day_data)
                            profit_factor_str = f" | Profit Factor: {profit_factor:6.2f}"
                        
                        # Profit indicator
                        profit_indicator = "+" if total_r > 0 else "-" if total_r < 0 else " "
                        
                        # Format the output line
                        print(f"{day:<10} | Trades: {trade_count:3d} | Profit Rate: {win_rate:6.2f}% | TP Rate: {tp_rate:6.2f}% | "
                              f"Total R: {profit_indicator}{abs(total_r):6.2f} | Avg R: {avg_r:6.2f} | Expectancy: {expectancy:6.2f}{profit_factor_str}")
            else:
                print("No valid short trades to analyze by day of week with filters.")
        
        # If analysis_mode is "long" or "short", keep the original logic but for filtered data
        else:
            for day in weekday_order:
                if day in filtered_valid_trades['day_of_week'].values:
                    day_data = filtered_valid_trades[filtered_valid_trades['day_of_week'] == day]
                    
                    trade_count = len(day_data)
                    win_rate, _, _, expectancy = calculate_expectancy(day_data)
                    tp_rate = (day_data["exit_reason"] == "TP").mean() * 100
                    total_r = day_data['pnl_R'].sum()
                    avg_r = day_data['pnl_R'].mean()
                    
                    # Calculate profit factor if requested
                    profit_factor_str = ""
                    if params["calculate_profit_factor"]:
                        profit_factor = calculate_profit_factor(day_data)
                        profit_factor_str = f" | Profit Factor: {profit_factor:6.2f}"
                    
                    # Profit indicator
                    profit_indicator = "+" if total_r > 0 else "-" if total_r < 0 else " "
                    
                    # Format the output line
                    print(f"{day:<10} | Trades: {trade_count:3d} | Profit Rate: {win_rate:6.2f}% | TP Rate: {tp_rate:6.2f}% | "
                          f"Total R: {profit_indicator}{abs(total_r):6.2f} | Avg R: {avg_r:6.2f} | Expectancy: {expectancy:6.2f}{profit_factor_str}")

def display_top_configs(results_df, filtered_df, unfiltered_valid_trades_df, filtered_valid_trades, params):
    """Display top configurations"""
    if not params["show_top_configs"]:
        return
    
    print("\n" + "=" * 100)
    print(" " * 30 + f"TOP {params['top_configs']} MOST PROFITABLE CONFIGURATIONS")
    print("=" * 100)
    
    # First show unfiltered top configurations
    if params["use_wdr_filter"] or params["use_m7b_filter"]:
        print("\n--- WITHOUT FILTERS ---")
    
    # Group for SD-based statistics
    unfiltered_grouped = unfiltered_valid_trades_df.groupby(['day_of_week', 'confirm_time_str', 'trade_direction', 'sd_retracement', 'target_sd'])
    
    # Calculate metrics for each configuration (unfiltered)
    config_metrics_df = None
    filtered_config_metrics_df = None
    
    if not unfiltered_valid_trades_df.empty:
        config_metrics = []
        
        for name, group in unfiltered_grouped:
            day, conf_time, direction, sd_retrace, target_sd = name
            grouping_key = {'day': day, 'time': conf_time, 'direction': direction, 
                            'sd_retracement': sd_retrace, 'target_sd': target_sd}
                
            valid = group[group["trade_entry"].notna()]
            valid_count = len(valid)
            
            if valid_count >= params["min_trades"]:  # Only consider configurations with at least min_trades_for_analysis trades
                # Use corrected expectancy calculation
                win_rate, avg_winner, avg_loser, expectancy = calculate_expectancy(valid)
                total_r = valid["pnl_R"].sum()
                
                # Add profit factor if requested
                if params["calculate_profit_factor"]:
                    profit_factor = calculate_profit_factor(valid)
                else:
                    profit_factor = None
                
                config_entry = grouping_key.copy()
                config_entry.update({
                    'trades': valid_count,
                    'win_rate': win_rate,
                    'total_r': total_r,
                    'expectancy': expectancy,
                    'profit_factor': profit_factor
                })
                config_metrics.append(config_entry)
        
        # Sort by total R (most profitable first)
        config_metrics_df = pd.DataFrame(config_metrics)
        if not config_metrics_df.empty:
            sorted_configs = config_metrics_df.sort_values('total_r', ascending=False).head(params["top_configs"])
            
            # Print the top configurations
            for i, row in sorted_configs.iterrows():
                # Profit indicator
                profit_indicator = "+" if row['total_r'] > 0 else "-" if row['total_r'] < 0 else " "
                exp_indicator = "+" if row['expectancy'] > 0 else "-" if row['expectancy'] < 0 else " "
                
                # SD-based approach
                output_line = (f"#{i+1:2d}: {row['day']:<9} at {row['time']} ({row['direction'].capitalize():<5}, "
                      f"SD Retrace: {float(row['sd_retracement']):+.2f}, Target SD: {float(row['target_sd']):+.2f}): "
                      f"Trades: {row['trades']:3d}, Profit Rate: {row['win_rate']:6.2f}%, "
                      f"Total R: {profit_indicator}{abs(row['total_r']):6.2f}, Expectancy: {exp_indicator}{abs(row['expectancy']):5.2f}")
                
                if params["calculate_profit_factor"] and 'profit_factor' in row and row['profit_factor'] is not None:
                    output_line += f", Profit Factor: {row['profit_factor']:.2f}"
                
                print(output_line)
        else:
            print("No configurations with sufficient trades to analyze.")
    else:
        print("No valid trades found for analysis.")
    
    # Then show filtered top configurations (if enabled)
    if (params["use_wdr_filter"] or params["use_m7b_filter"]) and filtered_valid_trades is not None and not filtered_valid_trades.empty:
        print("\n--- WITH FILTERS ---")
        
        # Group and calculate metrics for filtered configurations
        filtered_grouped = filtered_valid_trades.groupby(['day_of_week', 'confirm_time_str', 'trade_direction', 'sd_retracement', 'target_sd'])
            
        filtered_config_metrics = []
        
        for name, group in filtered_grouped:
            day, conf_time, direction, sd_retrace, target_sd = name
            grouping_key = {'day': day, 'time': conf_time, 'direction': direction, 
                            'sd_retracement': sd_retrace, 'target_sd': target_sd}
                
            valid = group[group["trade_entry"].notna()]
            valid_count = len(valid)
            
            if valid_count >= params["min_trades"]:
                # Use corrected expectancy calculation
                win_rate, avg_winner, avg_loser, expectancy = calculate_expectancy(valid)
                total_r = valid["pnl_R"].sum()
                
                # Add profit factor if requested
                if params["calculate_profit_factor"]:
                    profit_factor = calculate_profit_factor(valid)
                else:
                    profit_factor = None
                
                config_entry = grouping_key.copy()
                config_entry.update({
                    'trades': valid_count,
                    'win_rate': win_rate,
                    'total_r': total_r,
                    'expectancy': expectancy,
                    'profit_factor': profit_factor
                })
                filtered_config_metrics.append(config_entry)
        
        # Sort by total R (most profitable first)
        filtered_config_metrics_df = pd.DataFrame(filtered_config_metrics)
        if not filtered_config_metrics_df.empty:
            filtered_sorted_configs = filtered_config_metrics_df.sort_values('total_r', ascending=False).head(params["top_configs"])
            
            # Print the top filtered configurations
            for i, row in filtered_sorted_configs.iterrows():
                # Profit indicator
                profit_indicator = "+" if row['total_r'] > 0 else "-" if row['total_r'] < 0 else " "
                exp_indicator = "+" if row['expectancy'] > 0 else "-" if row['expectancy'] < 0 else " "
                
                # SD-based approach
                output_line = (f"#{i+1:2d}: {row['day']:<9} at {row['time']} ({row['direction'].capitalize():<5}, "
                      f"SD Retrace: {float(row['sd_retracement']):+.2f}, Target SD: {float(row['target_sd']):+.2f}): "
                      f"Trades: {row['trades']:3d}, Profit Rate: {row['win_rate']:6.2f}%, "
                      f"Total R: {profit_indicator}{abs(row['total_r']):6.2f}, Expectancy: {exp_indicator}{abs(row['expectancy']):5.2f}")
                
                if params["calculate_profit_factor"] and 'profit_factor' in row and row['profit_factor'] is not None:
                    output_line += f", Profit Factor: {row['profit_factor']:.2f}"
                
                print(output_line)
        else:
            print("No configurations with sufficient trades to analyze after filtering.")
    
    return config_metrics_df, filtered_config_metrics_df

def display_target_configs(config_metrics_df, filtered_config_metrics_df, params):
    """Display configurations meeting target criteria"""
    if not params["show_target_configs"] or (params["target_win_rate"] is None and params["target_expectancy"] is None):
        return
    
    print("\n" + "=" * 100)
    print(" " * 30 + "CONFIGURATIONS MEETING TARGET CRITERIA")
    print("=" * 100)
    print(f"Target Win Rate: {params['target_win_rate']}%, Target Expectancy: {params['target_expectancy']}R")
    
    # First show unfiltered configurations meeting criteria
    if params["use_wdr_filter"] or params["use_m7b_filter"]:
        print("\n--- WITHOUT FILTERS ---")
    
    # Filter configurations meeting criteria (unfiltered)
    if config_metrics_df is not None and not config_metrics_df.empty:
        criteria_configs = config_metrics_df.copy()
        
        if params["target_win_rate"] is not None:
            criteria_configs = criteria_configs[criteria_configs['win_rate'] >= params["target_win_rate"]]
        if params["target_expectancy"] is not None:
            criteria_configs = criteria_configs[criteria_configs['expectancy'] >= params["target_expectancy"]]
        
        if not criteria_configs.empty:
            # Sort by total R
            sorted_criteria = criteria_configs.sort_values('total_r', ascending=False)
            
            for i, row in sorted_criteria.iterrows():
                # Profit indicator
                profit_indicator = "+" if row['total_r'] > 0 else "-" if row['total_r'] < 0 else " "
                exp_indicator = "+" if row['expectancy'] > 0 else "-" if row['expectancy'] < 0 else " "
                
                # SD-based approach
                output_line = (f"{row['day']:<9} at {row['time']} ({row['direction'].capitalize():<5}, "
                      f"SD Retrace: {float(row['sd_retracement']):+.2f}, Target SD: {float(row['target_sd']):+.2f}): "
                      f"Trades: {row['trades']:3d}, Profit Rate: {row['win_rate']:6.2f}%, "
                      f"Total R: {profit_indicator}{abs(row['total_r']):6.2f}, Expectancy: {exp_indicator}{abs(row['expectancy']):5.2f}")
                
                if params["calculate_profit_factor"] and 'profit_factor' in row and row['profit_factor'] is not None:
                    output_line += f", Profit Factor: {row['profit_factor']:.2f}"
                
                print(output_line)
        else:
            print("No configurations meet the target criteria.")
    else:
        print("No configuration metrics available for target filtering.")
    
    # Then show filtered configurations meeting criteria (if enabled)
    if (params["use_wdr_filter"] or params["use_m7b_filter"]) and filtered_config_metrics_df is not None and not filtered_config_metrics_df.empty:
        print("\n--- WITH FILTERS ---")
        
        filtered_criteria = filtered_config_metrics_df.copy()
        
        if params["target_win_rate"] is not None:
            filtered_criteria = filtered_criteria[filtered_criteria['win_rate'] >= params["target_win_rate"]]
        if params["target_expectancy"] is not None:
            filtered_criteria = filtered_criteria[filtered_criteria['expectancy'] >= params["target_expectancy"]]
        
        if not filtered_criteria.empty:
            # Sort by total R
            sorted_filtered_criteria = filtered_criteria.sort_values('total_r', ascending=False)
            
            for i, row in sorted_filtered_criteria.iterrows():
                # Profit indicator
                profit_indicator = "+" if row['total_r'] > 0 else "-" if row['total_r'] < 0 else " "
                exp_indicator = "+" if row['expectancy'] > 0 else "-" if row['expectancy'] < 0 else " "
                
                # SD-based approach
                output_line = (f"{row['day']:<9} at {row['time']} ({row['direction'].capitalize():<5}, "
                      f"SD Retrace: {float(row['sd_retracement']):+.2f}, Target SD: {float(row['target_sd']):+.2f}): "
                      f"Trades: {row['trades']:3d}, Profit Rate: {row['win_rate']:6.2f}%, "
                      f"Total R: {profit_indicator}{abs(row['total_r']):6.2f}, Expectancy: {exp_indicator}{abs(row['expectancy']):5.2f}")
                
                if params["calculate_profit_factor"] and 'profit_factor' in row and row['profit_factor'] is not None:
                    output_line += f", Profit Factor: {row['profit_factor']:.2f}"
                
                print(output_line)
        else:
            print("No configurations meet the target criteria after filtering.")

def display_filter_stats(results_df, params):
    """Display filter statistics"""
    if not params["show_filter_stats"]:
        return
    
    weekday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']
    
    # Print M7B statistics if filter is enabled
    if params["use_m7b_filter"] and 'm7b_invalid' in results_df.columns:
        # Fixed: Use fillna to handle NaN values in boolean filtering
        m7b_counts = results_df[results_df['m7b_invalid'].fillna(False) & results_df['unfiltered_trade_entry'].notna()].shape[0]
        
        print("\n" + "=" * 100)
        print(" " * 35 + "M7B FILTER STATISTICS")
        print("=" * 100)
        
        # Print overall filter count
        unfiltered_total = results_df['unfiltered_trade_entry'].notna().sum()
        filtered_total = results_df['trade_entry'].notna().sum()
        filtered_out = unfiltered_total - filtered_total
        pct_filtered = (filtered_out / unfiltered_total * 100) if unfiltered_total > 0 else 0
        
        print(f"\nM7B filter removed {filtered_out} of {unfiltered_total} trades ({pct_filtered:.2f}%)")
        
        # Print sample filtered trades
        if 'm7b_upper' in results_df.columns and 'm7b_lower' in results_df.columns:
            # Fixed: Use fillna in boolean filtering
            sample_data = results_df[results_df['m7b_invalid'].fillna(False) & results_df['unfiltered_trade_entry'].notna()].head(5)
            if not sample_data.empty:
                print("\nSample of filtered trades (first 5):")
                for _, row in sample_data.iterrows():
                    direction = row['trade_direction']
                    entry = row['unfiltered_trade_entry']
                    if pd.notna(entry):  # Make sure entry is not NaN
                        if direction == 'long':
                            boundary = row['m7b_lower']
                            if pd.notna(boundary):  # Make sure boundary is not NaN
                                print(f"Long trade entry: {entry:.2f}, M7B lower: {boundary:.2f}, Entry < M7B lower: {entry < boundary}")
                        else:
                            boundary = row['m7b_upper']
                            if pd.notna(boundary):  # Make sure boundary is not NaN
                                print(f"Short trade entry: {entry:.2f}, M7B upper: {boundary:.2f}, Entry > M7B upper: {entry > boundary}")
        
        # Print by reason if available
        if 'm7b_reason' in results_df.columns:
            # Count invalidation reasons - Fixed: Use fillna in boolean filtering
            m7b_invalid_trades = results_df[results_df['m7b_invalid'].fillna(False) & results_df['unfiltered_trade_entry'].notna()]
            if not m7b_invalid_trades.empty:
                reason_counts = m7b_invalid_trades['m7b_reason'].value_counts()
                print("\nM7B invalidation reasons:")
                for reason, count in reason_counts.items():
                    if reason and pd.notna(reason) and count > 0:  # Only print non-empty reasons
                        pct = count / m7b_counts * 100 if m7b_counts > 0 else 0
                        print(f"  - {reason}: {count} ({pct:.2f}%)")
        
        # By day of week
        print("\nM7B filtered trades by day of week:")
        # Group by day of week and count trades and filtered trades - Fixed: Use fillna in boolean filtering
        day_counts = {}
        for day in weekday_order:
            day_trades = results_df[(results_df['day_of_week'] == day) & results_df['unfiltered_trade_entry'].notna()]
            # Use fillna to handle NaN values in boolean filtering
            day_filtered = day_trades[day_trades['m7b_invalid'].fillna(False)].shape[0]
            day_total = day_trades.shape[0]
            if day_total > 0:
                day_counts[day] = (day_filtered, day_total, day_filtered/day_total*100)
        
        for day, (filtered, total, pct) in day_counts.items():
            print(f"  - {day}: {filtered}/{total} ({pct:.2f}%)")
        
        # By trade direction
        print("\nM7B filtered trades by direction:")
        # Group by trade direction and count trades and filtered trades - Fixed: Use fillna in boolean filtering
        direction_counts = {}
        for direction in ['long', 'short']:
            dir_trades = results_df[(results_df['trade_direction'] == direction) & results_df['unfiltered_trade_entry'].notna()]
            # Use fillna to handle NaN values in boolean filtering
            dir_filtered = dir_trades[dir_trades['m7b_invalid'].fillna(False)].shape[0]
            dir_total = dir_trades.shape[0]
            if dir_total > 0:
                direction_counts[direction] = (dir_filtered, dir_total, dir_filtered/dir_total*100)
        
        for direction, (filtered, total, pct) in direction_counts.items():
            print(f"  - {direction.capitalize()}: {filtered}/{total} ({pct:.2f}%)")

    # Print WDR statistics if filter is enabled
    if params["use_wdr_filter"] and 'wdr_session_invalid' in results_df.columns:
        session_counts = results_df['wdr_session_invalid'].sum()
        # Fixed: Use fillna in boolean filtering for wdr_cluster_invalid
        cluster_counts = results_df[~results_df['wdr_session_invalid'] & results_df['wdr_cluster_invalid'].fillna(False)].shape[0]
        
        print("\n" + "=" * 100)
        print(" " * 35 + "WDR FILTER STATISTICS")
        print("=" * 100)
        
        # Print by reason if available
        if 'wdr_reason' in results_df.columns:
            # Count session invalidation reasons
            session_invalid = results_df[results_df['wdr_session_invalid']]
            if not session_invalid.empty:
                reason_counts = session_invalid['wdr_reason'].value_counts()
                print("\nSession invalidation reasons:")
                for reason, count in reason_counts.items():
                    if reason and pd.notna(reason) and count > 0:  # Only print non-empty reasons
                        pct = count / session_counts * 100 if session_counts > 0 else 0
                        print(f"  - {reason}: {count} ({pct:.2f}%)")
            
            # Count cluster invalidation reasons - Fixed: Use fillna in boolean filtering
            cluster_invalid = results_df[~results_df['wdr_session_invalid'] & results_df['wdr_cluster_invalid'].fillna(False)]
            if not cluster_invalid.empty:
                cluster_reason_counts = cluster_invalid['wdr_reason'].value_counts()
                print("\nCluster invalidation reasons:")
                for reason, count in cluster_reason_counts.items():
                    if reason and pd.notna(reason) and count > 0:  # Only print non-empty reasons
                        pct = count / cluster_counts * 100 if cluster_counts > 0 else 0
                        print(f"  - {reason}: {count} ({pct:.2f}%)")
        
        # By day of week
        print("\nWDR filtered sessions by day of week:")
        day_total = results_df.groupby('day_of_week').size()
        day_invalid = session_invalid.groupby('day_of_week').size() if 'session_invalid' in locals() and not session_invalid.empty else pd.Series()
        
        for day in weekday_order:
            if day in day_total.index:
                invalid_count = day_invalid.get(day, 0)
                total_count = day_total[day]
                invalid_pct = invalid_count / total_count * 100 if total_count > 0 else 0
                print(f"  - {day}: {invalid_count}/{total_count} ({invalid_pct:.2f}%)")

# =========================== INTERACTIVE INTERFACE FUNCTIONS ===========================

def get_user_input():
    """Interactive parameter input from the user with improved organization"""
    print("\n" + "=" * 80)
    print(" " * 20 + "ES TRADING STRATEGY CONFIGURATION")
    print("=" * 80)
    
    params = default_params.copy()
    
    # ALWAYS enable parameter sweep by default
    params["perform_param_sweep"] = True
    
    # Main configuration categories with improved organization
    categories = [
        "1. Core Strategy Parameters",
        "2. Time Windows",
        "3. Trade Parameters",
        "4. Analysis Parameters",
        "5. Filtering Options", 
        "6. Analysis Periods",
        "7. Output Controls",
        "8. Performance Options",
        "9. File Options",
        "10. Run with current settings"
    ]
    
    while True:
        print("\nConfiguration Categories:")
        for category in categories:
            print(f"{category}")
        
        try:
            choice = int(input("\nSelect a category (1-10): "))
            if not 1 <= choice <= len(categories):
                print("Invalid selection. Please try again.")
                continue
            
            # If user selects "Run with current settings"
            if choice == 10:
                break
                
            # Process each category
            if choice == 1:  # Core Strategy Parameters
                print("\n--- Core Strategy Parameters ---")
                
                # Analysis mode
                print(f"\nCurrent analysis mode: {params['analysis_mode']}")
                mode_choice = input("Select analysis mode (long/short/both) [leave blank to keep current]: ")
                if mode_choice.strip() in ["long", "short", "both"]:
                    params["analysis_mode"] = mode_choice
                
                # SD retracement levels
                print(f"\nCurrent SD retracement levels: {params['sd_retracement_levels']}")
                sd_input = input("Enter SD retracement levels separated by spaces [leave blank to keep current]: ")
                if sd_input.strip():
                    try:
                        params["sd_retracement_levels"] = [float(x) for x in sd_input.split()]
                        print(f"New SD retracement levels: {params['sd_retracement_levels']}")
                    except ValueError:
                        print("Invalid input. Keeping current SD retracement levels.")
                
                # Target SD ranges
                print(f"\nCurrent target SD ranges: {params['target_sd_ranges']}")
                target_ranges_input = input("Enter target SD values separated by spaces [leave blank to keep current]: ")
                if target_ranges_input.strip():
                    try:
                        params["target_sd_ranges"] = [float(x) for x in target_ranges_input.split()]
                        print(f"New target SD ranges: {params['target_sd_ranges']}")
                    except ValueError:
                        print("Invalid input. Keeping current values.")
                
                # Selected days
                weekdays = ["Monday", "Tuesday", "Wednesday", "Thursday", "Friday"]
                current_days = params["selected_days"] if params["selected_days"] else "All days"
                print(f"\nCurrent selected days: {current_days}")
                print("Select days of week to analyze:")
                print("1. All days")
                print("2. Specific days")
                days_choice = input("Enter choice (1-2) [leave blank to keep current]: ")
                
                if days_choice == "1":
                    params["selected_days"] = None
                elif days_choice == "2":
                    selected = []
                    for day in weekdays:
                        include = input(f"Include {day}? (y/n): ").lower()
                        if include == "y":
                            selected.append(day)
                    if selected:
                        params["selected_days"] = selected
                        print(f"Selected days: {selected}")
                    else:
                        print("No days selected. Using all days.")
                        params["selected_days"] = None
                
            elif choice == 2:  # Time Windows
                print("\n--- Time Windows ---")
                
                # Long confirmation window
                print(f"Current long confirmation window: {params['long_confirm_start']} to {params['long_confirm_end']}")
                long_start = input("Long confirmation start time (HH:MM) [leave blank to keep current]: ")
                if long_start.strip():
                    params["long_confirm_start"] = long_start
                
                long_end = input("Long confirmation end time (HH:MM) [leave blank to keep current]: ")
                if long_end.strip():
                    params["long_confirm_end"] = long_end
                
                # Short confirmation window
                print(f"Current short confirmation window: {params['short_confirm_start']} to {params['short_confirm_end']}")
                short_start = input("Short confirmation start time (HH:MM) [leave blank to keep current]: ")
                if short_start.strip():
                    params["short_confirm_start"] = short_start
                
                short_end = input("Short confirmation end time (HH:MM) [leave blank to keep current]: ")
                if short_end.strip():
                    params["short_confirm_end"] = short_end
            
            elif choice == 3:  # Trade Parameters
                print("\n--- Trade Parameters ---")
                
                # TP multiple or Target SD
                print(f"Current Target SD: {params['default_target_sd']}")
                target_sd_input = input("Target SD value [leave blank to keep current]: ")
                if target_sd_input.strip():
                    try:
                        params["default_target_sd"] = float(target_sd_input)
                    except ValueError:
                        print("Invalid input. Keeping current value.")
                
                # SL distance
                print(f"Current SL distance: {params['sl_distance']}")
                sl_input = input("SL distance as multiple of SD [leave blank to keep current]: ")
                if sl_input.strip():
                    try:
                        params["sl_distance"] = float(sl_input)
                    except ValueError:
                        print("Invalid input. Keeping current value.")
                
                # SL extra ticks
                print(f"Current SL extra ticks: {params['sl_extra_ticks']}")
                ticks_input = input("Additional ticks to add to stop loss [leave blank to keep current]: ")
                if ticks_input.strip():
                    try:
                        params["sl_extra_ticks"] = int(ticks_input)
                    except ValueError:
                        print("Invalid input. Keeping current value.")
            
            elif choice == 4:  # Analysis Parameters
                print("\n--- Analysis Parameters ---")
                
                # Minimum trades
                print(f"Current minimum trades for config analysis: {params['min_trades']}")
                min_trades_input = input("Minimum trades for a configuration to be considered [leave blank to keep current]: ")
                if min_trades_input.strip():
                    try:
                        params["min_trades"] = int(min_trades_input)
                    except ValueError:
                        print("Invalid input. Keeping current value.")
                
                # Top configs
                print(f"Current number of top configs to show: {params['top_configs']}")
                top_configs_input = input("Number of top configurations to display [leave blank to keep current]: ")
                if top_configs_input.strip():
                    try:
                        params["top_configs"] = int(top_configs_input)
                    except ValueError:
                        print("Invalid input. Keeping current value.")
                
                # Keep top configs
                print(f"Current number of top configs to keep for each day/time: {params['keep_top_configs']}")
                keep_top_input = input("Number of top configurations to keep for each day/time/direction [leave blank to keep current]: ")
                if keep_top_input.strip():
                    try:
                        params["keep_top_configs"] = int(keep_top_input)
                    except ValueError:
                        print("Invalid input. Keeping current value.")
                
                # Target win rate
                print(f"Current target win rate: {params['target_win_rate']}%")
                win_rate_input = input("Target win rate for filtering (%) [leave blank to keep current]: ")
                if win_rate_input.strip():
                    try:
                        params["target_win_rate"] = float(win_rate_input)
                    except ValueError:
                        print("Invalid input. Keeping current value.")
                
                # Target expectancy
                print(f"Current target expectancy: {params['target_expectancy']}R")
                expectancy_input = input("Target expectancy for filtering (R) [leave blank to keep current]: ")
                if expectancy_input.strip():
                    try:
                        params["target_expectancy"] = float(expectancy_input)
                    except ValueError:
                        print("Invalid input. Keeping current value.")
                
                # Max R reduction percentage
                print(f"Current maximum allowed R reduction: {params['max_r_reduction_pct']}%")
                max_r_input = input("Maximum allowed R reduction percentage [leave blank to keep current]: ")
                if max_r_input.strip():
                    try:
                        params["max_r_reduction_pct"] = float(max_r_input)
                    except ValueError:
                        print("Invalid input. Keeping current value.")
            
            elif choice == 5:  # Filtering Options
                print("\n--- Filtering Options ---")
                
                # WDR filter
                print(f"Current WDR filter status: {'Enabled' if params['use_wdr_filter'] else 'Disabled'}")
                wdr_choice = input("Enable WDR filter? (y/n) [leave blank to keep current]: ").lower()
                if wdr_choice in ["y", "n"]:
                    params["use_wdr_filter"] = (wdr_choice == "y")
                
                # M7B filter
                print(f"Current M7B filter status: {'Enabled' if params['use_m7b_filter'] else 'Disabled'}")
                m7b_choice = input("Enable M7B filter? (y/n) [leave blank to keep current]: ").lower()
                if m7b_choice in ["y", "n"]:
                    params["use_m7b_filter"] = (m7b_choice == "y")
                    
            elif choice == 6:  # Analysis Periods
                print("\n--- Analysis Periods ---")
                
                # Parameter sweep - DEFAULT to TRUE for better user experience
                # Most users expect parameter sweep to be on
                params["perform_param_sweep"] = True
                print(f"Parameter Sweep: Enabled (default)")
                
                # Walk-forward analysis
                print(f"Current walk-forward analysis status: {'Enabled' if params['perform_walk_forward'] else 'Disabled'}")
                wf_choice = input("Enable walk-forward analysis? (y/n) [leave blank to keep current]: ").lower()
                if wf_choice in ["y", "n"]:
                    params["perform_walk_forward"] = (wf_choice == "y")
                
                if params["perform_walk_forward"]:
                    # Backtest period
                    print("\nSpecify backtest period:")
                    backtest_start = input("Backtest start date (YYYY-MM-DD) [leave blank for first date in data]: ")
                    if backtest_start.strip():
                        params["backtest_start_date"] = backtest_start
                    
                    backtest_end = input("Backtest end date (YYYY-MM-DD) [leave blank for 80% of data]: ")
                    if backtest_end.strip():
                        params["backtest_end_date"] = backtest_end
                    
                    # Walk-forward period
                    print("\nSpecify walk-forward period:")
                    walkforward_start = input("Walk-forward start date (YYYY-MM-DD) [leave blank for after backtest end]: ")
                    if walkforward_start.strip():
                        params["walkforward_start_date"] = walkforward_start
                    
                    walkforward_end = input("Walk-forward end date (YYYY-MM-DD) [leave blank for last date in data]: ")
                    if walkforward_end.strip():
                        params["walkforward_end_date"] = walkforward_end
                
                # Generate equity curve
                print(f"\nCurrent equity curve generation status: {'Enabled' if params['generate_equity_curve'] else 'Disabled'}")
                equity_choice = input("Generate equity curves? (y/n) [leave blank to keep current]: ").lower()
                if equity_choice in ["y", "n"]:
                    params["generate_equity_curve"] = (equity_choice == "y")
            
            elif choice == 7:  # Output Controls
                print("\n--- Output Controls ---")
                output_options = [
                    ("show_raw_data", "Show raw configuration data"),
                    ("show_overall_stats", "Show overall statistics"),
                    ("show_day_performance", "Show day of week performance"),
                    ("show_top_configs", "Show top configurations"),
                    ("show_target_configs", "Show configurations meeting target criteria"),
                    ("show_filter_stats", "Show filter statistics"),
                    ("calculate_profit_factor", "Calculate profit factor"),
                    ("analyze_streaks", "Analyze winning and losing streaks")
                ]
                
                print("Toggle output sections:")
                for i, (param_name, description) in enumerate(output_options, 1):
                    status = "Enabled" if params[param_name] else "Disabled"
                    print(f"{i}. {description}: {status}")
                
                option_choice = input("\nSelect option to toggle (1-8) or 'a' for all, 'n' for none [leave blank to keep current]: ")
                if option_choice.strip():
                    if option_choice.lower() == "a":
                        for param_name, _ in output_options:
                            params[param_name] = True
                        print("All output sections enabled.")
                    elif option_choice.lower() == "n":
                        for param_name, _ in output_options:
                            params[param_name] = False
                        print("All output sections disabled.")
                    elif option_choice.isdigit() and 1 <= int(option_choice) <= len(output_options):
                        idx = int(option_choice) - 1
                        param_name = output_options[idx][0]
                        params[param_name] = not params[param_name]
                        status = "Enabled" if params[param_name] else "Disabled"
                        print(f"{output_options[idx][1]}: {status}")
            
            elif choice == 8:  # Performance Options
                print("\n--- Performance Options ---")
                
                # GPU acceleration
                if GPU_AVAILABLE:
                    print(f"Current GPU acceleration status: {'Enabled' if params['use_gpu'] else 'Disabled'}")
                    gpu_choice = input("Enable GPU acceleration? (y/n) [leave blank to keep current]: ").lower()
                    if gpu_choice in ["y", "n"]:
                        params["use_gpu"] = (gpu_choice == "y")
                else:
                    print("GPU acceleration not available on this system (RAPIDS/CuPy not installed)")
                    params["use_gpu"] = False
                
                # Multiprocessing
                print(f"Current multiprocessing status: {'Enabled' if params['use_multiprocessing'] else 'Disabled'}")
                mp_choice = input("Enable multiprocessing for faster calculations? (y/n) [leave blank to keep current]: ").lower()
                if mp_choice in ["y", "n"]:
                    params["use_multiprocessing"] = (mp_choice == "y")
                    
                # Maximum CPU cores
                print(f"Current max CPU cores: {params['max_cpu_cores']} (system has {mp.cpu_count()} cores)")
                max_cores_input = input(f"Maximum CPU cores to use (1-{mp.cpu_count()}) [leave blank to keep current]: ")
                if max_cores_input.strip():
                    try:
                        max_cores = int(max_cores_input)
                        if 1 <= max_cores <= mp.cpu_count():
                            params["max_cpu_cores"] = max_cores
                        else:
                            print(f"Invalid input. Value must be between 1 and {mp.cpu_count()}.")
                    except ValueError:
                        print("Invalid input. Keeping current value.")
                
                # CPU usage limit
                print(f"Current CPU usage limit: {params['cpu_usage_limit']}%")
                cpu_limit_input = input("Maximum CPU usage percentage (1-100) [leave blank to keep current]: ")
                if cpu_limit_input.strip():
                    try:
                        cpu_limit = int(cpu_limit_input)
                        if 1 <= cpu_limit <= 100:
                            params["cpu_usage_limit"] = cpu_limit
                        else:
                            print("Invalid input. Value must be between 1 and 100.")
                    except ValueError:
                        print("Invalid input. Keeping current value.")
                
                # Temperature limit
                print(f"Current temperature limit: {params['max_temperature']}째C")
                temp_limit_input = input("Maximum temperature in Celsius [leave blank to keep current]: ")
                if temp_limit_input.strip():
                    try:
                        temp_limit = int(temp_limit_input)
                        if 50 <= temp_limit <= 90:  # Safe range
                            params["max_temperature"] = temp_limit
                        else:
                            print("Invalid input. Value must be between 50 and 90.")
                    except ValueError:
                        print("Invalid input. Keeping current value.")
                
                # Temperature monitoring
                print(f"Current temperature monitoring: {'Enabled' if params['enable_temp_monitoring'] else 'Disabled'}")
                temp_monitor = input("Enable temperature monitoring? (y/n) [leave blank to keep current]: ").lower()
                if temp_monitor in ["y", "n"]:
                    params["enable_temp_monitoring"] = (temp_monitor == "y")
            
            elif choice == 9:  # File Options
                print("\n--- File Options ---")
                
                # Data file
                print(f"Current data file: {params['file_options']['data_file']}")
                data_file = input("Input data file path [leave blank to keep current]: ")
                if data_file.strip():
                    params['file_options']["data_file"] = data_file
                
                # Output directory
                print(f"Current output directory: {params['file_options']['output_dir']}")
                output_dir = input("Directory to save output files [leave blank to keep current]: ")
                if output_dir.strip():
                    params['file_options']["output_dir"] = output_dir
        
        except (ValueError, IndexError):
            print("Invalid input. Please try again.")
    
    # Final confirmation
    print("\n" + "=" * 80)
    print("CURRENT CONFIGURATION SUMMARY:")
    print("=" * 80)
    print(f"Analysis Mode: {params['analysis_mode']}")
    print(f"Selected Days: {params['selected_days'] if params['selected_days'] else 'All days'}")
    
    print(f"SD Retracement Levels: {params['sd_retracement_levels']}")
    print(f"Target SD: {params['default_target_sd']}")
    
    print(f"SL Distance: {params['sl_distance']}")
    print(f"SL Extra Ticks: {params['sl_extra_ticks']}")
    print(f"WDR Filter: {'Enabled' if params['use_wdr_filter'] else 'Disabled'}")
    print(f"M7B Filter: {'Enabled' if params['use_m7b_filter'] else 'Disabled'}")
    print(f"GPU Acceleration: {'Enabled' if params['use_gpu'] and GPU_AVAILABLE else 'Disabled'}")
    print(f"Multiprocessing: {'Enabled' if params['use_multiprocessing'] else 'Disabled'}")
    print(f"Long Confirmation Window: {params['long_confirm_start']} to {params['long_confirm_end']}")
    print(f"Short Confirmation Window: {params['short_confirm_start']} to {params['short_confirm_end']}")
    
    # Always show parameter sweep status (it's enabled by default)
    print(f"\nParameter Sweep: Enabled")
    print(f"Testing all SD retracement levels: {params['sd_retracement_levels']}")
    print(f"With all target SD values: {params['target_sd_ranges']}")
    
    if params['perform_walk_forward']:
        print("\nWalk-forward Analysis: Enabled")
        print(f"Backtest Period: {params['backtest_start_date'] or 'From start'} to {params['backtest_end_date'] or '80% of data'}")
        print(f"Walk-forward Period: {params['walkforward_start_date'] or 'After backtest'} to {params['walkforward_end_date'] or 'To end'}")
    
    confirm = input("\nRun analysis with these settings? (y/n): ").lower()
    if confirm != "y":
        print("Aborted by user.")
        sys.exit()
    
    return params

# =========================== MAIN FUNCTION ===========================

def main():
    """Main function with improved flow and organization"""
    
    print("\n" + "=" * 80)
    print(" " * 20 + "ES TRADING STRATEGY ANALYZER")
    print(" " * 15 + "IMPROVED VERSION - MARCH 2025")
    print("=" * 80)
    
    # Get user parameters through interactive prompts
    params = get_user_input()
    
    # FORCE parameter sweep to be enabled
    params['perform_param_sweep'] = True
    
    # Create output directory if needed and doesn't exist
    output_dir = params['file_options']['output_dir']
    os.makedirs(output_dir, exist_ok=True)
    
    # Data Import & Preprocessing
    print(f"\nLoading data from {params['file_options']['data_file']}...")
    cols = ["date_str", "time_str", "open", "high", "low", "close", "volume"]

    try:
        # Use GPU acceleration if enabled
        if params['use_gpu'] and GPU_AVAILABLE:
            print("Using GPU for data loading and preprocessing...")
            # Use cuDF to read the CSV if available, otherwise use CuPy + pandas
            try:
                if CUDF_AVAILABLE:
                    # cuDF version
                    df = cudf.read_csv(params['file_options']['data_file'], delimiter=";", header=None, names=cols)
                    # Convert timestamp
                    df["timestamp"] = cudf.to_datetime(df["date_str"] + " " + df["time_str"], format="%d/%m/%Y %H:%M")
                    df = df.drop(columns=["date_str", "time_str"])
                    df["date"] = df["timestamp"].dt.date
                    
                    # Convert back to pandas for the rest of the processing since all our functions expect pandas
                    df = df.to_pandas()
                    df["timestamp"] = pd.to_datetime(df["timestamp"])
                    df["timestamp"] = df["timestamp"].dt.tz_localize("America/New_York")
                else:
                    # CuPy + pandas version (CuPy for computation, pandas for dataframes)
                    print("Using pandas for dataframes and CuPy for computations")
                    df = pd.read_csv(params['file_options']['data_file'], delimiter=";", header=None, names=cols)
                    df["timestamp"] = pd.to_datetime(df["date_str"] + " " + df["time_str"], format="%d/%m/%Y %H:%M")
                    df.drop(columns=["date_str", "time_str"], inplace=True)
                    df["timestamp"] = df["timestamp"].dt.tz_localize("America/New_York")
                    df["date"] = df["timestamp"].dt.date
            except Exception as e:
                print(f"Error using GPU for data loading: {e}")
                print("Falling back to CPU for data loading...")
                # Fall back to pandas
                df = pd.read_csv(params['file_options']['data_file'], delimiter=";", header=None, names=cols)
                df["timestamp"] = pd.to_datetime(df["date_str"] + " " + df["time_str"], format="%d/%m/%Y %H:%M")
                df.drop(columns=["date_str", "time_str"], inplace=True)
                df["timestamp"] = df["timestamp"].dt.tz_localize("America/New_York")
                df["date"] = df["timestamp"].dt.date
        else:
            # Standard pandas loading
            df = pd.read_csv(params['file_options']['data_file'], delimiter=";", header=None, names=cols)
            df["timestamp"] = pd.to_datetime(df["date_str"] + " " + df["time_str"], format="%d/%m/%Y %H:%M")
            df.drop(columns=["date_str", "time_str"], inplace=True)
            df["timestamp"] = df["timestamp"].dt.tz_localize("America/New_York")
            df["date"] = df["timestamp"].dt.date
    except Exception as e:
        print(f"Error loading data file: {e}")
        sys.exit(1)
    
    print(f"Loaded {len(df)} data points across {len(df['date'].unique())} trading days")
    
    # Filter by selected days if specified
    if params['selected_days'] is not None:
        print(f"Filtering for selected days: {params['selected_days']}")
        filtered_dates = []
        for date in df["date"].unique():
            if pd.Timestamp(date).day_name() in params['selected_days']:
                filtered_dates.append(date)
        df = df[df["date"].isin(filtered_dates)]
        print(f"After day filtering: {len(df)} data points across {len(df['date'].unique())} trading days")
    
    # Display strategy parameters
    print("\nStrategy Parameters:")
    print(f"Analysis Mode: {params['analysis_mode']}")
    print(f"Using SD Retracement Levels: {params['sd_retracement_levels']}")
    print(f"Default Target SD: {params['default_target_sd']}")
    print(f"SL Distance: {params['sl_distance']}")
    print(f"SL Extra Ticks: {params['sl_extra_ticks']}")
    print(f"Use WDR Filter: {params['use_wdr_filter']}")
    print(f"Use M7B Filter: {params['use_m7b_filter']}")
    print(f"Long Confirm Window: {params['long_confirm_start']} to {params['long_confirm_end']}")
    print(f"Short Confirm Window: {params['short_confirm_start']} to {params['short_confirm_end']}")
    print(f"Parameter Sweep: {'Enabled' if params['perform_param_sweep'] else 'Disabled'}")
    print(f"Walk-Forward Analysis: {'Enabled' if params['perform_walk_forward'] else 'Disabled'}")
    
    # Run parameter sweep (always enabled)
    print("\nStarting parameter sweep with all SD retracement levels...")
    top_configs = perform_parameter_sweep(df, params)
    
    # Run walk-forward analysis if requested
    if params['perform_walk_forward']:
        # Run walk-forward with top configs
        print("\nStarting walk-forward analysis with the top configurations...")
        perform_walk_forward_analysis(df, params, top_configs)
    
    print("\nAnalysis complete!")

if __name__ == "__main__":
    main()
